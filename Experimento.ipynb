{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPK9oHzMelTjre8oKjoI/+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresMontesDeOca/Laboratorio3/blob/main/Experimento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pendientes y To-Do\n"
      ],
      "metadata": {
        "id": "v4D6E5YeQ1oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log:**\n",
        "- Pruebas iniciales, Mean 12 meses y Igual a Dec 2019.\n",
        "- Se observa la tendencia negatvia\n",
        "- Tambien se observa la bajada de ventas en Aug 2018. Probamos mean, jul*1.1, del aug\n",
        "- Los 74 productos mas importantes de los 780, representan un 66% de las toneladas totales vendidas\n",
        "- Verificamos que productos se vendieron los 36 meses, y cuales son productos nuevos\n",
        "- Modelo Loco: Analisis de ventas en los Febrero en relacion al promedio del ano anterior. Los productos con ratio similar en 2017 y 2018, se predijeron de la misma manera para 2019\n",
        "- Parametros Fijos: Horizon de 2 (mes +2) | Batcn Size de 32 | Loss \"MSE\" | Optimzer \"Adam\" | Epochs 500 | EarlyStop 20\n",
        "- Armamos una red neuronal BASELINE por 1x Capa Conv + MaxPool, 3x Capa LSTM BiDireccional, 1x Capa Dropuput (%30)\n",
        "- Probamos con varias distintas configuraciones de modelos con distintas capas y neuronas\n",
        "- Tambien probamos distintos tamanos de Window (3, 6 y 12), y por ahora nos quedamos con 3\n",
        "- Mucha varianza en los modelos. Un mismo producto puede tener una varianza en la prediccion del 20%, segun la semilla. Nos dimos cuenta que la arquitectura de la red, no es lo mas importante para trabajar\n",
        "- Entrenamos hasta Jul 19, Validamos hasta Oct 19, y Test Dec 19 (Window de 3). De esta manera medimos de la misma manera que Kaggle, pero prediciendo Dec 19. Guardamos predicciones en un Excel\n",
        "- Probamos con Scalers MinMax y Robust de ScikitLearn. No observamos que el Robust se comporte mejor en las series con Outliers, el MinMax sigue pareciendo mas estable\n",
        "- Intento #1: Para casi los 600 clientes, armamos una red para cada uno con los 780 productos. Obtuvimos malos resultados\n",
        "- Intento #2: Por Cliente y Categoria, osea 3 modelos por clientes, unos 2300 modelos. Tambein con malos resultados.\n",
        "- Wolk Forward Validation (TimeSeriesSplit en SCikit-Learn), no lo pude hacer funcionar bien, tenia problemas con el early stopping\n",
        "- Identifico los clientes que no hicieron compras en los ultimos 3 meses, y los marco como desactivados. Excluyo sus compras del Dataset maestro. Mejora en las predicciones en todos los modelos hasta ahora probados\n",
        "- Indentificacion de productos no vendidos por cliente, se completo con ceros, para qeu el Dataframe arupado siempre tenga 36 elementos\n",
        "- Empezamos a overtitiar al Publico, para tener mas data para chequear el modelo final. Creamos documento con los valores de las predicciones que cuentan en el Publico para Feb 2020.\n",
        "- Agregamos las Categorias extendidas, Marcas y SKUs a los Productos\n",
        "- Intento #3: Por Cliente, Cat 1 y Cat 2. Malos resultados Tambien\n",
        "- Intento #4: Probamos haciendo un modelo por producto, osea 780 multivariados de casi 600 columnas cada uno (una por cliente). Empezamos a llevar nota de los resultados\n",
        "- Problemas con la impiutacion de valores faltantes. Prbamos con 0, backfill, Interpolacion (solo en productos hermanos), Nos terminamos quedando con KNN Inputer.\n",
        "- Nos olvidamos del Escalador 0-1 (MinMaxScaler), y empezamos  a probar con el RobustScaler\n",
        "- Intento #5(Productos Hermanos): Analizamos los casos individuales donde todas las predicciones fueron malas, y verificamos sus productos hermanos (misma cat1, cat2, cat3 y brand). Las predicciones no mejoran\n",
        "- Feature Engineering(Clase #7): Nos enteramos de que para Redes Neuronales, tambien deberiamos haber agregado variables por FE. Modificamos nuestros modelos existentes y vemos una mejora considerable (de un .34, a un .31).\n",
        "- Probamos varias librerias para el FE, pero las terminamos creando a mano\n",
        "- El 07/10 nos dimos cuenta que estuvimos entrenando con datos del futuro, y tuvimos que rehacer los experimentos\n",
        "- Entrenando hasta Oct 19, para predecir Dec 19, el mejor modelo con un Window de 3, nos dio 0.28\n",
        "- Tambien nos dio 0.28 en Kaggle, y cuando lo mixeamos con el Modelo Loco nos baja un punto a 0.27\n",
        "\n",
        "**Por hacer:**\n",
        "\n",
        "- Identificar los productos que no fueron bien preedichos por ninguno de mis modelos, y probar alternativas individuales\n",
        "\n",
        "**Pendientes para Labo 4:**\n",
        "\n",
        "- Mejoras al FE\n",
        "- Normalizacion Logaritmica p/outliers\n",
        "- Weights\n",
        "- Clusters de DTW\n",
        "- Analisis de Productos Centinela\n",
        "- Inputacion alternativa al KNN Inputer\n",
        "\n"
      ],
      "metadata": {
        "id": "4AlxzYcxRDP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones Auxiliares"
      ],
      "metadata": {
        "id": "crj3eqSV_WvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "eL3soxEh_cgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ColabNotebook = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if ColabNotebook: # maquina virtual colab\n",
        "    # monta G-drive en entorno COLAB\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    path_data = 'drive//MyDrive//Colab Notebooks//Data//Labo3//'\n",
        "    path_predicciones = path_data + 'Predicciones_cat1_cat2//'"
      ],
      "metadata": {
        "id": "HkJAzqZfivZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e424955-2cd6-4a0f-f8fa-35916717c03f"
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "import os\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "\n",
        "import warnings\n",
        "# warnings.filterwarnings('ignore', category=ValueWarning)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Asegurarte de que Pandas muestre los valores con la máxima precisión\n",
        "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
        "\n",
        "# Ajustar la opción para mostrar más filas\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Si también quieres mostrar más columnas\n",
        "pd.set_option('display.max_columns', 20)\n",
        "\n",
        "\n",
        "# Vamos a suprimir la notacion cientifica\n",
        "pd.set_option(\"display.float_format\", lambda x:\"%.2f\" %x)\n",
        "\n",
        "\n",
        "def print_ds(ds, take=3):\n",
        "    for ex in ds.take(take):\n",
        "        print(ex)\n"
      ],
      "metadata": {
        "id": "4dKLvveO9QDs"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga Datos"
      ],
      "metadata": {
        "id": "CCBiyQvD_nC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local"
      ],
      "metadata": {
        "id": "rhxib23DtW_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Code to read csv file into Colaboratory:\n",
        "# # # !pip install -U -q PyDrive\n",
        "# # from pydrive.auth import GoogleAuth\n",
        "# # from pydrive.drive import GoogleDrive\n",
        "# # from google.colab import auth, drive\n",
        "# # from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# # # Authenticate and create the PyDrive client.\n",
        "# # auth.authenticate_user()\n",
        "# # gauth = GoogleAuth()\n",
        "# # gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# # drive = GoogleDrive(gauth)\n",
        "\n",
        "# ######################################################################\n",
        "\n",
        "\n",
        "# ################################# Datasets ###################################\n",
        "# # # Ventas\n",
        "# data_ventas = pd.read_csv(\"G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\sell-in.txt\", sep=\"\\t\")\n",
        "# data_ventas['periodo'] = pd.to_datetime(data_ventas['periodo'], format='%Y%m')\n",
        "# data_ventas['customer_id'] = data_ventas['customer_id'].astype(str)\n",
        "# data_ventas['product_id'] = data_ventas['product_id'].astype(str)\n",
        "# data = data_ventas.copy()\n",
        "# data.set_index('periodo', inplace=True)\n",
        "# data_bkp = data.copy()\n",
        "\n",
        "# # # Productos\n",
        "# data_productos = pd.read_csv(\"G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\tb_productos.txt\", sep=\"\\t\")\n",
        "# data_productos['product_id'] = data_productos['product_id'].astype(str)\n",
        "\n",
        "# # # Stocks\n",
        "# data_stocks = pd.read_csv(\"G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\tb_stocks.txt\", sep=\"\\t\")\n",
        "# data_stocks['periodo'] = pd.to_datetime(data_stocks['periodo'], format='%Y%m')\n",
        "# data_stocks['product_id'] = data_stocks['product_id'].astype(str)\n",
        "\n",
        "# # # Productos a predecir\n",
        "# data_productos_a_predecir = pd.read_csv(\"G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\productos_a_predecir.txt\", sep=\"\\t\")\n",
        "# data_productos_a_predecir['product_id'] = data_productos_a_predecir['product_id'].astype(str)\n",
        "\n",
        "# # # BASELINE modelo loco 1, 303 Public y 249 Private\n",
        "# BASELINE = pd.read_csv(\"modelo_loco (1)_303_249.csv\")\n",
        "# BASELINE.set_index('product_id', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# # # Agregado 06/15, Ojo que puede romper en algun lado ya que antes no usaba el product_id como index\n",
        "# data_productos_a_predecir_index = data_productos_a_predecir.set_index('product_id', inplace=True)\n",
        "# # # No va\n",
        "# # data_productos_a_predecir.set_index('product_id', inplace=True)\n",
        "\n",
        "\n",
        "# # Comentado por las dudas 06/15\n",
        "# # data_productos_a_predecir_con_categorias = data_productos_a_predecir.join(data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()[['cat1', 'cat2', 'cat3']])\n",
        "\n",
        "\n",
        "# # # Productos con Categorias extendidas\n",
        "# data_productos_con_categorias = pd.read_csv(\"G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\tb_productos_descripcion.txt\", sep=\"\\t\")\n",
        "# data_productos_con_categorias['product_id'] = data_productos_con_categorias['product_id'].astype(str)\n",
        "\n",
        "# # # Hago el join de todos los datos brutos, y de las categorias extendidas\n",
        "# data['periodo'] = data.index\n",
        "# data_extended = pd.merge(data, data_productos_con_categorias, on='product_id', how='left')\n",
        "# data_extended.set_index('periodo', inplace=True)\n",
        "# data.drop(columns='periodo', inplace=True) # Elimino el index como columna del Dataframe original\n",
        "\n",
        "# # Elimino los productos de la Categoria1 REF\n",
        "# data_extended = data_extended[data_extended['cat1'] != 'REF']\n"
      ],
      "metadata": {
        "id": "jzQavP-RtCH2"
      },
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab"
      ],
      "metadata": {
        "id": "04LDga8vtOpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################# Datasets ###################################\n",
        "# # Ventas\n",
        "data_ventas = pd.read_csv(path_data + \"sell-in.txt\", sep=\"\\t\")\n",
        "data_ventas['periodo'] = pd.to_datetime(data_ventas['periodo'], format='%Y%m')\n",
        "data_ventas['customer_id'] = data_ventas['customer_id'].astype(str)\n",
        "data_ventas['product_id'] = data_ventas['product_id'].astype(str)\n",
        "data = data_ventas.copy()\n",
        "data.set_index('periodo', inplace=True)\n",
        "\n",
        "# # Productos\n",
        "data_productos = pd.read_csv(path_data + \"tb_productos.txt\", sep=\"\\t\")\n",
        "data_productos['product_id'] = data_productos['product_id'].astype(str)\n",
        "\n",
        "# # Stocks\n",
        "data_stocks = pd.read_csv(path_data + \"tb_stocks.txt\", sep=\"\\t\")\n",
        "data_stocks['periodo'] = pd.to_datetime(data_stocks['periodo'], format='%Y%m')\n",
        "data_stocks['product_id'] = data_stocks['product_id'].astype(str)\n",
        "\n",
        "# # Productos a predecir\n",
        "data_productos_a_predecir = pd.read_csv(path_data + \"productos_a_predecir.txt\", sep=\"\\t\")\n",
        "data_productos_a_predecir['product_id'] = data_productos_a_predecir['product_id'].astype(str)\n",
        "\n",
        "# # BASELINE modelo loco 1, 303 Public y 249 Private\n",
        "BASELINE = pd.read_csv(path_data + \"modelo_loco (1)_303_249.csv\")\n",
        "BASELINE.set_index('product_id', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# # Agregado 06/15, Ojo que puede romper en algun lado ya que antes no usaba el product_id como index\n",
        "data_productos_a_predecir_index = data_productos_a_predecir.set_index('product_id', inplace=True)\n",
        "# # No va\n",
        "# data_productos_a_predecir.set_index('product_id', inplace=True)\n",
        "\n",
        "\n",
        "# Comentado por las dudas 06/15\n",
        "# data_productos_a_predecir_con_categorias = data_productos_a_predecir.join(data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()[['cat1', 'cat2', 'cat3']])\n",
        "\n",
        "\n",
        "# # Productos con Categorias extendidas\n",
        "data_productos_con_categorias = pd.read_csv(path_data + \"tb_productos_descripcion.txt\", sep=\"\\t\")\n",
        "data_productos_con_categorias['product_id'] = data_productos_con_categorias['product_id'].astype(str)\n",
        "# Para compatibilidad con el codigo viejo\n",
        "data_productos_a_predecir_con_categorias = data_productos_con_categorias.copy()\n",
        "\n",
        "# # Hago el join de todos los datos brutos, y de las categorias extendidas\n",
        "data['periodo'] = data.index\n",
        "data_extended = pd.merge(data, data_productos_con_categorias, on='product_id', how='left')\n",
        "data_extended.set_index('periodo', inplace=True)\n",
        "data = data_extended.copy()\n",
        "data_bkp = data.copy()\n",
        "\n",
        "# # No se qie hace esto aca\n",
        "# data.drop(columns='periodo', inplace=True) # Elimino el index como columna del Dataframe original\n",
        "\n",
        "\n",
        "# Elimino los productos de la Categoria1 REF\n",
        "# data_extended = data_extended[data_extended['cat1'] != 'REF']\n",
        "\n",
        "\n",
        "# Entreno hasta Oct 2020, predecir Dec 2020. Data generada en \"07/07 - Por Cliente GENERAL\"\n",
        "data_predicciones_dec2019 = pd.read_excel(path_data + 'Bitacora.xlsx')\n",
        "data_predicciones_dec2019['product_id'] = data_predicciones_dec2019['product_id'].astype(str)\n",
        "data_predicciones_dec2019['product_id'] = data_predicciones_dec2019['product_id'].str.replace('.0', '')\n",
        "data_predicciones_dec2019.set_index('product_id', inplace=True)\n",
        "\n",
        "# data.head()"
      ],
      "metadata": {
        "id": "21T9XgDHD4TB"
      },
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Functions"
      ],
      "metadata": {
        "id": "dJckQiyL08r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "def filter_productos_hermanos(df, product_id):\n",
        "  df_product = df.query(\"product_id == @product_id\").head(1) # Sabemos que las Categorias son unicas por producto\n",
        "  cat1 = df_product['cat1'][0]\n",
        "  cat2 = df_product['cat2'][0]\n",
        "  cat3 = df_product['cat3'][0]\n",
        "  brand = df_product['brand'][0]\n",
        "\n",
        "  df_filtered = df.query(\"cat1 == @cat1 and cat2 == @cat2 and cat3 == @cat3 and brand == @brand\")\n",
        "  df_filtered.reset_index(inplace=True)\n",
        "  # df_grouped = pd.DataFrame(df_filtered.groupby(['product_id', 'periodo', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size', 'descripcion'])['tn'].sum())\n",
        "\n",
        "  result = df_filtered.groupby('product_id').agg(\n",
        "    tn=('tn', 'sum'),\n",
        "    fecha_mas_antigua=('periodo', 'min'),\n",
        "    fecha_mas_reciente=('periodo', 'max'),\n",
        "    cat1=('cat1', 'first'),\n",
        "    cat2=('cat2', 'first'),\n",
        "    cat3=('cat3', 'first'),\n",
        "    brand=('brand', 'first'),\n",
        "    descripcion=('descripcion', 'first'),\n",
        "    sku_size=('sku_size', 'first')\n",
        "    )\n",
        "\n",
        "  display(result)\n",
        "  return(result)\n",
        "##############################################################################\n",
        "def filter_products_data_grouped(data_grouped, data_productos_a_predecir):\n",
        "    \"\"\"\n",
        "    Filtra las columnas del DataFrame 'data_grouped' para que solo contenga los productos especificados en 'data_productos_a_predecir'.\n",
        "\n",
        "    Parameters:\n",
        "    data_grouped (pd.DataFrame): DataFrame con los 'product_id' como nombres de columna.\n",
        "    data_productos_a_predecir (pd.DataFrame): DataFrame donde 'product_id' es el índice y contiene los productos de interés.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame filtrado que solo contiene los productos especificados en 'data_productos_a_predecir'.\n",
        "    \"\"\"\n",
        "    productos_interes = data_productos_a_predecir.index\n",
        "    data_filtrada = data_grouped[productos_interes]\n",
        "    return data_filtrada\n",
        "##############################################################################\n",
        "def filter_products_data(data, data_productos_a_predecir):\n",
        "    \"\"\"\n",
        "    Filtra el DataFrame 'data' para que solo contenga los productos especificados en 'data_productos_a_predecir'.\n",
        "\n",
        "    Parameters:\n",
        "    data (pd.DataFrame): DataFrame original que contiene una columna 'product_id'.\n",
        "    data_productos_a_predecir (pd.DataFrame): DataFrame donde 'product_id' es el índice y contiene los productos de interés.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame filtrado que solo contiene los productos especificados en 'data_productos_a_predecir'.\n",
        "    \"\"\"\n",
        "    productos_interes = data_productos_a_predecir.index\n",
        "    data_filtrada = data[data['product_id'].isin(productos_interes)]\n",
        "    return data_filtrada\n",
        "##############################################################################\n",
        "def filter_active_clients_data(df, show_plot=False):\n",
        "  presencia_clientes = df.groupby(['periodo', 'customer_id']).size().unstack(fill_value=0)\n",
        "  # Convertimos las cantidades en 1 para indicar presencia\n",
        "  presencia_clientes[presencia_clientes > 0] = 1\n",
        "  # Creamos la Maskara, en este caso para los ultimos 3 meses\n",
        "  mask_active_clients = presencia_clientes.loc['2019-10':'2019-12'].sum(axis=0) == 3\n",
        "  # Filtramos y devolvemos el Dataset\n",
        "  active_data = df[df['customer_id'].isin(mask_active_clients[mask_active_clients].index)]\n",
        "\n",
        "  if show_plot:\n",
        "    # Configuramos el tamaño de la figura\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Creamos el heatmap\n",
        "    sns.heatmap(presencia_clientes, cmap='viridis', cbar=False, linewidths=.5)\n",
        "\n",
        "    # Añadimos los títulos y etiquetas\n",
        "    plt.title('Presencia de Clientes por Mes')\n",
        "    plt.xlabel('Clientes')\n",
        "    plt.ylabel('Mes')\n",
        "\n",
        "    # Mostramos el gráfico\n",
        "    plt.show()\n",
        "\n",
        "  return active_data\n",
        "##############################################################################\n",
        "def filter_data_por_categoria(df, categoria, categoria_columna):\n",
        "    \"\"\"\n",
        "    Filtra los productos de un DataFrame dado una categoría y el DataFrame de productos con categorías.\n",
        "\n",
        "    Args:\n",
        "    dataframe (pd.DataFrame): DataFrame con las ventas de productos (cada columna es un product_id).\n",
        "    categoria (str): Categoría a filtrar (valor de cat1, cat2 o cat3).\n",
        "    categoria_columna (str): Nombre de la columna de categoría ('cat1', 'cat2' o 'cat3').\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame filtrado con solo los productos de la categoría especificada.\n",
        "    \"\"\"\n",
        "    # Filtrar los productos que pertenecen a la categoría especificada\n",
        "    productos_filtrados = data_productos_a_predecir_con_categorias[data_productos_a_predecir_con_categorias[categoria_columna] == categoria].index\n",
        "\n",
        "    # Filtrar el DataFrame de ventas usando los product_ids de los productos filtrados\n",
        "    productos_en_data = [col for col in df.columns if col in productos_filtrados]\n",
        "    df_filtrado = df[productos_en_data]\n",
        "\n",
        "    return df_filtrado"
      ],
      "metadata": {
        "id": "nnSdLDk60-bO"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Functions"
      ],
      "metadata": {
        "id": "w-TmJKaCP5gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def plot_product(df, product_id, pipeline_type):\n",
        "\n",
        "  # Filtro solo por el producto interesado\n",
        "  data_product = df.query(\"product_id == @product_id\")\n",
        "\n",
        "  # Agrupo el Dataframe\n",
        "  data_grouped = group_data(data_product, 'product_id')\n",
        "\n",
        "  # # Arreglo fechas en productos jovenes, o discontinuados\n",
        "  data_grouped = fill_missing_dates(data_grouped, pipeline_type)\n",
        "\n",
        "  # Modelo naif, asi que llenamos con bfill para atrasm y 0 para adelante\n",
        "  data_grouped.bfill(inplace=True)\n",
        "  data_grouped.fillna(0, inplace=True)\n",
        "\n",
        "  # Arreglo Agosto 2019\n",
        "  data_grouped = fix_aug2019(data_grouped, 'mean')\n",
        "\n",
        "  display(data_productos_con_categorias.query(\"product_id == @product_id\").head())\n",
        "  display(data_predicciones_dec2019.loc[product_id].to_frame().T)\n",
        "\n",
        "  data_grouped.plot(figsize=(12, 4), title=f'Producto {product_id}')\n",
        "  plt.show()\n",
        "#############################################################################\n",
        "def plot_grouped_data_heatmap(df, eje_x):\n",
        "  presencia = df.groupby(['periodo', eje_x]).size().unstack(fill_value=0)\n",
        "  presencia[presencia > 0] = 1\n",
        "\n",
        "  # Configuramos el tamaño de la figura\n",
        "  plt.figure(figsize=(20, 10))\n",
        "  # Creamos el heatmap\n",
        "  sns.heatmap(presencia, cmap='viridis', cbar=False, linewidths=.5)\n",
        "#############################################################################\n",
        "def plot_products_sales(df, start=None, end=50, show_plot=True):\n",
        "    # Agrupa los datos por 'periodo' y 'product_id', luego deshace la pila para obtener una tabla con valores 0 o 1\n",
        "    presencia_productos = df.groupby(['periodo', 'product_id']).size().unstack(fill_value=0)\n",
        "    presencia_productos[presencia_productos > 0] = 1\n",
        "    subset = presencia_productos.iloc[:, start:end]  # Selecciona el subconjunto de productos para graficar\n",
        "\n",
        "    productos_decomisados = []  # Lista para productos que dejaron de venderse\n",
        "    productos_nuevos = []  # Lista para productos que empezaron a venderse después del inicio\n",
        "\n",
        "    if show_plot:\n",
        "        # Crear el gráfico de líneas\n",
        "        plt.figure(figsize=(20, 10))\n",
        "\n",
        "        for column in subset.columns:\n",
        "            series = subset[column]\n",
        "            # Detectar los meses con presencia\n",
        "            presencia = series == 1\n",
        "\n",
        "        # Resaltar productos que no se vendieron algún mes y añadir a la leyenda\n",
        "        for column in subset.columns:\n",
        "            series = subset[column]\n",
        "            if (series == 0).any():  # Verificar si hubo algún mes sin ventas para el producto\n",
        "                plt.plot(series.index, series, label=f\"{column} (sin ventas)\")\n",
        "                if series.iloc[0] == 1 and (series == 0).any():  # Producto que dejó de venderse\n",
        "                    productos_decomisados.append(column)\n",
        "                if series.iloc[0] == 0 and (series == 1).any():  # Producto que empezó a venderse después\n",
        "                    productos_nuevos.append(column)\n",
        "\n",
        "        # Añadir solo los productos sin ventas a la leyenda\n",
        "        handles, labels = plt.gca().get_legend_handles_labels()\n",
        "        new_handles = [handle for handle, label in zip(handles, labels) if '(sin ventas)' in label]\n",
        "        new_labels = [label for label in labels if '(sin ventas)' in label]\n",
        "\n",
        "        # Añadir título y etiquetas a los ejes\n",
        "        plt.title('Ausencia de Ventas por Productos')\n",
        "        plt.xlabel('Mes')\n",
        "        plt.ylabel('Presencia (1 = Sí, 0 = No)')\n",
        "        plt.legend(new_handles, new_labels, loc='upper right', bbox_to_anchor=(1.15, 1))  # Posicionar la leyenda\n",
        "        plt.show()  # Mostrar el gráfico\n",
        "\n",
        "    # Calcular las listas de productos decomisados y nuevos aunque no se muestre el gráfico\n",
        "    for column in subset.columns:\n",
        "        series = subset[column]\n",
        "        if (series == 0).any():\n",
        "            if series.iloc[0] == 1 and (series == 0).any():  # Producto que dejó de venderse\n",
        "                productos_decomisados.append(column)\n",
        "            if series.iloc[0] == 0 and (series == 1).any():  # Producto que empezó a venderse después\n",
        "                productos_nuevos.append(column)\n",
        "\n",
        "    return productos_decomisados, productos_nuevos  # Devolver las listas de productos decomisados y nuevos\n",
        "#############################################################################\n",
        "def plot_history(history, start_epoch=0, metrics=None):\n",
        "    if isinstance(metrics, str):\n",
        "        metrics = [metrics]\n",
        "\n",
        "    if metrics is None:\n",
        "        metrics = [x for x in history.history.keys() if x[:4] != 'val_']\n",
        "\n",
        "    if len(metrics) == 0:\n",
        "        print('No metrics to display.')\n",
        "        return\n",
        "\n",
        "    # Get the epochs and filter them starting from start_epoch\n",
        "    x = history.epoch[start_epoch:]\n",
        "\n",
        "    rows = 1\n",
        "    cols = len(metrics)\n",
        "    count = 0\n",
        "\n",
        "    plt.figure(figsize=(12 * cols, 8))\n",
        "\n",
        "    for metric in sorted(metrics):\n",
        "        count += 1\n",
        "        plt.subplot(rows, cols, count)\n",
        "        plt.plot(x, history.history[metric][start_epoch:], label='Train')\n",
        "        val_metric = f'val_{metric}'\n",
        "        if val_metric in history.history.keys():\n",
        "            plt.plot(x, history.history[val_metric][start_epoch:], label='Validation')\n",
        "        plt.title(metric.capitalize())\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "################################################################\n",
        "def plot_predictions(data_all, predictions):\n",
        "  data_conc = pd.concat([data_all, predictions])\n",
        "  separation = data_all.index[-1]\n",
        "\n",
        "  # Crear una figura y un eje\n",
        "  fig, ax = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "  # Plotear toda la serie con un color\n",
        "  ax.plot(data_conc.index, data_conc.values, label='Datos', color='blue')\n",
        "\n",
        "  ax.axvline(x=separation, color='green', linestyle='--', label='Fecha de separación')\n",
        "\n",
        "  # Sobrescribir los últimos dos elementos con un color diferente\n",
        "  ax.plot(data_conc.index[-3:], data_conc.values[-3:], color='red')\n",
        "\n",
        "  # Añadir texto para la fecha de separación\n",
        "  ax.text(separation, ax.get_ylim()[1], separation.strftime('%Y-%m'),\n",
        "            color='green', verticalalignment='bottom', horizontalalignment='right')\n",
        "\n",
        "  # Añadir leyenda y mostrar el gráfico\n",
        "  ax.legend(['Datos', 'Predicciones'])\n",
        "  plt.show()\n",
        "  display(predictions)\n",
        "  ################################################################\n",
        "def plot_predictions_dec2019(data_all, predictions_original):\n",
        "  separation = data_all.index[-3]\n",
        "  predictions_updated = pd.concat([data_all.loc['2019-10'], predictions_original])\n",
        "\n",
        "  # Crear una figura y un eje\n",
        "  fig, ax = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "  # Plotear toda la serie con un color\n",
        "  ax.plot(data_all.index, data_all.values, label='Datos', color='blue')\n",
        "\n",
        "  # Sobrescribir los últimos dos elementos con un color diferente\n",
        "  ax.plot(predictions_updated.index, predictions_updated.values, color='red', label='Predicciones')\n",
        "\n",
        "  # Linea de corte\n",
        "  ax.axvline(x=separation, color='black', linestyle='--', linewidth=0.5)\n",
        "\n",
        "  # Añadir texto para la fecha de corte\n",
        "  ax.text(separation, ax.get_ylim()[1], separation.strftime('%Y-%m'),\n",
        "            color='black', verticalalignment='bottom', horizontalalignment='right')\n",
        "\n",
        "  # Añadir una línea horizontal en el último valor de predictions_updated\n",
        "  ax.axhline(y=predictions_updated.iloc[-1], color='green', linestyle='--', linewidth=1)\n",
        "\n",
        "  # Calculo la Diferencia\n",
        "  diff =  np.round((predictions.iloc[-1] - data_all.iloc[-1]) / data_all.iloc[-1] * 100, 2)\n",
        "\n",
        "  # Añadir leyenda y mostrar el gráfico\n",
        "  ax.legend()\n",
        "  plt.title(f'Producto: {predictions.name}, Diferencia en la prediccion %{diff}')\n",
        "  plt.show()\n",
        "  print('Real value:', data_all.iloc[-1])\n",
        "  print(predictions)\n",
        "#############################################################################"
      ],
      "metadata": {
        "id": "vVoxNL0mP5od"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group Data"
      ],
      "metadata": {
        "id": "5L-kvH-J_7SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "# Siempre como values toma las toneladas vendidas\n",
        "def group_data(data, column):\n",
        "  grouped_data = data.groupby([column, 'periodo']).sum().reset_index()\n",
        "\n",
        "  # Crea un DataFrame pivoteado donde las filas son las fechas y las columnas son los product_id\n",
        "  pivot_data = grouped_data.pivot(index='periodo', columns=column, values='tn')\n",
        "\n",
        "  # Asegúrate de que los nombres de las columnas sean strings\n",
        "  pivot_data.columns = pivot_data.columns.astype(str)\n",
        "\n",
        "  # Restablece el índice para asegurarse de que 'product_id' no sea un índice compuesto\n",
        "  pivot_data.columns.name = None\n",
        "\n",
        "  return pivot_data"
      ],
      "metadata": {
        "id": "Mrkkv5Kd0yCG"
      },
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill & Fix Functions"
      ],
      "metadata": {
        "id": "p_XRfvor4fhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # QUE HACEMOS CON LOS NULL????\n",
        "###########################################################################\n",
        "def fill_missing_dates(df, pipeline_type):\n",
        "  # Genero coras del Dataframe Agrupado\n",
        "  start_date='2017-01-01'\n",
        "  if pipeline_type == 'Kaggle':\n",
        "    end_date = '2019-12-01'\n",
        "  elif pipeline_type == 'Local':\n",
        "    end_date = '2019-10-01'\n",
        "  else:\n",
        "    raise ValueError(\"El pipeline_type debe ser 'Kaggle' o 'Local'.\")\n",
        "\n",
        "  # Crear un rango de todas las fechas mensuales dentro del rango especificado\n",
        "  all_dates = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
        "  # Crear un DataFrame con todas las fechas\n",
        "  df_full = pd.DataFrame(all_dates, columns=['periodo'])\n",
        "  # Combinar el DataFrame original con el nuevo DataFrame de fechas\n",
        "  df_full = df_full.merge(df, on='periodo', how='left')\n",
        "  df_full.set_index('periodo', inplace=True)\n",
        "\n",
        "  # Identificar el primer y último índice con valores no nulos\n",
        "  first_valid_index = df_full.first_valid_index()\n",
        "  last_valid_index = df_full.last_valid_index()\n",
        "\n",
        "  # Rellenar los valores nulos intermedios con ceros\n",
        "  # df_full.loc[first_valid_index:last_valid_index] = df_full.loc[first_valid_index:last_valid_index].fillna(0)\n",
        "\n",
        "  # Imputer\n",
        "  imputer = KNNImputer()\n",
        "  df_imputed = imputer.fit_transform(df_full)\n",
        "  df_full = pd.DataFrame(df_imputed, columns=df_full.columns, index=df_full.index)\n",
        "\n",
        "  # display(df_full)\n",
        "  return df_full\n",
        "###########################################################################\n",
        "# Aca hay que ver que opciones mejores tenemos, en lugar de rellenar con ceros\n",
        "def fill_gruoped_data(df):\n",
        "  df_return = df.fillna(0)\n",
        "\n",
        "  return df_return\n",
        "###########################################################################\n",
        "def fix_grouped_data(df):\n",
        "    # Generar el rango completo de fechas desde enero 2017 hasta diciembre 2019\n",
        "    full_periods = pd.date_range(start='2017-01-01', end='2019-12-31', freq='MS')\n",
        "\n",
        "    # Asegurarse de que el índice es datetime\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Crear un DataFrame completo con todos los períodos y unirlo con el DataFrame original\n",
        "    full_df = pd.DataFrame(index=full_periods)\n",
        "    merged_df = pd.merge(full_df, df, left_index=True, right_index=True, how='left')\n",
        "\n",
        "    # Ordenar por fecha por si las fechas originales no estaban ordenadas\n",
        "    merged_df = merged_df.sort_index()\n",
        "\n",
        "    return merged_df\n",
        "###########################################################################\n",
        "def fix_holes(df):\n",
        "  \"\"\"\n",
        "  Recibe data, agrupa por producto, elimina los innecesarios, agrego los huecos.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame con las ventas bruto\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: data:grouped con las ventas agrupadas por producto\n",
        "  \"\"\"\n",
        "  # Creamos las listas de los productos dados de baja, y los nuevos\n",
        "  cant_productos = df['product_id'].unique().size\n",
        "  productos_baja, productos_alta = plot_products_sales(df, 0, cant_productos-1, False)\n",
        "\n",
        "  # Verificar cuáles productos de la lista de baja deben predecirse\n",
        "  productos_en_data = data_productos_a_predecir.index.isin(productos_baja)\n",
        "\n",
        "  # Filtrar los productos que están en la lista\n",
        "  productos_baja_en_data = data_productos_a_predecir.index[productos_en_data]\n",
        "\n",
        "  print(\"Productos con huecos en las ventasm, presentes en la lista de Productos a Predecir:\")\n",
        "  print(productos_baja_en_data)\n",
        "\n",
        "  # Generamos el Dataframe agrupando las ventas por producto\n",
        "  data_gruoped_fixed_holes = group_data(data, 'product_id')\n",
        "\n",
        "  # Arreglamos a manopla, todos analizados en el EDA\n",
        "  data_gruoped_fixed_holes['20089'].loc['2018-07'] = None\n",
        "  data_gruoped_fixed_holes['20192'] = data_gruoped_fixed_holes['20192'].ffill()\n",
        "  data_gruoped_fixed_holes['20313'] = data_gruoped_fixed_holes['20313'].bfill()\n",
        "  data_gruoped_fixed_holes['20426'] = data_gruoped_fixed_holes['20426'].bfill()\n",
        "  data_gruoped_fixed_holes['20456'].loc['2018-10'] = data_gruoped_fixed_holes['20456'].loc['2017-10'].values[0]*.9\n",
        "  data_gruoped_fixed_holes['20456'].loc['2018-11'] = data_gruoped_fixed_holes['20456'].loc['2017-11'].values[0]*.9\n",
        "  data_gruoped_fixed_holes['20469'].loc['2017-07'] = data_gruoped_fixed_holes['20469'].loc['2017-06'].values[0]\n",
        "  data_gruoped_fixed_holes['20596'].loc['2019-04'] = data_gruoped_fixed_holes['20596'].loc['2019-03'].values[0]*.8\n",
        "  data_gruoped_fixed_holes['20596'].loc['2019-05'] = data_gruoped_fixed_holes['20596'].loc['2019-03'].values[0]\n",
        "  data_gruoped_fixed_holes['20596'].loc['2019-06'] = data_gruoped_fixed_holes['20596'].loc['2019-03'].values[0]*.8\n",
        "  data_gruoped_fixed_holes['20641'].loc['2019-05'] = data_gruoped_fixed_holes['20641'].loc['2018-05'].values[0]*.9\n",
        "  data_gruoped_fixed_holes['20641'].loc['2019-06'] = data_gruoped_fixed_holes['20641'].loc['2018-06'].values[0]*.9\n",
        "  data_gruoped_fixed_holes['20666'].loc['2018-01'] = data_gruoped_fixed_holes['20666'].loc['2019-01'].values[0]*1.1\n",
        "  data_gruoped_fixed_holes['20666'].loc['2018-02'] = data_gruoped_fixed_holes['20666'].loc['2019-02'].values[0]*1.1\n",
        "  data_gruoped_fixed_holes['20786'].loc['2017-01'] = data_gruoped_fixed_holes['20786'].loc['2017-03'].values[0]\n",
        "  data_gruoped_fixed_holes['20786'].loc['2017-02'] = data_gruoped_fixed_holes['20786'].loc['2017-03'].values[0]\n",
        "  data_gruoped_fixed_holes['20793'].loc['2017-01'] = data_gruoped_fixed_holes['20793'].loc['2017-03'].values[0]*.5\n",
        "  data_gruoped_fixed_holes['20793'].loc['2017-02'] = data_gruoped_fixed_holes['20793'].loc['2017-03'].values[0]*.75\n",
        "  data_gruoped_fixed_holes['20824'].loc['2018-09'] = data_gruoped_fixed_holes['20824'].loc['2019-01'].values[0]*4\n",
        "  data_gruoped_fixed_holes['20824'].loc['2018-10'] = data_gruoped_fixed_holes['20824'].loc['2019-01'].values[0]*3\n",
        "  data_gruoped_fixed_holes['20824'].loc['2018-11'] = data_gruoped_fixed_holes['20824'].loc['2019-01'].values[0]*4\n",
        "  data_gruoped_fixed_holes['20824'].loc['2018-12'] = data_gruoped_fixed_holes['20824'].loc['2019-01'].values[0]*3\n",
        "  data_gruoped_fixed_holes['20879'] = data_gruoped_fixed_holes['20879'].bfill()\n",
        "  data_gruoped_fixed_holes['20885'].loc['2017-04'] = data_gruoped_fixed_holes['20885'].loc['2017-02'].values[0]\n",
        "  data_gruoped_fixed_holes['20885'].loc['2017-05'] = data_gruoped_fixed_holes['20885'].loc['2017-03'].values[0]\n",
        "  data_gruoped_fixed_holes['20936'].loc['2019-04'] = data_gruoped_fixed_holes['20936'].loc['2019-07'].values[0]*.25\n",
        "  data_gruoped_fixed_holes['20936'].loc['2019-05'] = data_gruoped_fixed_holes['20936'].loc['2019-07'].values[0]*.5\n",
        "  data_gruoped_fixed_holes['20936'].loc['2019-06'] = data_gruoped_fixed_holes['20936'].loc['2019-07'].values[0]*.75\n",
        "  data_gruoped_fixed_holes['21049'].loc['2018-05'] = data_gruoped_fixed_holes['21049'].loc['2018-07'].values[0]*.9\n",
        "  data_gruoped_fixed_holes['21049'].loc['2018-06'] = data_gruoped_fixed_holes['21049'].loc['2018-07'].values[0]*.9\n",
        "  # data_gruoped_fixed_holes['21170'].loc['2017-07'] = data_gruoped_fixed_holes['21170'].loc['2017-09'].values[0]*.9\n",
        "  # data_gruoped_fixed_holes['21170'].loc['2017-08'] = data_gruoped_fixed_holes['21170'].loc['2017-09'].values[0]*.9\n",
        "  data_gruoped_fixed_holes['21190'] = data_gruoped_fixed_holes['21190'].bfill()\n",
        "  data_gruoped_fixed_holes['21200'] = data_gruoped_fixed_holes['21200'].bfill()\n",
        "\n",
        "  return data_gruoped_fixed_holes\n",
        "###########################################################################\n",
        "def fix_aug2019(df_grouped, option):\n",
        "  if option == 'mean':\n",
        "    df_grouped_tmp = df_grouped.drop(index='2019-08', axis=1)\n",
        "    data_agosto_2019_mean = df_grouped_tmp.loc[['2019-07', '2019-09']].mean().to_frame().transpose()\n",
        "    data_agosto_2019_mean.index = pd.to_datetime(['2019-08-01'])\n",
        "    data_grouped_return = pd.concat([df_grouped_tmp, data_agosto_2019_mean]).sort_index()\n",
        "  elif option == 'drop':\n",
        "    data_grouped_return = df_grouped.drop(index='2019-08', axis=1)\n",
        "  elif option == 'julplus10':\n",
        "    df_grouped_tmp = df_grouped.drop(index='2019-08', axis=1)\n",
        "    data_agosto_2019_jul_plus10 = df_grouped_tmp.loc['2019-07']*1.1\n",
        "    data_agosto_2019_jul_plus10.index = pd.to_datetime(['2019-08-01'])\n",
        "    data_grouped_return = pd.concat([df_grouped_tmp, data_agosto_2019_jul_plus10]).sort_index()\n",
        "  else:\n",
        "    raise ValueError(\"Invalid option. Choose 'mean' or 'drop'.\")\n",
        "    data_grouped_return= None\n",
        "\n",
        "  return data_grouped_return\n",
        "###########################################################################\n",
        "def fill_nulls(df):\n",
        "  # Primero usamos bfill para completar las ordenes mas viejas con los valores de las ordenes mas recientes\n",
        "  df = df.bfill()\n",
        "  # Luego completamos con ceros los productos que dejamos de vender, o se discontinuaron\n",
        "  df = df.fillna(0)\n",
        "  return df\n",
        "###########################################################################\n",
        "## INVESTIGAR InterpolacionLineal y Media Movil\n",
        "# # Interpolación lineal\n",
        "# df_interpolated = df.interpolate(method='linear')\n",
        "\n",
        "# # Mostrar la serie con interpolación\n",
        "# print(\"\\nSerie con interpolación lineal:\")\n",
        "# print(df_interpolated)\n",
        "\n",
        "# # Relleno hacia adelante (forward fill)\n",
        "# df_ffill = df.fillna(method='ffill')\n",
        "\n",
        "# # Mostrar la serie con relleno hacia adelante\n",
        "# print(\"\\nSerie con relleno hacia adelante:\")\n",
        "# print(df_ffill)\n",
        "\n",
        "# # Relleno hacia atrás (backward fill)\n",
        "# df_bfill = df.fillna(method='bfill')\n",
        "\n",
        "# # Mostrar la serie con relleno hacia atrás\n",
        "# print(\"\\nSerie con relleno hacia atrás:\")\n",
        "# print(df_bfill)\n",
        "\n",
        "# # Relleno con la media móvil\n",
        "# window_size = 3\n",
        "# df['value'] = df['value'].fillna(df['value'].rolling(window=window_size, min_periods=1).mean())\n",
        "# OJO, por ahora no tiene en cuenta los productos de ALTA ni BAJA, solo arrega los HOLES\n",
        "###########################################################################"
      ],
      "metadata": {
        "id": "WPghARRT5HE3"
      },
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize Data"
      ],
      "metadata": {
        "id": "a2ctIs9UCRRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creo que solo come DataFrames\n",
        "###########################################################################\n",
        "def normalize_data(train, valid, normalization=\"MinMax\"):\n",
        "    if normalization == \"MinMax\":\n",
        "        scaler = MinMaxScaler()\n",
        "    elif normalization == \"ZScore\":\n",
        "        scaler = StandardScaler()\n",
        "    else:\n",
        "        raise ValueError(\"normalization parameter must be either 'MinMax' or 'ZScore'\")\n",
        "\n",
        "    # Normalizar el conjunto de entrenamiento\n",
        "    train_norm = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
        "\n",
        "    # Normalizar el conjunto de validación\n",
        "    valid_norm = pd.DataFrame(scaler.transform(valid), columns=valid.columns, index=valid.index)\n",
        "\n",
        "    if normalization == \"MinMax\":\n",
        "        normalization_params = pd.DataFrame({\n",
        "            'min': scaler.data_min_,\n",
        "            'max': scaler.data_max_\n",
        "        }, index=train.columns)\n",
        "    elif normalization == \"ZScore\":\n",
        "        normalization_params = pd.DataFrame({\n",
        "            'mean': scaler.mean_,\n",
        "            'std': scaler.scale_\n",
        "        }, index=train.columns)\n",
        "\n",
        "    return train_norm, valid_norm, normalization_params\n",
        "###########################################################################\n",
        "def denormalize_data(normalized_series, normalization_params, normalization=\"MinMax\"):\n",
        "    if normalization == \"MinMax\":\n",
        "        denormalized_data = normalized_series * (normalization_params['max'] - normalization_params['min']) + normalization_params['min']\n",
        "    elif normalization == \"ZScore\":\n",
        "        denormalized_data = normalized_series * normalization_params['std'] + normalization_params['mean']\n",
        "    else:\n",
        "        raise ValueError(\"normalization parameter must be either 'MinMax' or 'ZScore'\")\n",
        "\n",
        "    return denormalized_data\n",
        "###########################################################################"
      ],
      "metadata": {
        "id": "J2HbH2ew3yzL"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data"
      ],
      "metadata": {
        "id": "6z54dcDlIyHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "def split_data_timeseriessplit(df, n_splits):\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    splits = [(train_idx, test_idx) for train_idx, test_idx in tscv.split(df)]\n",
        "    return splits\n",
        "###############################################################################\n",
        "def split_data_fe(data_grouped, window_size, horizon):\n",
        "    if (window_size == 3) or (window_size== 6):\n",
        "        df_train = data_grouped.iloc[:-(window_size + horizon)]\n",
        "        df_valid = data_grouped.iloc[-(window_size + horizon):]\n",
        "        return df_train, df_valid\n",
        "    else:\n",
        "        print('Error: Verificar el split_data_fe')\n",
        "        return None, None\n",
        "###############################################################################\n",
        "def split_data(df, window_size):\n",
        "  if window_size == 10:\n",
        "    df_train = df.loc['2017-01':'2018-12']\n",
        "    df_valid = df.loc['2019-01':'2019-12']\n",
        "  elif window_size == 12:\n",
        "    df_train = df.loc['2017-01':'2018-10']\n",
        "    df_valid = df.loc['2018-11':'2019-12']\n",
        "  elif window_size == 6:\n",
        "    df_train = df.loc['2017-01':'2019-04']\n",
        "    df_valid = df.loc['2019-05':'2019-12']\n",
        "  elif window_size == 3:\n",
        "    df_train = df.loc['2017-01':'2019-07']\n",
        "    df_valid = df.loc['2019-08':'2019-12']\n",
        "  else:\n",
        "    raise ValueError(\"window_size must be different\")\n",
        "  return pd.DataFrame(df_train), pd.DataFrame(df_valid)\n",
        "###############################################################################\n",
        "def split_data_201807(df, window_size):\n",
        "  if window_size == 12:\n",
        "    df_train = df.loc['2017-01':'2017-12']\n",
        "    df_valid = df.loc['2018-01':'2018-05']\n",
        "  elif window_size == 6:\n",
        "    df_train = df.loc['2017-01':'2017-10']\n",
        "    df_valid = df.loc['2017-11':'2018-05']\n",
        "  elif window_size == 3:\n",
        "    df_train = df.loc['2017-01':'2018-07']\n",
        "    df_valid = df.loc['2018-03':'2018-07']\n",
        "  else:\n",
        "    raise ValueError(\"window_size must be different\")\n",
        "  return pd.DataFrame(df_train), pd.DataFrame(df_valid)\n",
        "###############################################################################\n",
        "def split_data_201805(df, window_size):\n",
        "  if window_size == 12:\n",
        "    df_train = df.loc['2017-01':'2017-12']\n",
        "    df_valid = df.loc['2018-01':'2018-05']\n",
        "  elif window_size == 6:\n",
        "    df_train = df.loc['2017-01':'2017-10']\n",
        "    df_valid = df.loc['2017-11':'2018-05']\n",
        "  elif window_size == 3:\n",
        "    df_train = df.loc['2017-01':'2017-12']\n",
        "    df_valid = df.loc['2018-01':'2018-05']\n",
        "  else:\n",
        "    raise ValueError(\"window_size must be different\")\n",
        "  return pd.DataFrame(df_train), pd.DataFrame(df_valid)\n",
        "\n",
        "#############################################################################\n",
        "# Hay que seguir probando con otras window (ej 6, 12)\n",
        "# y ver como no pincha el entrenamiento por quedarnos con pocos datos de validacion\n",
        "def split_data_dec2019(df, window_size):\n",
        "  if window_size == 3:\n",
        "    df_train = df.loc['2017-01':'2019-05']\n",
        "    df_valid = df.loc['2019-06':'2019-10']\n",
        "    df_test = df.loc['2019-11':'2019-12']\n",
        "  elif window_size == 4:\n",
        "    df_train = df.loc['2017-01':'2019-04']\n",
        "    df_valid = df.loc['2019-05':'2019-10']\n",
        "    df_test = df.loc['2019-11':'2019-12']\n",
        "  elif window_size == 6:\n",
        "    df_train = df.loc['2017-01':'2019-02']\n",
        "    df_valid = df.loc['2019-03':'2019-10']\n",
        "    df_test = df.loc['2019-11':'2019-12']\n",
        "  else:\n",
        "    raise ValueError(\"window_size must be different\")\n",
        "  return pd.DataFrame(df_train), pd.DataFrame(df_valid), pd.DataFrame(df_test)\n",
        "#############################################################################\n",
        "# split mas chico posible, con window 3 y batch 1\n",
        "def split_data_test(df):\n",
        "  df_train = df.loc['2017-01':'2019-07']\n",
        "  df_valid = df.loc['2019-08':'2019-12']\n",
        "  return pd.DataFrame(df_train), pd.DataFrame(df_valid)\n",
        "############################################################################\n",
        "# # No se si este split tiene sentido, pierdo muchos datos de entrenamiento\n",
        "# def split_data_2019(df):\n",
        "#   df_train = df.loc['2017-01':'2018-12']\n",
        "#   df_valid = df.loc['2019-01':'2019-10']\n",
        "#   df_test =  df.loc['2019-11':'2019-12']\n",
        "#   return df_train, df_valid, df_test\n",
        "############################################################################"
      ],
      "metadata": {
        "id": "zZUKfILzC6Un"
      },
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Window Data"
      ],
      "metadata": {
        "id": "xbYFP2j6EiEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def windowed_dataset(sequence, data_split, window_size, horizon, batch_size, shuffle_buffer=1000):\n",
        "    \"\"\"Generates dataset windows.\n",
        "\n",
        "    Args:\n",
        "      sequence (array-like): Contains the values of the time series.\n",
        "      data_split (str): Specifies if the dataset is for training or validation/test.\n",
        "      window_size (int): The number of time steps to include in the feature.\n",
        "      horizon (int): The number of future time steps to predict.\n",
        "      batch_size (int): The batch size.\n",
        "      shuffle_buffer (int): Buffer size to use for the shuffle method.\n",
        "\n",
        "    Returns:\n",
        "      tf.data.Dataset: TF Dataset containing time windows.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "\n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size + horizon, shift=1, drop_remainder=True)\n",
        "\n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + horizon))\n",
        "\n",
        "    # Create tuples with features and labels\n",
        "    dataset = dataset.map(lambda window: (window[:-horizon], window[-horizon:]))\n",
        "\n",
        "    if data_split == 'train':\n",
        "        # Shuffle the training data to improve generalization\n",
        "        dataset = dataset.shuffle(shuffle_buffer)\n",
        "    else:\n",
        "        # Cache the validation/test data for improved performance\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "    # Create batches of windows and prefetch for performance\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "li3XXSO0YOJv"
      },
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "R4ZtLl9-YjWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "# Tengo que decidir que hacer con los nulos\n",
        "def feature_engineering(df):\n",
        "  product_id = df.columns[0]\n",
        "  antiguedad_inicial = df.shape[0]\n",
        "  num_total_elements = len(df[product_id])\n",
        "  num_unique_elements = df[product_id].nunique()\n",
        "\n",
        "# #########################################################################################\n",
        "# # 07/13 - Agrego Antigueadad a FE\n",
        "# #########################################################################################\n",
        "#   if num_total_elements == num_unique_elements:\n",
        "#     # print(\"Todos los elementos de la columna 'producto' son distintos.\")\n",
        "#     df['old'] = np.arange(antiguedad_inicial, antiguedad_inicial + df.shape[0])\n",
        "#   else:\n",
        "#     # print(\"No todos los elementos de la columna 'producto' son distintos.\")\n",
        "#     serie = df[product_id]\n",
        "#     duplicados = serie.duplicated()\n",
        "#     ultima_posicion_duplicada = np.where(duplicados)[0][-1]\n",
        "#     df['old'] = 0\n",
        "#     n = df.shape[0] - ultima_posicion_duplicada\n",
        "#     print(n)\n",
        "#     for i in range(n):\n",
        "#         df.iloc[ultima_posicion_duplicada + i, df.columns.get_loc('old')] = i\n",
        "#     # print(f\"La ultima posición con un valor duplicado es: {ultima_posicion_duplicada}\")\n",
        "# #########################################################################################\n",
        "\n",
        "  data_grouped = fix_aug2019(df, 'mean')\n",
        "  data_grouped.index.name = 'periodo'\n",
        "  data_grouped = data_grouped.asfreq('MS')\n",
        "\n",
        "  data_grouped['month'] = data_grouped.index.month\n",
        "  data_grouped['quarter'] = data_grouped.index.quarter\n",
        "  data_grouped['year'] = data_grouped.index.year\n",
        "\n",
        "  for lag in range(1, 13):  # Creación de 12 lags\n",
        "      data_grouped[f'lag_{lag}'] = data_grouped[product_id].shift(lag)\n",
        "\n",
        "  data_grouped['rolling_mean_3'] = data_grouped[product_id].rolling(window=3).mean()\n",
        "  data_grouped['rolling_std_3'] = data_grouped[product_id].rolling(window=3).std()\n",
        "  data_grouped['rolling_mean_6'] = data_grouped[product_id].rolling(window=6).mean()\n",
        "  data_grouped['rolling_std_6'] = data_grouped[product_id].rolling(window=6).std()\n",
        "  data_grouped['rolling_mean_12'] = data_grouped[product_id].rolling(window=12).mean()\n",
        "  data_grouped['rolling_std_12'] = data_grouped[product_id].rolling(window=12).std()\n",
        "\n",
        "  # One-Hot Encoding para month, quarter y year\n",
        "  data_grouped = pd.get_dummies(data_grouped, columns=['year', 'month', 'quarter']).astype(float)\n",
        "\n",
        "  return data_grouped.dropna()\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "wGIWhbdeYleb"
      },
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Functions"
      ],
      "metadata": {
        "id": "x_Zm-EgneATh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(model_trained, data_valid, scaler):\n",
        "  data_valid_array = data_valid.values\n",
        "  column_names = data_valid.columns\n",
        "  input_data = data_valid_array[-window_size:].reshape((1, window_size, n_features))\n",
        "  pred = model_trained.predict(input_data)\n",
        "  pred = pred.reshape((1, horizon, n_features))\n",
        "  pred_df = pd.DataFrame(pred[0], columns=column_names)\n",
        "  pred_df.index = pd.date_range(start='2020-01-01', periods=horizon, freq='MS')\n",
        "\n",
        "  pred_df_feb = pd.DataFrame(pred_df.loc['2020-02-01']).T\n",
        "\n",
        "  pred_def_feb_desnorm = scaler.inverse_transform(pred_df_feb)\n",
        "  pred_def_feb_desnorm_df = pd.DataFrame(pred_def_feb_desnorm, columns=pred_df_feb.columns).T\n",
        "  pred_def_feb_desnorm_df.rename(columns={0:'tn'}, inplace=True)\n",
        "\n",
        "  # display(pred_def_feb_desnorm_df)\n",
        "  return pred_def_feb_desnorm_df.squeeze()\n",
        "##############################################################\n",
        "# # Este va a reemplazar al otro\n",
        "def generate_predictions_4(model_trained, data_valid_norm, data_norm_params):\n",
        "  data_norm_array = data_valid_norm.values\n",
        "  column_names = data_valid_norm.columns\n",
        "  input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "  pred = model_trained.predict(input_data)\n",
        "  pred = pred.reshape((1, horizon, n_features))\n",
        "  pred_df = pd.DataFrame(pred[0], columns=column_names)\n",
        "  pred_df.index = pd.date_range(start=data_valid_norm.index[-1] + pd.DateOffset(months=1), periods=horizon, freq='MS')\n",
        "\n",
        "  # Generamos la salida de la segunda prediccion, Febrero 2020 (Mes +2 del ultimo mes en data_validation_norm)\n",
        "  pred_plus2 = pred_df.iloc[1]\n",
        "  pred_plus2_denorm = denormalize_data(pred_plus2, data_norm_params, normalization=normalization)\n",
        "  # pred_plus2_denorm = pd.Series(pred_plus2_denorm, name=str(column_names.values[0]))\n",
        "  # pred_plus2_df = pd.DataFrame(pred_plus2_denorm)\n",
        "  # pred_plus2_df['periodo'] = pd.to_datetime(pred_df.iloc[1].name)\n",
        "  # pred_plus2_df.set_index('periodo', inplace=True)\n",
        "\n",
        "  # # Concateno las dos predicciones\n",
        "  # pred_final_df = pd.concat([pred_plus1_df,pred_plus2_df])\n",
        "\n",
        "  # # En lugar de DF, devuelvo una serie, para que matchee con el input\n",
        "  # pred_final_serie = pred_final_df.squeeze()\n",
        "\n",
        "  return pred_plus2_denorm\n",
        "##########################################################################\n",
        "# # Este va a reemplazar al otro\n",
        "def generate_predictions_3(model_trained, data_valid_norm, data_norm_params):\n",
        "  data_norm_array = data_valid_norm.values\n",
        "  column_names = data_valid_norm.columns\n",
        "  input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "  pred = model_trained.predict(input_data)\n",
        "  pred = pred.reshape((1, horizon, n_features))\n",
        "  pred_df = pd.DataFrame(pred[0], columns=column_names)\n",
        "  pred_df.index = pd.date_range(start=data_valid_norm.index[-1] + pd.DateOffset(months=1), periods=horizon, freq='MS')\n",
        "\n",
        "  # # Generamos la salida de la primer prediccion, Enero 2020 (Mes +1 del ultimo mes en data_validation_norm)\n",
        "  # pred_plus1 = pred_df.iloc[0]\n",
        "  # pred_plus1_denorm = denormalize_data(pred_plus1, data_norm_params, normalization=normalization)\n",
        "  # pred_plus1_denorm = pd.Series(pred_plus1_denorm, name=str(column_names.values[0]))\n",
        "  # pred_plus1_df = pd.DataFrame(pred_plus1_denorm)\n",
        "  # pred_plus1_df['periodo'] = pd.to_datetime(pred_df.iloc[0].name)\n",
        "  # pred_plus1_df.set_index('periodo', inplace=True)\n",
        "\n",
        "  # Generamos la salida de la segunda prediccion, Febrero 2020 (Mes +2 del ultimo mes en data_validation_norm)\n",
        "  pred_plus2 = pred_df.iloc[1]\n",
        "  pred_plus2_denorm = denormalize_data(pred_plus2, data_norm_params, normalization=normalization)\n",
        "  # pred_plus2_denorm = pd.Series(pred_plus2_denorm, name=str(column_names.values[0]))\n",
        "  # pred_plus2_df = pd.DataFrame(pred_plus2_denorm)\n",
        "  # pred_plus2_df['periodo'] = pd.to_datetime(pred_df.iloc[1].name)\n",
        "  # pred_plus2_df.set_index('periodo', inplace=True)\n",
        "\n",
        "  # # Concateno las dos predicciones\n",
        "  # pred_final_df = pd.concat([pred_plus1_df,pred_plus2_df])\n",
        "\n",
        "  # # En lugar de DF, devuelvo una serie, para que matchee con el input\n",
        "  # pred_final_serie = pred_final_df.squeeze()\n",
        "\n",
        "  return pred_plus2_denorm\n",
        "##########################################################################\n",
        "# # Este va a reemplazar al otro\n",
        "def generate_predictions_2(model_trained, data_valid_norm, data_norm_params):\n",
        "  data_norm_array = data_valid_norm.values\n",
        "  column_names = data_valid_norm.columns\n",
        "  input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "  pred = model_trained.predict(input_data)\n",
        "  pred = pred.reshape((1, horizon, n_features))\n",
        "  pred_df = pd.DataFrame(pred[0], columns=column_names)\n",
        "  pred_df.index = pd.date_range(start=data_valid_norm.index[-1] + pd.DateOffset(months=1), periods=horizon, freq='MS')\n",
        "\n",
        "  # Generamos la salida de la primer prediccion, Enero 2020 (Mes +1 del ultimo mes en data_validation_norm)\n",
        "  pred_plus1 = pred_df.iloc[0]\n",
        "  pred_plus1_denorm = denormalize_data(pred_plus1, data_norm_params, normalization=normalization)\n",
        "  pred_plus1_denorm = pd.Series(pred_plus1_denorm, name=str(column_names.values[0]))\n",
        "  pred_plus1_df = pd.DataFrame(pred_plus1_denorm)\n",
        "  pred_plus1_df['periodo'] = pd.to_datetime(pred_df.iloc[0].name)\n",
        "  pred_plus1_df.set_index('periodo', inplace=True)\n",
        "\n",
        "  # Generamos la salida de la segunda prediccion, Febrero 2020 (Mes +2 del ultimo mes en data_validation_norm)\n",
        "  pred_plus2 = pred_df.iloc[1]\n",
        "  pred_plus2_denorm = denormalize_data(pred_plus2, data_norm_params, normalization=normalization)\n",
        "  pred_plus2_denorm = pd.Series(pred_plus2_denorm, name=str(column_names.values[0]))\n",
        "  pred_plus2_df = pd.DataFrame(pred_plus2_denorm)\n",
        "  pred_plus2_df['periodo'] = pd.to_datetime(pred_df.iloc[1].name)\n",
        "  pred_plus2_df.set_index('periodo', inplace=True)\n",
        "\n",
        "  # Concateno las dos predicciones\n",
        "  pred_final_df = pd.concat([pred_plus1_df,pred_plus2_df])\n",
        "\n",
        "  # En lugar de DF, devuelvo una serie, para que matchee con el input\n",
        "  pred_final_serie = pred_final_df.squeeze()\n",
        "\n",
        "  return pred_final_serie\n",
        "##########################################################################\n",
        "def generate_predictions_OLD(data_norm, data_norm_params, export_csv):\n",
        "  data_norm_array = data_norm.values\n",
        "  column_names = data_norm.columns\n",
        "  input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "  pred = model.predict(input_data)\n",
        "  pred = pred.reshape((1, horizon, n_features))\n",
        "  pred_df = pd.DataFrame(pred[0], columns=column_names)\n",
        "  pred_df.index = pd.date_range(start='2020-01-01', periods=horizon, freq='MS')\n",
        "  pred_feb = pred_df.loc['2020-02-01']\n",
        "  pred_denorm = denormalize_data(pred_feb, norm_params, normalization)\n",
        "  pred_denorm = pred_denorm.reset_index()\n",
        "  pred_denorm.columns = ['product_id', 'tn']\n",
        "  # display(pred_denorm)\n",
        "  # # Esto no creo que sea necesario\n",
        "  # predicciones = filter_data(data_pred1_denorm, data_productos_a_predecir)\n",
        "\n",
        "  filename = f\"{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "\n",
        "  if export_csv:\n",
        "    pred_denorm.to_csv(filename, header=True, index=False)\n",
        "\n",
        "  # print(filename)\n",
        "\n",
        "  return pred_denorm['tn']\n",
        "##########################################################################\n",
        "def to_kaggle(serie, name='kaggle_submission'):\n",
        "  serie.columns = ['product_id', 'tn']\n",
        "  serie.to_csv(f'{name}.csv', header=True, index=False)\n",
        "##########################################################################\n",
        "def to_kaggle_2(serie, name='kaggle_submission'):\n",
        "    df = serie.reset_index()  # Convierte el índice en una columna\n",
        "    df.columns = ['product_id', 'tn']  # Asigna nombres a las columnas\n",
        "    path = 'drive//MyDrive//Colab Notebooks//Data//Labo3//'\n",
        "    df.to_csv(f'{path}{name}.csv', header=True, index=False)  # Exporta el DataFrame a CSV\n",
        "##########################################################################\n",
        "def calcular_error(df):\n",
        "  error = np.divide(np.abs(predictions_df['true'] - predictions_df['predicted']).sum(), predictions_df['true'].sum()).round(4)\n",
        "  print(error)\n",
        "##########################################################################\n",
        "def sumar_predicciones(df1, df2):\n",
        "    # Asegúrate de que las columnas necesarias estén en los DataFrames\n",
        "    if 'product_id' not in df1.columns or 'tn' not in df1.columns:\n",
        "        raise ValueError(\"df1 debe contener las columnas 'product_id' y 'tn'\")\n",
        "    if 'product_id' not in df2.columns or 'tn' not in df2.columns:\n",
        "        raise ValueError(\"df2 debe contener las columnas 'product_id' y 'tn'\")\n",
        "\n",
        "    # Suma los valores de 'tn' para cada 'product_id' de ambos DataFrames\n",
        "    result = df1.set_index('product_id').add(df2.set_index('product_id'), fill_value=0).reset_index()\n",
        "\n",
        "    return result\n",
        "##########################################################################\n",
        "def sumar_series(series1, series2):\n",
        "    # Suma los valores de ambas Series\n",
        "    result = series1.add(series2, fill_value=0)\n",
        "\n",
        "    # Ordena el resultado por el índice (product_id)\n",
        "    result = result.sort_index()\n",
        "\n",
        "    return result\n",
        "\n",
        "###########################################################################\n",
        "def comparar_arrays(array1, array2):\n",
        "    set1 = set(array1)\n",
        "    set2 = set(array2)\n",
        "\n",
        "    # Elementos en array1 pero no en array2\n",
        "    diferencia1 = set1 - set2\n",
        "    # diferencia1_array = np.sort(np.array(list(diferencia1)))\n",
        "    # print('Elementos en array1 pero no en array2', diferencia1_array, '\\n')\n",
        "\n",
        "    # Elementos en array2 pero no en array1\n",
        "    diferencia2 = set2 - set1\n",
        "    # print('Elementos en array2 pero no en array1', diferencia2)\n",
        "\n",
        "    return list(diferencia1)#, diferencia2\n",
        "##########################################################################\n",
        "def check_preddictions_files(path_predicciones):\n",
        "  # customers_ids = group_data(data, 'customer_id').loc['2019'].sum().index\n",
        "\n",
        "  data_filtrada = data[data['product_id'].isin(data_productos_a_predecir.index)]\n",
        "  customers_ids =  data_filtrada['customer_id'].unique()\n",
        "\n",
        "\n",
        "  nombres_archivos = []\n",
        "  for archivo in os.listdir(path_predicciones):\n",
        "    nombre, extension = os.path.splitext(archivo)\n",
        "    if extension:\n",
        "      nombres_archivos.append(nombre)\n",
        "    else:\n",
        "      nombres_archivos.append(archivo)  # si no hay extensión, agrega el nombre completo\n",
        "\n",
        "  faltantes = comparar_arrays(customers_ids, nombres_archivos)\n",
        "  faltantes.sort()\n",
        "  print((faltantes))\n",
        "  # print('Elementos xxx:\\n', faltantes)\n",
        "##########################################################################\n",
        "def generate_predictions_from_files(directory_path):\n",
        "    # Obtener la lista de archivos .csv en el directorio, sin incluir subcarpetas\n",
        "    files = [f for f in os.listdir(directory_path) if f.endswith('.csv') and os.path.isfile(os.path.join(directory_path, f))]\n",
        "\n",
        "    # Crear un diccionario para almacenar los DataFrames temporales\n",
        "    dataframes = {}\n",
        "\n",
        "    for file in files:\n",
        "        # Leer cada archivo .csv en un DataFrame temporal\n",
        "        temp_df = pd.read_csv(os.path.join(directory_path, file))\n",
        "\n",
        "        # Obtener el nombre base del archivo (sin extensión) para usar como nombre de la columna\n",
        "        col_name = os.path.splitext(file)[0]\n",
        "\n",
        "        # Agregar el DataFrame temporal al diccionario\n",
        "        dataframes[col_name] = temp_df['tn']\n",
        "\n",
        "    # Crear un DataFrame único donde cada columna es un archivo .csv diferente\n",
        "    result_df = pd.DataFrame(dataframes)\n",
        "\n",
        "    # Agregar la columna 'product_id' desde el primer DataFrame temporal\n",
        "    result_df['product_id'] = temp_df['product_id']\n",
        "\n",
        "    # Reordenar las columnas para que 'product_id' sea la primera columna\n",
        "    cols = ['product_id'] + [col for col in result_df.columns if col != 'product_id']\n",
        "    result_df = result_df[cols]\n",
        "    result_df.set_index('product_id', inplace=True)\n",
        "\n",
        "    return result_df\n",
        "##########################################################################"
      ],
      "metadata": {
        "id": "jmLuXgOGeC57"
      },
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Sales"
      ],
      "metadata": {
        "id": "Tsi7tkBEgtWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_sales(df1, df2):\n",
        "    # Crear un rango de fechas desde enero 2017 hasta diciembre 2019\n",
        "    fechas_completas = pd.date_range(start='2017-01-01', end='2019-12-01', freq='MS')\n",
        "\n",
        "    # Reindexar el DataFrame para asegurar que todas las fechas estén presentes\n",
        "    df1 = df1.reindex(fechas_completas, fill_value=pd.NA)\n",
        "\n",
        "    # Obtener los product_id del primer DataFrame (nombres de las columnas)\n",
        "    product_ids_df1 = df1.columns.tolist()\n",
        "\n",
        "    # Obtener los product_id del segundo DataFrame (valores en la columna 'product_id')\n",
        "    product_ids_df2 = df2['product_id'].tolist()\n",
        "\n",
        "    # Identificar los product_id que faltan en df1\n",
        "    product_ids_faltantes = [pid for pid in product_ids_df2 if pid not in product_ids_df1]\n",
        "\n",
        "    # Crear un DataFrame con las columnas faltantes y valores NaN\n",
        "    df_faltantes = pd.DataFrame(index=df1.index, columns=product_ids_faltantes)\n",
        "\n",
        "    # Concatenar el DataFrame original con el DataFrame de faltantes\n",
        "    df_resultante = pd.concat([df1, df_faltantes], axis=1)\n",
        "\n",
        "    return df_resultante\n",
        "\n"
      ],
      "metadata": {
        "id": "aSHUuvHXgtf8"
      },
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks"
      ],
      "metadata": {
        "id": "brfkj2XkXena"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def MyCallbacks(patience):\n",
        "    \"\"\"\n",
        "    Devuelve una lista de callbacks para el entrenamiento del modelo.\n",
        "\n",
        "    Parameters:\n",
        "    patience (int): Número de épocas a esperar para ver una mejora en 'val_loss' antes de detener el entrenamiento.\n",
        "\n",
        "    Returns:\n",
        "    list: Lista de callbacks de Keras.\n",
        "    \"\"\"\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "    return [early_stop]"
      ],
      "metadata": {
        "id": "fcfAKyAS_iD2"
      },
      "execution_count": 359,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "#### VIEJO\n",
        "# #############################################################################\n",
        "# class MAEThresholdCallback(Callback):\n",
        "#     def __init__(self, threshold=0.15):\n",
        "#         super(MAEThresholdCallback, self).__init__()\n",
        "#         self.threshold = threshold\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "#         val_mae = logs.get('val_mae')\n",
        "#         if val_mae is not None and val_mae <= self.threshold:\n",
        "#             print(f'\\nEpoch {epoch+1}: Validation MAE has reached {val_mae:.4f}, stopping training.')\n",
        "#             self.model.stop_training = True\n",
        "\n",
        "# def MyCallbacks(model_name, patience):\n",
        "#     earlystop = tf.keras.callbacks.EarlyStopping('val_loss', patience=patience, restore_best_weights=True)\n",
        "#     # checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=f'ckpts/{model_name}-' + '{epoch:02d}-{val_loss:.4f}.h5', monitor='val_loss')\n",
        "#     # mae_threshold_callback = MAEThresholdCallback(threshold=0.015)\n",
        "#     return [earlystop] #, checkpoint] #, mae_threshold_callback]\n",
        "\n",
        "# #############################################################################"
      ],
      "metadata": {
        "id": "ETzh0JyBXgRt"
      },
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Design"
      ],
      "metadata": {
        "id": "cGAz7W4mXqO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def compile_model(new_model, loss, optimizer):\n",
        "    new_model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
        "    # print(new_model.summary())\n",
        "    return new_model\n",
        "#############################################################################\n",
        "def MyModel(loss, optimizer, window_size, horizon, n_features):\n",
        "    new_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer((window_size, n_features)),\n",
        "        # tf.keras.layers.Masking(mask_value=0),  # Assuming that 0 is used for padding/missing values\n",
        "        tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='causal'),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=False)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(n_features * horizon, activation='relu'),\n",
        "        tf.keras.layers.Reshape((horizon, n_features)),\n",
        "    ])\n",
        "    return compile_model(new_model, loss, optimizer)\n",
        "#############################################################################"
      ],
      "metadata": {
        "id": "v1XojStQ3FQw"
      },
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Train"
      ],
      "metadata": {
        "id": "7uqfuDfTJD8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repetimos la prediccion n veces\n",
        "def model_train(epochs, iteraciones):\n",
        "  pred_list = []\n",
        "\n",
        "  for i in range(iteraciones):\n",
        "    print(f'Interacion {i+1}')\n",
        "    history = model.fit(\n",
        "        data_train_windowed,\n",
        "        validation_data = data_valid_windowed,\n",
        "        callbacks = callbacks,\n",
        "        verbose=0,\n",
        "        epochs=epochs)\n",
        "\n",
        "    predicted = generate_predictions(data_valid_norm, norm_params, False)\n",
        "    print(f'Prediction: {predicted}')\n",
        "    pred_list.append(generate_predictions(data_valid_norm, norm_params, False))\n",
        "\n",
        "    plot_history(history)\n",
        "  print('Producto: ', producto)\n",
        "  print(f'Mean Loss across all splits: {np.mean(pred_list)}')\n",
        "  print(f'Median Loss across all splits: {np.median(pred_list)}')\n",
        "\n",
        "  return(np.mean(pred_list), np.median(pred_list))\n",
        "\n"
      ],
      "metadata": {
        "id": "DSrrGEGPJFyO"
      },
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Pipeline Principal"
      ],
      "metadata": {
        "id": "F4yiWWu8FJZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parametros Generales y Filtro de Datos"
      ],
      "metadata": {
        "id": "HWg00hIFX64c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Dataframe raw:', data.shape)\n",
        "\n",
        "###########################################################################\n",
        "# Parametros Generales\n",
        "\n",
        "# Indica como hace el corte de los datos en Train, Valod y Test\n",
        "# Importante para que no pinche el EarlyStop de mi Red\n",
        "pipeline_type = 'Local' # Kaggle: 2019-10 | Local: 2019-12\n",
        "\n",
        "# Instancia, si se corre en mas de una maquina a lavez\n",
        "instance = A\n",
        "\n",
        "# Cuantas unidades de tiempo hacia atras usamos para crear el Windowed Dataframe\n",
        "window_size = 3 # Probar con 6 tambien\n",
        "\n",
        "# Predecimos el mes +2, osea 2019-12 (Local) o 2020-02 (Kaggle)\n",
        "horizon = 2\n",
        "\n",
        "# Normalization\n",
        "normalization = \"MinMax\"\n",
        "\n",
        "# Batch size para el entrenamiento del modelo\n",
        "# No tuvimos tiempo para experimentar con otros valores\n",
        "batch_size = 32\n",
        "###########################################################################\n",
        "# Filtros Iniciales\n",
        "\n",
        "# Solo tenemos en cuenta los 780 productos a predecir\n",
        "data =filter_products_data(data, data_productos_a_predecir)\n",
        "print('Filtro Productos a Predecir:', data.shape)\n",
        "\n",
        "\n",
        "# Si quiero solo las ventas de los clientes activos\n",
        "data = filter_active_clients_data(data)\n",
        "print('Filtro Clientes Activos:', data.shape)\n",
        "\n",
        "\n",
        "# Ploteamos para ver como quedan las ventas totales acumuladas\n",
        "group_data(data, 'product_id').sum(axis=1).plot(figsize=(15, 5))\n",
        "plt.title('Ventas totales en Toneladas')\n",
        "plt.show()\n",
        "###########################################################################\n",
        "# Decomisionado\n",
        "\n",
        "# # # ACTUALIZAR\n",
        "# data = fix_holes(data) # Falta vacios al principioy final\n",
        "# print(data.shape)\n",
        "\n",
        "# # # ACTUALIZAR\n",
        "# # Estrategia para completar los NaN de los datos agrupados\n",
        "# # data_grouped = fill_nulls(data_grouped)\n",
        "\n",
        "# # Estrategia para solucionar el problema de Agosto 2019\n",
        "# data = fix_aug2019(data, 'julplus10')\n",
        "###########################################################################\n",
        "\n",
        "# Hago un backup del Dataframe con los filtrados que quiero\n",
        "data_filtered = data.copy()"
      ],
      "metadata": {
        "id": "vLrQTz5YEkPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "92842fea-7577-499b-b5f6-ce2111fdeaf7"
      },
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe raw: (2945818, 12)\n",
            "Filtro Productos a Predecir: (2293481, 12)\n",
            "Filtro Clientes Activos: (1968598, 12)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generador de Predicciones"
      ],
      "metadata": {
        "id": "jmAbT059s9lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# En esta celda estsa todo el codigo que utilizamos para la generacion de nuestras predicciones\n",
        "# Deberiamos haber utilizado distintas semillas, para haber facilitado mas la reproducibilidad del experimento\n",
        "# Indicar el product_id de inicio y de fin, y generara al menos un archivo con las predicciones para ese rango\n",
        "\n",
        "\n",
        "# Acotamos el bucle, solo los productos que nos interesa\n",
        "product_id_start = '20001'\n",
        "product_id_end = '20001'\n",
        "start_index = data_productos_a_predecir.index.get_loc(product_id_start)\n",
        "end_index = data_productos_a_predecir.index.get_loc(product_id_end) + 1  # +1 to include the end inde\n",
        "\n",
        "\n",
        "# Inicializamos Dataframe de Predicciones\n",
        "product_predictions = data_productos_a_predecir.copy()\n",
        "product_predictions['Real'] = None\n",
        "product_predictions['MinMax'] = None\n",
        "product_predictions['Robust'] = None\n",
        "\n",
        "# Bucle principal de Productos\n",
        "for producto in data_productos_a_predecir.index[start_index:end_index]:\n",
        "  # Busco el valor real de Dec 2019 en mi Dataframe original\n",
        "  print('Producto:', producto)\n",
        "  valor_dec_19 = group_data(data_bkp.query('periodo == \"2019-12-01\"'), 'product_id')[producto].values[0]\n",
        "  product_predictions.loc[producto, 'Real'] = valor_dec_19\n",
        "\n",
        "  # Filtro por producto\n",
        "  data_product = data.query(\"product_id == @producto\")\n",
        "  # display(data_product.tail())\n",
        "\n",
        "  # Agrupo el Dataframe\n",
        "  data_grouped = group_data(data_product, 'product_id')\n",
        "\n",
        "  # Arreglo fechas en productos jovenes, o discontinuados\n",
        "  data_grouped = fill_missing_dates(data_grouped, pipeline_type)\n",
        "\n",
        "  # Arreglo Agosto 2019\n",
        "  data_grouped = fix_aug2019(data_grouped, 'mean')\n",
        "\n",
        "  # Generacion de variables adicionales\n",
        "  data_grouped = feature_engineering(data_grouped)\n",
        "  display(data_grouped)\n",
        "\n",
        "  # Cant de veces que se repite cada experimento con los mismos parametros, pero sin semilla\n",
        "  n = 1\n",
        "\n",
        "  # Split. No pude hacer funcionar el TimeSeriesSplit\n",
        "  data_train, data_valid = split_data_fe(data_grouped, window_size, horizon)\n",
        "  # display(data_train)\n",
        "  # display(data_valid)\n",
        "\n",
        "  # Generamos predicciones, pero utilizando dos Normalizadores distintos\n",
        "  ##########################################################################\n",
        "  # Robust Scaler primero\n",
        "\n",
        "  print('Robust')\n",
        "  scaler = RobustScaler() # Probar con transformaciones no-lineales\n",
        "  data_train_norm = pd.DataFrame(scaler.fit_transform(data_train), columns=data_train.columns, index=data_train.index)\n",
        "  data_valid_norm = pd.DataFrame(scaler.transform(data_valid), columns=data_valid.columns, index=data_valid.index)\n",
        "\n",
        "  predictions_list = []\n",
        "\n",
        "  for i in range(n):\n",
        "    # Creo los Windowed Datasets\n",
        "    data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "    data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "    # Parmetros del modelo\n",
        "    loss = 'mse'\n",
        "    optimizer = 'adam'\n",
        "    patience = 20\n",
        "    epochs = 500\n",
        "    n_features = data_train.shape[1]\n",
        "    callbacks = MyCallbacks(patience)\n",
        "\n",
        "    # Modelo\n",
        "    model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "    history = model.fit(\n",
        "        data_train_windowed,\n",
        "        validation_data=data_valid_windowed,\n",
        "        callbacks=callbacks,\n",
        "        verbose=0,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    # plot_history(history) # Visualizamos el entrenamiento del modelo\n",
        "\n",
        "    # Para ver como esta compuesto el vector de predicciones\n",
        "    print(generate_predictions(model, data_valid_norm, scaler))\n",
        "\n",
        "    # Genero la prediccion, y la agrego al vector\n",
        "    prediction = generate_predictions(model, data_valid_norm, scaler)[1]\n",
        "    predictions_list.append(prediction)\n",
        "\n",
        "    # Liberar memoria\n",
        "    del model\n",
        "    del data_train_windowed\n",
        "    del data_valid_windowed\n",
        "    clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "  # display(data_grouped.loc['2019-12'])\n",
        "  # print('¨Prediccion:', np.mean(predictions_list))\n",
        "  product_predictions.loc[producto, 'Robust'] = np.mean(predictions_list)\n",
        "  ##########################################################################\n",
        "  # MinMax Scaler segundo\n",
        "\n",
        "  print('MinMax')\n",
        "  scaler = MinMaxScaler() # Probar con transformaciones no-lineales\n",
        "  data_train_norm = pd.DataFrame(scaler.fit_transform(data_train), columns=data_train.columns, index=data_train.index)\n",
        "  data_valid_norm = pd.DataFrame(scaler.transform(data_valid), columns=data_valid.columns, index=data_valid.index)\n",
        "\n",
        "  predictions_list = []\n",
        "\n",
        "  for i in range(n):\n",
        "    # Creo los Windowed Datasets\n",
        "    data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "    data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "    # Parmetros del modelo\n",
        "    loss = 'mse'\n",
        "    optimizer = 'adam'\n",
        "    patience = 20\n",
        "    epochs = 500\n",
        "    n_features = data_train.shape[1]\n",
        "    callbacks = MyCallbacks(patience)\n",
        "\n",
        "    # Modelo\n",
        "    model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "    history = model.fit(\n",
        "        data_train_windowed,\n",
        "        validation_data=data_valid_windowed,\n",
        "        callbacks=callbacks,\n",
        "        verbose=0, # Poner en 0 si se hacen bucles largos\n",
        "        epochs=epochs\n",
        "    )\n",
        "    # plot_history(history)\n",
        "\n",
        "    # Para ver como esta compuesto el vector de predicciones\n",
        "    print(generate_predictions(model, data_valid_norm, scaler))\n",
        "\n",
        "    # Genero la prediccion, y la agrego al vector\n",
        "    prediction = generate_predictions(model, data_valid_norm, scaler)[1]\n",
        "    predictions_list.append(prediction)\n",
        "\n",
        "    # Liberar memoria\n",
        "    del model\n",
        "    del data_train_windowed\n",
        "    del data_valid_windowed\n",
        "    clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "  # display(data_grouped.loc['2019-12'])\n",
        "  # print('¨Prediccion:', np.mean(predictions_list))\n",
        "  product_predictions.loc[producto, 'MinMax'] = np.mean(predictions_list)\n",
        "\n",
        "  # Exporta solo cuando el ultimo digito del ultimo producto coincide con el que ultimo del producto actual\n",
        "  # Para no exportar con cada producto, esto exporta 1 cada ~10\n",
        "  if producto[-1] == product_id_end[-1]:\n",
        "    filename = f\"{instance}_Win{window_size}_{producto}.csv\"\n",
        "    product_predictions.to_csv(path_data + filename)\n",
        "\n",
        "  # Si es para predecir Dec 19, los muestro\n",
        "  if pipeline_type == \"Local\":\n",
        "    display(product_predictions.loc[producto].to_frame().T)"
      ],
      "metadata": {
        "id": "iN-DxvObqi5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "381fcb8b-8b6d-433e-e8d7-f9c1c629255b"
      },
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Producto: 20001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             20001   lag_1   lag_2   lag_3   lag_4   lag_5   lag_6   lag_7  \\\n",
              "periodo                                                                      \n",
              "2018-01-01 1146.61 1037.21 1547.21 1428.89 1300.53 1246.18 1027.88 1475.53   \n",
              "2018-02-01 1012.00 1146.61 1037.21 1547.21 1428.89 1300.53 1246.18 1027.88   \n",
              "2018-03-01 1805.69 1012.00 1146.61 1037.21 1547.21 1428.89 1300.53 1246.18   \n",
              "2018-04-01 1220.76 1805.69 1012.00 1146.61 1037.21 1547.21 1428.89 1300.53   \n",
              "2018-05-01 1245.05 1220.76 1805.69 1012.00 1146.61 1037.21 1547.21 1428.89   \n",
              "2018-06-01 1144.82 1245.05 1220.76 1805.69 1012.00 1146.61 1037.21 1547.21   \n",
              "2018-07-01 1465.04 1144.82 1245.05 1220.76 1805.69 1012.00 1146.61 1037.21   \n",
              "2018-08-01 1793.03 1465.04 1144.82 1245.05 1220.76 1805.69 1012.00 1146.61   \n",
              "2018-09-01 1424.30 1793.03 1465.04 1144.82 1245.05 1220.76 1805.69 1012.00   \n",
              "2018-10-01 2287.11 1424.30 1793.03 1465.04 1144.82 1245.05 1220.76 1805.69   \n",
              "2018-11-01 1804.60 2287.11 1424.30 1793.03 1465.04 1144.82 1245.05 1220.76   \n",
              "2018-12-01 1481.09 1804.60 2287.11 1424.30 1793.03 1465.04 1144.82 1245.05   \n",
              "2019-01-01 1273.02 1481.09 1804.60 2287.11 1424.30 1793.03 1465.04 1144.82   \n",
              "2019-02-01 1256.08 1273.02 1481.09 1804.60 2287.11 1424.30 1793.03 1465.04   \n",
              "2019-03-01 1461.67 1256.08 1273.02 1481.09 1804.60 2287.11 1424.30 1793.03   \n",
              "2019-04-01 1640.79 1461.67 1256.08 1273.02 1481.09 1804.60 2287.11 1424.30   \n",
              "2019-05-01 1627.90 1640.79 1461.67 1256.08 1273.02 1481.09 1804.60 2287.11   \n",
              "2019-06-01 1105.80 1627.90 1640.79 1461.67 1256.08 1273.02 1481.09 1804.60   \n",
              "2019-07-01 1674.14 1105.80 1627.90 1640.79 1461.67 1256.08 1273.02 1481.09   \n",
              "2019-08-01 1663.14 1674.14 1105.80 1627.90 1640.79 1461.67 1256.08 1273.02   \n",
              "2019-09-01 1652.14 1663.14 1674.14 1105.80 1627.90 1640.79 1461.67 1256.08   \n",
              "2019-10-01 1553.44 1652.14 1663.14 1674.14 1105.80 1627.90 1640.79 1461.67   \n",
              "\n",
              "             lag_8   lag_9  ...  month_7  month_8  month_9  month_10  \\\n",
              "periodo                     ...                                        \n",
              "2018-01-01 1475.61 1062.05  ...     0.00     0.00     0.00      0.00   \n",
              "2018-02-01 1475.53 1475.61  ...     0.00     0.00     0.00      0.00   \n",
              "2018-03-01 1027.88 1475.53  ...     0.00     0.00     0.00      0.00   \n",
              "2018-04-01 1246.18 1027.88  ...     0.00     0.00     0.00      0.00   \n",
              "2018-05-01 1300.53 1246.18  ...     0.00     0.00     0.00      0.00   \n",
              "2018-06-01 1428.89 1300.53  ...     0.00     0.00     0.00      0.00   \n",
              "2018-07-01 1547.21 1428.89  ...     1.00     0.00     0.00      0.00   \n",
              "2018-08-01 1037.21 1547.21  ...     0.00     1.00     0.00      0.00   \n",
              "2018-09-01 1146.61 1037.21  ...     0.00     0.00     1.00      0.00   \n",
              "2018-10-01 1012.00 1146.61  ...     0.00     0.00     0.00      1.00   \n",
              "2018-11-01 1805.69 1012.00  ...     0.00     0.00     0.00      0.00   \n",
              "2018-12-01 1220.76 1805.69  ...     0.00     0.00     0.00      0.00   \n",
              "2019-01-01 1245.05 1220.76  ...     0.00     0.00     0.00      0.00   \n",
              "2019-02-01 1144.82 1245.05  ...     0.00     0.00     0.00      0.00   \n",
              "2019-03-01 1465.04 1144.82  ...     0.00     0.00     0.00      0.00   \n",
              "2019-04-01 1793.03 1465.04  ...     0.00     0.00     0.00      0.00   \n",
              "2019-05-01 1424.30 1793.03  ...     0.00     0.00     0.00      0.00   \n",
              "2019-06-01 2287.11 1424.30  ...     0.00     0.00     0.00      0.00   \n",
              "2019-07-01 1804.60 2287.11  ...     1.00     0.00     0.00      0.00   \n",
              "2019-08-01 1481.09 1804.60  ...     0.00     1.00     0.00      0.00   \n",
              "2019-09-01 1273.02 1481.09  ...     0.00     0.00     1.00      0.00   \n",
              "2019-10-01 1256.08 1273.02  ...     0.00     0.00     0.00      1.00   \n",
              "\n",
              "            month_11  month_12  quarter_1  quarter_2  quarter_3  quarter_4  \n",
              "periodo                                                                     \n",
              "2018-01-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2018-02-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2018-03-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2018-04-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2018-05-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2018-06-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2018-07-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2018-08-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2018-09-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2018-10-01      0.00      0.00       0.00       0.00       0.00       1.00  \n",
              "2018-11-01      1.00      0.00       0.00       0.00       0.00       1.00  \n",
              "2018-12-01      0.00      1.00       0.00       0.00       0.00       1.00  \n",
              "2019-01-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2019-02-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2019-03-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2019-04-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2019-05-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2019-06-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2019-07-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2019-08-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2019-09-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2019-10-01      0.00      0.00       0.00       0.00       0.00       1.00  \n",
              "\n",
              "[22 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8a6aad5-1060-45b0-9545-635964cddf1e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>20001</th>\n",
              "      <th>lag_1</th>\n",
              "      <th>lag_2</th>\n",
              "      <th>lag_3</th>\n",
              "      <th>lag_4</th>\n",
              "      <th>lag_5</th>\n",
              "      <th>lag_6</th>\n",
              "      <th>lag_7</th>\n",
              "      <th>lag_8</th>\n",
              "      <th>lag_9</th>\n",
              "      <th>...</th>\n",
              "      <th>month_7</th>\n",
              "      <th>month_8</th>\n",
              "      <th>month_9</th>\n",
              "      <th>month_10</th>\n",
              "      <th>month_11</th>\n",
              "      <th>month_12</th>\n",
              "      <th>quarter_1</th>\n",
              "      <th>quarter_2</th>\n",
              "      <th>quarter_3</th>\n",
              "      <th>quarter_4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>periodo</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-01-01</th>\n",
              "      <td>1146.61</td>\n",
              "      <td>1037.21</td>\n",
              "      <td>1547.21</td>\n",
              "      <td>1428.89</td>\n",
              "      <td>1300.53</td>\n",
              "      <td>1246.18</td>\n",
              "      <td>1027.88</td>\n",
              "      <td>1475.53</td>\n",
              "      <td>1475.61</td>\n",
              "      <td>1062.05</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-02-01</th>\n",
              "      <td>1012.00</td>\n",
              "      <td>1146.61</td>\n",
              "      <td>1037.21</td>\n",
              "      <td>1547.21</td>\n",
              "      <td>1428.89</td>\n",
              "      <td>1300.53</td>\n",
              "      <td>1246.18</td>\n",
              "      <td>1027.88</td>\n",
              "      <td>1475.53</td>\n",
              "      <td>1475.61</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-03-01</th>\n",
              "      <td>1805.69</td>\n",
              "      <td>1012.00</td>\n",
              "      <td>1146.61</td>\n",
              "      <td>1037.21</td>\n",
              "      <td>1547.21</td>\n",
              "      <td>1428.89</td>\n",
              "      <td>1300.53</td>\n",
              "      <td>1246.18</td>\n",
              "      <td>1027.88</td>\n",
              "      <td>1475.53</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-04-01</th>\n",
              "      <td>1220.76</td>\n",
              "      <td>1805.69</td>\n",
              "      <td>1012.00</td>\n",
              "      <td>1146.61</td>\n",
              "      <td>1037.21</td>\n",
              "      <td>1547.21</td>\n",
              "      <td>1428.89</td>\n",
              "      <td>1300.53</td>\n",
              "      <td>1246.18</td>\n",
              "      <td>1027.88</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-05-01</th>\n",
              "      <td>1245.05</td>\n",
              "      <td>1220.76</td>\n",
              "      <td>1805.69</td>\n",
              "      <td>1012.00</td>\n",
              "      <td>1146.61</td>\n",
              "      <td>1037.21</td>\n",
              "      <td>1547.21</td>\n",
              "      <td>1428.89</td>\n",
              "      <td>1300.53</td>\n",
              "      <td>1246.18</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-06-01</th>\n",
              "      <td>1144.82</td>\n",
              "      <td>1245.05</td>\n",
              "      <td>1220.76</td>\n",
              "      <td>1805.69</td>\n",
              "      <td>1012.00</td>\n",
              "      <td>1146.61</td>\n",
              "      <td>1037.21</td>\n",
              "      <td>1547.21</td>\n",
              "      <td>1428.89</td>\n",
              "      <td>1300.53</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-07-01</th>\n",
              "      <td>1465.04</td>\n",
              "      <td>1144.82</td>\n",
              "      <td>1245.05</td>\n",
              "      <td>1220.76</td>\n",
              "      <td>1805.69</td>\n",
              "      <td>1012.00</td>\n",
              "      <td>1146.61</td>\n",
              "      <td>1037.21</td>\n",
              "      <td>1547.21</td>\n",
              "      <td>1428.89</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-08-01</th>\n",
              "      <td>1793.03</td>\n",
              "      <td>1465.04</td>\n",
              "      <td>1144.82</td>\n",
              "      <td>1245.05</td>\n",
              "      <td>1220.76</td>\n",
              "      <td>1805.69</td>\n",
              "      <td>1012.00</td>\n",
              "      <td>1146.61</td>\n",
              "      <td>1037.21</td>\n",
              "      <td>1547.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-09-01</th>\n",
              "      <td>1424.30</td>\n",
              "      <td>1793.03</td>\n",
              "      <td>1465.04</td>\n",
              "      <td>1144.82</td>\n",
              "      <td>1245.05</td>\n",
              "      <td>1220.76</td>\n",
              "      <td>1805.69</td>\n",
              "      <td>1012.00</td>\n",
              "      <td>1146.61</td>\n",
              "      <td>1037.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-10-01</th>\n",
              "      <td>2287.11</td>\n",
              "      <td>1424.30</td>\n",
              "      <td>1793.03</td>\n",
              "      <td>1465.04</td>\n",
              "      <td>1144.82</td>\n",
              "      <td>1245.05</td>\n",
              "      <td>1220.76</td>\n",
              "      <td>1805.69</td>\n",
              "      <td>1012.00</td>\n",
              "      <td>1146.61</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-11-01</th>\n",
              "      <td>1804.60</td>\n",
              "      <td>2287.11</td>\n",
              "      <td>1424.30</td>\n",
              "      <td>1793.03</td>\n",
              "      <td>1465.04</td>\n",
              "      <td>1144.82</td>\n",
              "      <td>1245.05</td>\n",
              "      <td>1220.76</td>\n",
              "      <td>1805.69</td>\n",
              "      <td>1012.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-12-01</th>\n",
              "      <td>1481.09</td>\n",
              "      <td>1804.60</td>\n",
              "      <td>2287.11</td>\n",
              "      <td>1424.30</td>\n",
              "      <td>1793.03</td>\n",
              "      <td>1465.04</td>\n",
              "      <td>1144.82</td>\n",
              "      <td>1245.05</td>\n",
              "      <td>1220.76</td>\n",
              "      <td>1805.69</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-01-01</th>\n",
              "      <td>1273.02</td>\n",
              "      <td>1481.09</td>\n",
              "      <td>1804.60</td>\n",
              "      <td>2287.11</td>\n",
              "      <td>1424.30</td>\n",
              "      <td>1793.03</td>\n",
              "      <td>1465.04</td>\n",
              "      <td>1144.82</td>\n",
              "      <td>1245.05</td>\n",
              "      <td>1220.76</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-01</th>\n",
              "      <td>1256.08</td>\n",
              "      <td>1273.02</td>\n",
              "      <td>1481.09</td>\n",
              "      <td>1804.60</td>\n",
              "      <td>2287.11</td>\n",
              "      <td>1424.30</td>\n",
              "      <td>1793.03</td>\n",
              "      <td>1465.04</td>\n",
              "      <td>1144.82</td>\n",
              "      <td>1245.05</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-03-01</th>\n",
              "      <td>1461.67</td>\n",
              "      <td>1256.08</td>\n",
              "      <td>1273.02</td>\n",
              "      <td>1481.09</td>\n",
              "      <td>1804.60</td>\n",
              "      <td>2287.11</td>\n",
              "      <td>1424.30</td>\n",
              "      <td>1793.03</td>\n",
              "      <td>1465.04</td>\n",
              "      <td>1144.82</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-04-01</th>\n",
              "      <td>1640.79</td>\n",
              "      <td>1461.67</td>\n",
              "      <td>1256.08</td>\n",
              "      <td>1273.02</td>\n",
              "      <td>1481.09</td>\n",
              "      <td>1804.60</td>\n",
              "      <td>2287.11</td>\n",
              "      <td>1424.30</td>\n",
              "      <td>1793.03</td>\n",
              "      <td>1465.04</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-05-01</th>\n",
              "      <td>1627.90</td>\n",
              "      <td>1640.79</td>\n",
              "      <td>1461.67</td>\n",
              "      <td>1256.08</td>\n",
              "      <td>1273.02</td>\n",
              "      <td>1481.09</td>\n",
              "      <td>1804.60</td>\n",
              "      <td>2287.11</td>\n",
              "      <td>1424.30</td>\n",
              "      <td>1793.03</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-06-01</th>\n",
              "      <td>1105.80</td>\n",
              "      <td>1627.90</td>\n",
              "      <td>1640.79</td>\n",
              "      <td>1461.67</td>\n",
              "      <td>1256.08</td>\n",
              "      <td>1273.02</td>\n",
              "      <td>1481.09</td>\n",
              "      <td>1804.60</td>\n",
              "      <td>2287.11</td>\n",
              "      <td>1424.30</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-07-01</th>\n",
              "      <td>1674.14</td>\n",
              "      <td>1105.80</td>\n",
              "      <td>1627.90</td>\n",
              "      <td>1640.79</td>\n",
              "      <td>1461.67</td>\n",
              "      <td>1256.08</td>\n",
              "      <td>1273.02</td>\n",
              "      <td>1481.09</td>\n",
              "      <td>1804.60</td>\n",
              "      <td>2287.11</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-08-01</th>\n",
              "      <td>1663.14</td>\n",
              "      <td>1674.14</td>\n",
              "      <td>1105.80</td>\n",
              "      <td>1627.90</td>\n",
              "      <td>1640.79</td>\n",
              "      <td>1461.67</td>\n",
              "      <td>1256.08</td>\n",
              "      <td>1273.02</td>\n",
              "      <td>1481.09</td>\n",
              "      <td>1804.60</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-09-01</th>\n",
              "      <td>1652.14</td>\n",
              "      <td>1663.14</td>\n",
              "      <td>1674.14</td>\n",
              "      <td>1105.80</td>\n",
              "      <td>1627.90</td>\n",
              "      <td>1640.79</td>\n",
              "      <td>1461.67</td>\n",
              "      <td>1256.08</td>\n",
              "      <td>1273.02</td>\n",
              "      <td>1481.09</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-10-01</th>\n",
              "      <td>1553.44</td>\n",
              "      <td>1652.14</td>\n",
              "      <td>1663.14</td>\n",
              "      <td>1674.14</td>\n",
              "      <td>1105.80</td>\n",
              "      <td>1627.90</td>\n",
              "      <td>1640.79</td>\n",
              "      <td>1461.67</td>\n",
              "      <td>1256.08</td>\n",
              "      <td>1273.02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22 rows × 38 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8a6aad5-1060-45b0-9545-635964cddf1e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e8a6aad5-1060-45b0-9545-635964cddf1e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e8a6aad5-1060-45b0-9545-635964cddf1e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ce55cb04-cbae-40c9-a4db-cdb3ca197495\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ce55cb04-cbae-40c9-a4db-cdb3ca197495')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ce55cb04-cbae-40c9-a4db-cdb3ca197495 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_de68e824-b125-4b0d-b49f-f2c224874034\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_grouped')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_de68e824-b125-4b0d-b49f-f2c224874034 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_grouped');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_grouped"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "20001             1461.67\n",
            "lag_1             1552.30\n",
            "lag_2             1554.42\n",
            "lag_3             1535.94\n",
            "lag_4             1507.28\n",
            "lag_5             1513.79\n",
            "lag_6             1300.53\n",
            "lag_7             1410.57\n",
            "lag_8             1358.39\n",
            "lag_9             1387.57\n",
            "lag_10            1341.03\n",
            "lag_11            1306.86\n",
            "lag_12            1266.24\n",
            "rolling_mean_3    1423.83\n",
            "rolling_std_3      287.24\n",
            "rolling_mean_6    1499.95\n",
            "rolling_std_6      296.30\n",
            "rolling_mean_12   1355.88\n",
            "rolling_std_12     318.19\n",
            "year_2017            0.00\n",
            "year_2018            1.03\n",
            "year_2019            0.49\n",
            "month_1              0.25\n",
            "month_2              0.09\n",
            "month_3              0.20\n",
            "month_4              0.00\n",
            "month_5              0.36\n",
            "month_6              0.00\n",
            "month_7              0.05\n",
            "month_8              0.14\n",
            "month_9              0.05\n",
            "month_10             0.06\n",
            "month_11             0.00\n",
            "month_12             0.00\n",
            "quarter_1            0.12\n",
            "quarter_2            0.21\n",
            "quarter_3            0.00\n",
            "quarter_4            0.44\n",
            "Name: tn, dtype: float32\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "20001             1461.67\n",
            "lag_1             1789.97\n",
            "lag_2             1716.67\n",
            "lag_3             1567.71\n",
            "lag_4             1424.30\n",
            "lag_5             1583.67\n",
            "lag_6             1433.52\n",
            "lag_7             1300.53\n",
            "lag_8             1368.46\n",
            "lag_9             1430.24\n",
            "lag_10            1246.18\n",
            "lag_11            1245.05\n",
            "lag_12            1294.64\n",
            "rolling_mean_3    1738.01\n",
            "rolling_std_3      441.15\n",
            "rolling_mean_6    1578.39\n",
            "rolling_std_6      340.82\n",
            "rolling_mean_12   1506.59\n",
            "rolling_std_12     355.70\n",
            "year_2017            0.00\n",
            "year_2018            1.00\n",
            "year_2019            0.28\n",
            "month_1              0.20\n",
            "month_2              0.00\n",
            "month_3              0.01\n",
            "month_4              0.00\n",
            "month_5              0.17\n",
            "month_6              0.00\n",
            "month_7              0.00\n",
            "month_8              0.48\n",
            "month_9              0.19\n",
            "month_10             0.06\n",
            "month_11             0.13\n",
            "month_12             0.57\n",
            "quarter_1            0.37\n",
            "quarter_2            0.00\n",
            "quarter_3            0.44\n",
            "quarter_4            1.03\n",
            "Name: tn, dtype: float32\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "MinMax\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         Real  MinMax  Robust\n",
              "20001 1504.69 1775.62 1671.14"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6195f482-2e9b-4fd5-a27d-6b2c979bd81d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Real</th>\n",
              "      <th>MinMax</th>\n",
              "      <th>Robust</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20001</th>\n",
              "      <td>1504.69</td>\n",
              "      <td>1775.62</td>\n",
              "      <td>1671.14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6195f482-2e9b-4fd5-a27d-6b2c979bd81d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6195f482-2e9b-4fd5-a27d-6b2c979bd81d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6195f482-2e9b-4fd5-a27d-6b2c979bd81d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    display(product_predictions\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Real\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 1504.68856,\n        \"max\": 1504.68856,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1504.68856\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MinMax\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1775.6163\",\n        \"max\": \"1775.6163\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1775.6163\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Robust\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1671.1364\",\n        \"max\": \"1671.1364\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1671.1364\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Producto: 20002\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             20002   lag_1   lag_2   lag_3   lag_4   lag_5   lag_6   lag_7  \\\n",
              "periodo                                                                      \n",
              "2018-01-01  975.47  793.53  733.12  854.51 1050.42  611.14  840.80  934.40   \n",
              "2018-02-01  698.15  975.47  793.53  733.12  854.51 1050.42  611.14  840.80   \n",
              "2018-03-01  947.94  698.15  975.47  793.53  733.12  854.51 1050.42  611.14   \n",
              "2018-04-01  983.00  947.94  698.15  975.47  793.53  733.12  854.51 1050.42   \n",
              "2018-05-01 1088.21  983.00  947.94  698.15  975.47  793.53  733.12  854.51   \n",
              "2018-06-01 1019.41 1088.21  983.00  947.94  698.15  975.47  793.53  733.12   \n",
              "2018-07-01  968.68 1019.41 1088.21  983.00  947.94  698.15  975.47  793.53   \n",
              "2018-08-01 1150.87  968.68 1019.41 1088.21  983.00  947.94  698.15  975.47   \n",
              "2018-09-01  949.98 1150.87  968.68 1019.41 1088.21  983.00  947.94  698.15   \n",
              "2018-10-01 1367.22  949.98 1150.87  968.68 1019.41 1088.21  983.00  947.94   \n",
              "2018-11-01 1751.03 1367.22  949.98 1150.87  968.68 1019.41 1088.21  983.00   \n",
              "2018-12-01  999.54 1751.03 1367.22  949.98 1150.87  968.68 1019.41 1088.21   \n",
              "2019-01-01 1261.29  999.54 1751.03 1367.22  949.98 1150.87  968.68 1019.41   \n",
              "2019-02-01 1037.76 1261.29  999.54 1751.03 1367.22  949.98 1150.87  968.68   \n",
              "2019-03-01 1076.76 1037.76 1261.29  999.54 1751.03 1367.22  949.98 1150.87   \n",
              "2019-04-01 1281.08 1076.76 1037.76 1261.29  999.54 1751.03 1367.22  949.98   \n",
              "2019-05-01 1030.41 1281.08 1076.76 1037.76 1261.29  999.54 1751.03 1367.22   \n",
              "2019-06-01  921.67 1030.41 1281.08 1076.76 1037.76 1261.29  999.54 1751.03   \n",
              "2019-07-01 1058.81  921.67 1030.41 1281.08 1076.76 1037.76 1261.29  999.54   \n",
              "2019-08-01 1073.18 1058.81  921.67 1030.41 1281.08 1076.76 1037.76 1261.29   \n",
              "2019-09-01 1087.55 1073.18 1058.81  921.67 1030.41 1281.08 1076.76 1037.76   \n",
              "2019-10-01 1979.12 1087.55 1073.18 1058.81  921.67 1030.41 1281.08 1076.76   \n",
              "\n",
              "             lag_8   lag_9  ...  month_7  month_8  month_9  month_10  \\\n",
              "periodo                     ...                                        \n",
              "2018-01-01  818.47  509.85  ...     0.00     0.00     0.00      0.00   \n",
              "2018-02-01  934.40  818.47  ...     0.00     0.00     0.00      0.00   \n",
              "2018-03-01  840.80  934.40  ...     0.00     0.00     0.00      0.00   \n",
              "2018-04-01  611.14  840.80  ...     0.00     0.00     0.00      0.00   \n",
              "2018-05-01 1050.42  611.14  ...     0.00     0.00     0.00      0.00   \n",
              "2018-06-01  854.51 1050.42  ...     0.00     0.00     0.00      0.00   \n",
              "2018-07-01  733.12  854.51  ...     1.00     0.00     0.00      0.00   \n",
              "2018-08-01  793.53  733.12  ...     0.00     1.00     0.00      0.00   \n",
              "2018-09-01  975.47  793.53  ...     0.00     0.00     1.00      0.00   \n",
              "2018-10-01  698.15  975.47  ...     0.00     0.00     0.00      1.00   \n",
              "2018-11-01  947.94  698.15  ...     0.00     0.00     0.00      0.00   \n",
              "2018-12-01  983.00  947.94  ...     0.00     0.00     0.00      0.00   \n",
              "2019-01-01 1088.21  983.00  ...     0.00     0.00     0.00      0.00   \n",
              "2019-02-01 1019.41 1088.21  ...     0.00     0.00     0.00      0.00   \n",
              "2019-03-01  968.68 1019.41  ...     0.00     0.00     0.00      0.00   \n",
              "2019-04-01 1150.87  968.68  ...     0.00     0.00     0.00      0.00   \n",
              "2019-05-01  949.98 1150.87  ...     0.00     0.00     0.00      0.00   \n",
              "2019-06-01 1367.22  949.98  ...     0.00     0.00     0.00      0.00   \n",
              "2019-07-01 1751.03 1367.22  ...     1.00     0.00     0.00      0.00   \n",
              "2019-08-01  999.54 1751.03  ...     0.00     1.00     0.00      0.00   \n",
              "2019-09-01 1261.29  999.54  ...     0.00     0.00     1.00      0.00   \n",
              "2019-10-01 1037.76 1261.29  ...     0.00     0.00     0.00      1.00   \n",
              "\n",
              "            month_11  month_12  quarter_1  quarter_2  quarter_3  quarter_4  \n",
              "periodo                                                                     \n",
              "2018-01-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2018-02-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2018-03-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2018-04-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2018-05-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2018-06-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2018-07-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2018-08-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2018-09-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2018-10-01      0.00      0.00       0.00       0.00       0.00       1.00  \n",
              "2018-11-01      1.00      0.00       0.00       0.00       0.00       1.00  \n",
              "2018-12-01      0.00      1.00       0.00       0.00       0.00       1.00  \n",
              "2019-01-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2019-02-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2019-03-01      0.00      0.00       1.00       0.00       0.00       0.00  \n",
              "2019-04-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2019-05-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2019-06-01      0.00      0.00       0.00       1.00       0.00       0.00  \n",
              "2019-07-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2019-08-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2019-09-01      0.00      0.00       0.00       0.00       1.00       0.00  \n",
              "2019-10-01      0.00      0.00       0.00       0.00       0.00       1.00  \n",
              "\n",
              "[22 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e81e1057-7225-49dd-adf8-53a813546b8b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>20002</th>\n",
              "      <th>lag_1</th>\n",
              "      <th>lag_2</th>\n",
              "      <th>lag_3</th>\n",
              "      <th>lag_4</th>\n",
              "      <th>lag_5</th>\n",
              "      <th>lag_6</th>\n",
              "      <th>lag_7</th>\n",
              "      <th>lag_8</th>\n",
              "      <th>lag_9</th>\n",
              "      <th>...</th>\n",
              "      <th>month_7</th>\n",
              "      <th>month_8</th>\n",
              "      <th>month_9</th>\n",
              "      <th>month_10</th>\n",
              "      <th>month_11</th>\n",
              "      <th>month_12</th>\n",
              "      <th>quarter_1</th>\n",
              "      <th>quarter_2</th>\n",
              "      <th>quarter_3</th>\n",
              "      <th>quarter_4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>periodo</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-01-01</th>\n",
              "      <td>975.47</td>\n",
              "      <td>793.53</td>\n",
              "      <td>733.12</td>\n",
              "      <td>854.51</td>\n",
              "      <td>1050.42</td>\n",
              "      <td>611.14</td>\n",
              "      <td>840.80</td>\n",
              "      <td>934.40</td>\n",
              "      <td>818.47</td>\n",
              "      <td>509.85</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-02-01</th>\n",
              "      <td>698.15</td>\n",
              "      <td>975.47</td>\n",
              "      <td>793.53</td>\n",
              "      <td>733.12</td>\n",
              "      <td>854.51</td>\n",
              "      <td>1050.42</td>\n",
              "      <td>611.14</td>\n",
              "      <td>840.80</td>\n",
              "      <td>934.40</td>\n",
              "      <td>818.47</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-03-01</th>\n",
              "      <td>947.94</td>\n",
              "      <td>698.15</td>\n",
              "      <td>975.47</td>\n",
              "      <td>793.53</td>\n",
              "      <td>733.12</td>\n",
              "      <td>854.51</td>\n",
              "      <td>1050.42</td>\n",
              "      <td>611.14</td>\n",
              "      <td>840.80</td>\n",
              "      <td>934.40</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-04-01</th>\n",
              "      <td>983.00</td>\n",
              "      <td>947.94</td>\n",
              "      <td>698.15</td>\n",
              "      <td>975.47</td>\n",
              "      <td>793.53</td>\n",
              "      <td>733.12</td>\n",
              "      <td>854.51</td>\n",
              "      <td>1050.42</td>\n",
              "      <td>611.14</td>\n",
              "      <td>840.80</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-05-01</th>\n",
              "      <td>1088.21</td>\n",
              "      <td>983.00</td>\n",
              "      <td>947.94</td>\n",
              "      <td>698.15</td>\n",
              "      <td>975.47</td>\n",
              "      <td>793.53</td>\n",
              "      <td>733.12</td>\n",
              "      <td>854.51</td>\n",
              "      <td>1050.42</td>\n",
              "      <td>611.14</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-06-01</th>\n",
              "      <td>1019.41</td>\n",
              "      <td>1088.21</td>\n",
              "      <td>983.00</td>\n",
              "      <td>947.94</td>\n",
              "      <td>698.15</td>\n",
              "      <td>975.47</td>\n",
              "      <td>793.53</td>\n",
              "      <td>733.12</td>\n",
              "      <td>854.51</td>\n",
              "      <td>1050.42</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-07-01</th>\n",
              "      <td>968.68</td>\n",
              "      <td>1019.41</td>\n",
              "      <td>1088.21</td>\n",
              "      <td>983.00</td>\n",
              "      <td>947.94</td>\n",
              "      <td>698.15</td>\n",
              "      <td>975.47</td>\n",
              "      <td>793.53</td>\n",
              "      <td>733.12</td>\n",
              "      <td>854.51</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-08-01</th>\n",
              "      <td>1150.87</td>\n",
              "      <td>968.68</td>\n",
              "      <td>1019.41</td>\n",
              "      <td>1088.21</td>\n",
              "      <td>983.00</td>\n",
              "      <td>947.94</td>\n",
              "      <td>698.15</td>\n",
              "      <td>975.47</td>\n",
              "      <td>793.53</td>\n",
              "      <td>733.12</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-09-01</th>\n",
              "      <td>949.98</td>\n",
              "      <td>1150.87</td>\n",
              "      <td>968.68</td>\n",
              "      <td>1019.41</td>\n",
              "      <td>1088.21</td>\n",
              "      <td>983.00</td>\n",
              "      <td>947.94</td>\n",
              "      <td>698.15</td>\n",
              "      <td>975.47</td>\n",
              "      <td>793.53</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-10-01</th>\n",
              "      <td>1367.22</td>\n",
              "      <td>949.98</td>\n",
              "      <td>1150.87</td>\n",
              "      <td>968.68</td>\n",
              "      <td>1019.41</td>\n",
              "      <td>1088.21</td>\n",
              "      <td>983.00</td>\n",
              "      <td>947.94</td>\n",
              "      <td>698.15</td>\n",
              "      <td>975.47</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-11-01</th>\n",
              "      <td>1751.03</td>\n",
              "      <td>1367.22</td>\n",
              "      <td>949.98</td>\n",
              "      <td>1150.87</td>\n",
              "      <td>968.68</td>\n",
              "      <td>1019.41</td>\n",
              "      <td>1088.21</td>\n",
              "      <td>983.00</td>\n",
              "      <td>947.94</td>\n",
              "      <td>698.15</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-12-01</th>\n",
              "      <td>999.54</td>\n",
              "      <td>1751.03</td>\n",
              "      <td>1367.22</td>\n",
              "      <td>949.98</td>\n",
              "      <td>1150.87</td>\n",
              "      <td>968.68</td>\n",
              "      <td>1019.41</td>\n",
              "      <td>1088.21</td>\n",
              "      <td>983.00</td>\n",
              "      <td>947.94</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-01-01</th>\n",
              "      <td>1261.29</td>\n",
              "      <td>999.54</td>\n",
              "      <td>1751.03</td>\n",
              "      <td>1367.22</td>\n",
              "      <td>949.98</td>\n",
              "      <td>1150.87</td>\n",
              "      <td>968.68</td>\n",
              "      <td>1019.41</td>\n",
              "      <td>1088.21</td>\n",
              "      <td>983.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-01</th>\n",
              "      <td>1037.76</td>\n",
              "      <td>1261.29</td>\n",
              "      <td>999.54</td>\n",
              "      <td>1751.03</td>\n",
              "      <td>1367.22</td>\n",
              "      <td>949.98</td>\n",
              "      <td>1150.87</td>\n",
              "      <td>968.68</td>\n",
              "      <td>1019.41</td>\n",
              "      <td>1088.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-03-01</th>\n",
              "      <td>1076.76</td>\n",
              "      <td>1037.76</td>\n",
              "      <td>1261.29</td>\n",
              "      <td>999.54</td>\n",
              "      <td>1751.03</td>\n",
              "      <td>1367.22</td>\n",
              "      <td>949.98</td>\n",
              "      <td>1150.87</td>\n",
              "      <td>968.68</td>\n",
              "      <td>1019.41</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-04-01</th>\n",
              "      <td>1281.08</td>\n",
              "      <td>1076.76</td>\n",
              "      <td>1037.76</td>\n",
              "      <td>1261.29</td>\n",
              "      <td>999.54</td>\n",
              "      <td>1751.03</td>\n",
              "      <td>1367.22</td>\n",
              "      <td>949.98</td>\n",
              "      <td>1150.87</td>\n",
              "      <td>968.68</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-05-01</th>\n",
              "      <td>1030.41</td>\n",
              "      <td>1281.08</td>\n",
              "      <td>1076.76</td>\n",
              "      <td>1037.76</td>\n",
              "      <td>1261.29</td>\n",
              "      <td>999.54</td>\n",
              "      <td>1751.03</td>\n",
              "      <td>1367.22</td>\n",
              "      <td>949.98</td>\n",
              "      <td>1150.87</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-06-01</th>\n",
              "      <td>921.67</td>\n",
              "      <td>1030.41</td>\n",
              "      <td>1281.08</td>\n",
              "      <td>1076.76</td>\n",
              "      <td>1037.76</td>\n",
              "      <td>1261.29</td>\n",
              "      <td>999.54</td>\n",
              "      <td>1751.03</td>\n",
              "      <td>1367.22</td>\n",
              "      <td>949.98</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-07-01</th>\n",
              "      <td>1058.81</td>\n",
              "      <td>921.67</td>\n",
              "      <td>1030.41</td>\n",
              "      <td>1281.08</td>\n",
              "      <td>1076.76</td>\n",
              "      <td>1037.76</td>\n",
              "      <td>1261.29</td>\n",
              "      <td>999.54</td>\n",
              "      <td>1751.03</td>\n",
              "      <td>1367.22</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-08-01</th>\n",
              "      <td>1073.18</td>\n",
              "      <td>1058.81</td>\n",
              "      <td>921.67</td>\n",
              "      <td>1030.41</td>\n",
              "      <td>1281.08</td>\n",
              "      <td>1076.76</td>\n",
              "      <td>1037.76</td>\n",
              "      <td>1261.29</td>\n",
              "      <td>999.54</td>\n",
              "      <td>1751.03</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-09-01</th>\n",
              "      <td>1087.55</td>\n",
              "      <td>1073.18</td>\n",
              "      <td>1058.81</td>\n",
              "      <td>921.67</td>\n",
              "      <td>1030.41</td>\n",
              "      <td>1281.08</td>\n",
              "      <td>1076.76</td>\n",
              "      <td>1037.76</td>\n",
              "      <td>1261.29</td>\n",
              "      <td>999.54</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-10-01</th>\n",
              "      <td>1979.12</td>\n",
              "      <td>1087.55</td>\n",
              "      <td>1073.18</td>\n",
              "      <td>1058.81</td>\n",
              "      <td>921.67</td>\n",
              "      <td>1030.41</td>\n",
              "      <td>1281.08</td>\n",
              "      <td>1076.76</td>\n",
              "      <td>1037.76</td>\n",
              "      <td>1261.29</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22 rows × 38 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e81e1057-7225-49dd-adf8-53a813546b8b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e81e1057-7225-49dd-adf8-53a813546b8b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e81e1057-7225-49dd-adf8-53a813546b8b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-df78d71b-c04f-4167-ac68-d4f064b0eb70\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df78d71b-c04f-4167-ac68-d4f064b0eb70')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-df78d71b-c04f-4167-ac68-d4f064b0eb70 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9051a973-3926-4677-9110-64859aa2a17f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_grouped')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9051a973-3926-4677-9110-64859aa2a17f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_grouped');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_grouped"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "20002             1173.95\n",
            "lag_1             1338.35\n",
            "lag_2             1092.55\n",
            "lag_3             1176.92\n",
            "lag_4             1107.83\n",
            "lag_5              990.19\n",
            "lag_6             1122.99\n",
            "lag_7             1020.06\n",
            "lag_8              975.57\n",
            "lag_9              934.40\n",
            "lag_10             854.51\n",
            "lag_11             926.86\n",
            "lag_12             823.66\n",
            "rolling_mean_3    1243.98\n",
            "rolling_std_3      201.47\n",
            "rolling_mean_6    1173.13\n",
            "rolling_std_6      252.68\n",
            "rolling_mean_12   1032.59\n",
            "rolling_std_12     229.70\n",
            "year_2017            0.00\n",
            "year_2018            1.00\n",
            "year_2019            0.16\n",
            "month_1              0.00\n",
            "month_2              0.00\n",
            "month_3              0.04\n",
            "month_4              0.00\n",
            "month_5              0.02\n",
            "month_6              0.00\n",
            "month_7              0.00\n",
            "month_8              0.00\n",
            "month_9              0.00\n",
            "month_10             0.00\n",
            "month_11             0.03\n",
            "month_12             0.00\n",
            "quarter_1            0.15\n",
            "quarter_2            0.00\n",
            "quarter_3            0.00\n",
            "quarter_4            0.31\n",
            "Name: tn, dtype: float32\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-370-e005cce4f4cf>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mdata_train_windowed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_valid_windowed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1872\u001b[0m                     \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2113\u001b[0m             \u001b[0;31m# Only restart wait if we beat both the baseline and our previous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;31m# best.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3045\u001b[0m         \"\"\"\n\u001b[1;32m   3046\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3047\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtraceback_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1876\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m                 \u001b[0moutput_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   4246\u001b[0m     \"\"\"\n\u001b[1;32m   4247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4249\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4250\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot get value inside Tensorflow graph function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4246\u001b[0m     \"\"\"\n\u001b[1;32m   4247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4249\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4250\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot get value inside Tensorflow graph function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    691\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \"\"\"\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m(no_copy)\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mno_copy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforward_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_copy_on_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[1;32m    809\u001b[0m           self.handle, self._dtype)\n\u001b[1;32m    810\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    532\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    535\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentos Individuales"
      ],
      "metadata": {
        "id": "SaiJuaUP3JUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 07/10 - Entrenamiento de Productos Hermanos"
      ],
      "metadata": {
        "id": "hy2HBE3XK5WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Seleccionamos uno o varios productos, y observamos sus categorias y productos hermanos\n",
        "# # Productos hermanos = Cat1 = Cat2 = Cat3 = brand\n",
        "\n",
        "# product_id_start = '20002'\n",
        "# product_id_end = '20002'\n",
        "# start_index = data_productos_a_predecir.index.get_loc(product_id_start)\n",
        "# end_index = data_productos_a_predecir.index.get_loc(product_id_end) + 1  # +1 to include the end index\n",
        "\n",
        "# for producto in data_productos_a_predecir.index[start_index:end_index]:\n",
        "\n",
        "#   # Ploteamos la serie dle producto, y comparamos el valor real con las predicciones con scalers MinMax y Robust a Dec2019\n",
        "#   plot_product(data, producto, pipeline_type)\n",
        "\n",
        "#   # Extraigo y muestro los productos hermanos, para futuro analisis\n",
        "#   # Esto puede servir para entrenar varios modelos, agrupados por productos hermanos\n",
        "#   productos_hermanos = filter_productos_hermanos(data, producto)"
      ],
      "metadata": {
        "id": "h_UOATGtLAM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ploteamos las series de productos hermanos en una sola grafica\n",
        "\n",
        "# # Me quedo solo con los productos hermanos\n",
        "# filtered_data = data[data['product_id'].isin(productos_hermanos.index)]\n",
        "\n",
        "# # Agrupo\n",
        "# data_grouped = group_data(filtered_data, 'product_id')\n",
        "\n",
        "# # Arreglo fechas en productos jovenes, o discontinuados\n",
        "# data_grouped = fill_missing_dates(data_grouped, pipeline_type)\n",
        "\n",
        "# # Arreglo Agosto 2019\n",
        "# data_grouped = fix_aug2019(data_grouped, 'mean')\n",
        "\n",
        "# # Grafico\n",
        "# data_grouped.plot(figsize=(10, 5))\n",
        "# plt.show()\n",
        "\n",
        "# # Verificamos como se KNNImputer maneja los nulos\n",
        "# display(data_grouped.head())"
      ],
      "metadata": {
        "id": "asAz6dxFOQOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Realizo una prediccion naif de todos los productos hermanos juntos, para ver si la red toma las relaciones entre ellos\n",
        "\n",
        "# window_size = 3\n",
        "\n",
        "# # Split\n",
        "# data_train, data_valid = split_data_fe(data_grouped, window_size, horizon)\n",
        "\n",
        "# # Escalo\n",
        "# scaler = MinMaxScaler() # RobustScaler\n",
        "# data_train_norm = pd.DataFrame(scaler.fit_transform(data_train), columns=data_train.columns, index=data_train.index)\n",
        "# data_valid_norm = pd.DataFrame(scaler.transform(data_valid), columns=data_valid.columns, index=data_valid.index)\n",
        "\n",
        "# # Creo los Windowed Datasets\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# # Parmetros del modelo\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 20\n",
        "# epochs = 500\n",
        "# n_features = data_train.shape[1]\n",
        "# callbacks = MyCallbacks(patience)\n",
        "\n",
        "# # Modelo\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data=data_valid_windowed,\n",
        "#     callbacks=callbacks,\n",
        "#     verbose=0, # Poner en 0 si se hacen bucles largos\n",
        "#     epochs=epochs\n",
        "# )\n",
        "# plot_history(history)\n",
        "\n",
        "# print(generate_predictions(model, data_valid_norm, scaler))"
      ],
      "metadata": {
        "id": "YX_mDJ7QQWi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Grafico cada producto hermano por separado, para verificar valores atipicos e intentar otros approaches\n",
        "# # Verifico si me combiene cambiar el scaler, o usar on horizon distinto si es que observo estacionariedades\n",
        "\n",
        "# product = '20002'\n",
        "# data_grouped[product].plot(figsize=(14, 5))\n",
        "# plt.title(f'Producto {product}')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "LNzo8L-tn9p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Probamos con distintas estrategias para cada producto individual\n",
        "# # Podemos cambiar el window_size,  el scaler, y la cantidad de iteraciones, para ver como se comprotan las predicciones\n",
        "\n",
        "# scaler_type = 'MinMax'\n",
        "# window_size = 3\n",
        "# n = 1\n",
        "\n",
        "# data_product = data.query(\"product_id == @producto\")\n",
        "# data_product_grouped = group_data(data_product, 'product_id')\n",
        "# display(data_product_grouped.head())\n",
        "# display(data_product_grouped.tail())\n",
        "\n",
        "# # # Esto no deberia de ser necesario\n",
        "# # data_product_grouped = fill_missing_dates(data_product_grouped)\n",
        "# # # data_product_grouped = fix_aug2019(data_product_grouped, 'mean')\n",
        "\n",
        "# data_product_grouped_fe = feature_engineering(data_product_grouped)\n",
        "\n",
        "# # # Split. No pude hacer funcionar el TimeSeriesSplit\n",
        "# data_train, data_valid = split_data_fe(data_product_grouped_fe, window_size, horizon)\n",
        "\n",
        "# # Escalo\n",
        "# if scaler_type=='MinMax':\n",
        "#   scaler = MinMaxScaler() # RobustScaler\n",
        "# elif scaler_type=='Robust':\n",
        "#   scaler = RobustScaler()\n",
        "# else:\n",
        "#   raise ValueError(\"Invalid scaler type\")\n",
        "# data_train_norm = pd.DataFrame(scaler.fit_transform(data_train), columns=data_train.columns, index=data_train.index)\n",
        "# data_valid_norm = pd.DataFrame(scaler.transform(data_valid), columns=data_valid.columns, index=data_valid.index)\n",
        "\n",
        "# predictions_list = []\n",
        "\n",
        "# for i in range(n):\n",
        "\n",
        "#   # Creo los Windowed Datasets\n",
        "#   data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   n_features = data_train.shape[1]\n",
        "\n",
        "#   # Parmetros del modelo\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 20\n",
        "#   epochs = 500\n",
        "#   n_features = data_train.shape[1]\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "\n",
        "#   # Modelo\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       validation_data=data_valid_windowed,\n",
        "#       callbacks=callbacks,\n",
        "#       verbose=0, # Poner en 0 si se hacen bucles largos\n",
        "#       epochs=epochs\n",
        "#   )\n",
        "#   plot_history(history)\n",
        "\n",
        "#   # Genero la prediccion, y la agrego al vector\n",
        "#   prediction = generate_predictions(model, data_valid_norm, scaler)[1]\n",
        "#   predictions_list.append(prediction)\n",
        "\n",
        "# print('Producto:', producto, '\\n')\n",
        "# print('¨Prediccion:', np.mean(predictions_list))\n",
        "# print(f'Scaler {scaler_type}, Window_size {window_size}')"
      ],
      "metadata": {
        "id": "-EiGS0B9dlDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 07/09 - Feature Engineering (A manopla)"
      ],
      "metadata": {
        "id": "_5kYUpih4gaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Selecciono un producto\n",
        "# product = '20334'\n",
        "# data_product = data.query(\"product_id == @product\")\n",
        "# data_grouped = group_data(data_product, 'product_id')\n",
        "# data_grouped = fix_aug2019(data_grouped, 'mean')\n",
        "# data_grouped.index.name = 'periodo'\n",
        "\n",
        "# # Genero el grouped Dataframe, crea los index faltantes, y \"maneja\" los nulos\n",
        "# data_grouped = fill_missing_dates(data_grouped, pipeline_type)\n",
        "\n",
        "# # Generacion de variables adicionales\n",
        "# data_grouped = feature_engineering(data_grouped)\n",
        "\n",
        "# # Split. No pude hacer funcionar el TimeSeriesSplit\n",
        "# data_train, data_valid = split_data_fe(data_grouped, window_size, horizon)\n",
        "\n",
        "# # Escalo\n",
        "# scaler = MinMaxScaler() # RobustScaler\n",
        "# data_train_norm = pd.DataFrame(scaler.fit_transform(data_train), columns=data_train.columns, index=data_train.index)\n",
        "# data_valid_norm = pd.DataFrame(scaler.transform(data_valid), columns=data_valid.columns, index=data_valid.index)\n",
        "\n",
        "# # Creo los Windowed Datasets\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# # Parmetros del modelo\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 20\n",
        "# epochs = 500\n",
        "# n_features = data_train.shape[1]\n",
        "# callbacks = MyCallbacks(patience)\n",
        "\n",
        "# # Modelo\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data=data_valid_windowed,\n",
        "#     callbacks=callbacks,\n",
        "#     verbose=0, # Poner en 0 si se hacen bucles largos\n",
        "#     epochs=epochs\n",
        "# )\n",
        "# plot_history(history)\n",
        "\n",
        "# generate_predictions(model, data_valid_norm, scaler)[0]"
      ],
      "metadata": {
        "id": "6dHvzgBpHzIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 07/08 - Feature Engineering (Featuretools). Muy complicado"
      ],
      "metadata": {
        "id": "I_lwRj4a3VX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install featuretools\n",
        "# import featuretools as ft\n",
        "# from featuretools.demo import load_weather\n",
        "# from featuretools.primitives import Lag, RollingMean, RollingMin\n",
        "# from featuretools.primitives import Lag, RollingMean, RollingMin\n",
        "\n",
        "\n",
        "\n",
        "# # Asegúrate de que data_grouped esté definido antes de este punto.\n",
        "# product = '20012'\n",
        "# data_product = data.query(\"product_id == @product\")\n",
        "# data_grouped = group_data(data_product, 'product_id')\n",
        "# data_grouped = fix_aug2019(data_grouped, 'mean')\n",
        "# data_grouped.reset_index(inplace=True)\n",
        "# data_grouped.rename(columns={'index': 'periodo'}, inplace=True)\n",
        "\n",
        "# # Configuración para lags y rolling por 12 meses\n",
        "# window_length = window_size\n",
        "# gap = horizon\n",
        "\n",
        "\n",
        "\n",
        "# es = ft.EntitySet(id=\"sales_data\")\n",
        "\n",
        "# es = es.add_dataframe(dataframe_name=\"sales\",\n",
        "#                       dataframe=data_grouped,\n",
        "#                       index=\"index\",\n",
        "#                       time_index=\"periodo\")\n",
        "\n",
        "# datetime_primitives = [\"Year\",  \"Month\"]\n",
        "\n",
        "# delaying_primitives = [Lag(periods=i + gap) for i in range(window_length)]\n",
        "\n",
        "# rolling_mean_primitive = RollingMean(\n",
        "#     window_length=window_length, gap=gap, min_periods=window_length)\n",
        "\n",
        "# rolling_min_primitive = RollingMin(\n",
        "#     window_length=window_length, gap=gap, min_periods=window_length)\n",
        "\n",
        "\n",
        "# data_fe, _ = ft.dfs(\n",
        "#     entityset=es,\n",
        "#     target_dataframe_name=\"sales\",\n",
        "#     trans_primitives=(\n",
        "#         # datetime_primitives +\n",
        "#          delaying_primitives\n",
        "#         + [rolling_mean_primitive, rolling_min_primitive]\n",
        "#     ),\n",
        "#     # cutoff_time=pd.Timestamp(\"2019-03-01\"),\n",
        "# )\n",
        "# data_fe.dropna(inplace=False)\n",
        "\n",
        "# data_fe.head()"
      ],
      "metadata": {
        "id": "W3hwamNEDLrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_grouped = data_fe.dropna()\n",
        "\n",
        "# data_train, data_valid = split_data_fe(data_grouped, window_size, horizon)\n",
        "\n",
        "# # Ojo aca, que estoy escalando mes y ano\n",
        "# scaler = MinMaxScaler()\n",
        "# data_train_norm = pd.DataFrame(scaler.fit_transform(data_train), columns=data_train.columns, index=data_train.index)\n",
        "# data_valid_norm = pd.DataFrame(scaler.transform(data_valid), columns=data_valid.columns, index=data_valid.index)\n",
        "\n",
        "# # Creo los Windowed Datasets\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# # Parmetros del modelo\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "# n_features = data_train.shape[1]\n",
        "# callbacks = MyCallbacks(patience)\n",
        "\n",
        "# # Modelo\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data=data_valid_windowed,\n",
        "#     callbacks=callbacks,\n",
        "#     verbose=0, # Poner en 0 si se hacen bucles largos\n",
        "#     epochs=epochs\n",
        "# )\n",
        "# plot_history(history)\n",
        "\n",
        "# generate_predictions(model, data_valid_norm, scaler)"
      ],
      "metadata": {
        "id": "xVXUIuuNavTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 07/08 - Productos con malas predicciones"
      ],
      "metadata": {
        "id": "TeXjIREO0IB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_predicciones_dec2019.head()"
      ],
      "metadata": {
        "id": "8ezSCAU__-Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Acotamos el bucle, solo los productos que nos interesa\n",
        "# product_id_start = '20001'\n",
        "# product_id_end = '20001'\n",
        "# start_index = data_productos_a_predecir.index.get_loc(product_id_start)\n",
        "# # end_index = data_productos_a_predecir.index.get_loc(product_id_end) + 1  # +1 to include the end index\n",
        "\n",
        "# for producto in data_productos_a_predecir.index[start_index:end_index]:\n",
        "#   plot_product(data, producto, pipeline_type)\n",
        "#   filter_productos_hermanos(data, producto)\n",
        "\n",
        "\n",
        "# # Esto puede servir para entrenar varios modelos, agrupados por productos hermanos"
      ],
      "metadata": {
        "id": "fjz7KiMf6Wq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 07/07 - Por Cliente GENERAL"
      ],
      "metadata": {
        "id": "uE-QvyZJ9n6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Inicializamos Dataframe de Predicciones\n",
        "# product_predictions = data_productos_a_predecir.copy()\n",
        "# product_predictions['Real'] = None\n",
        "# product_predictions['MinMax'] = None\n",
        "# product_predictions['Robust'] = None\n",
        "\n",
        "# # Acotamos el bucle, solo los productos que nos interesa\n",
        "# product_id_start = '20008'\n",
        "# product_id_end = '20008'\n",
        "# start_index = data_productos_a_predecir.index.get_loc(product_id_start)\n",
        "# end_index = data_productos_a_predecir.index.get_loc(product_id_end) + 1  # +1 to include the end index\n",
        "\n",
        "# for producto in data_productos_a_predecir.index[start_index:end_index]:\n",
        "#   print('Producto:', producto)\n",
        "#   # En caso de que quiera ver un producto en particular\n",
        "#   data_product = data.query(\"product_id == @producto\")\n",
        "\n",
        "#   # Agrupo el Dataframe\n",
        "#   data_grouped = group_data(data_product, 'product_id')\n",
        "#   product_predictions.loc[producto, 'Real'] = data_grouped.loc['2019-12'].values\n",
        "\n",
        "#   # Arreglo fechas en productos jovenes, o discontinuados\n",
        "#   data_grouped = fill_missing_dates(data_grouped, pipeline_type)\n",
        "\n",
        "#   # Modelo naif, asi que llenamos con bfill para atrasm y 0 para adelante\n",
        "#   data_grouped.bfill(inplace=True)\n",
        "#   data_grouped.fillna(0, inplace=True)\n",
        "\n",
        "#   # Arreglo Agosto 2019\n",
        "#   data_grouped = fix_aug2019(data_grouped, 'mean')\n",
        "#   # display(data_grouped)\n",
        "\n",
        "#   # Parametros para el Data Preprocessing\n",
        "#   n = 3\n",
        "#   horizon = 2\n",
        "#   window_size = 6\n",
        "#   batch_size = 32\n",
        "\n",
        "#   # Separo en Train y Test. Split para Feb2020, Split2 para Dic2019\n",
        "#   data_train, data_valid, data_test = split_data_dec2019(data_grouped, window_size)"
      ],
      "metadata": {
        "id": "ISlQIcBsdWLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # # Genero n predicciones usando RobustScaler como normalizador, y despues las promedio\n",
        "  # ##########################################################################\n",
        "  # # Robust Scaler primero\n",
        "  # print('Robust')\n",
        "  # scaler = RobustScaler() # Probar con transformaciones no-lineales\n",
        "  # data_train_norm = pd.DataFrame(scaler.fit_transform(data_train), columns=data_train.columns, index=data_train.index)\n",
        "  # data_valid_norm = pd.DataFrame(scaler.transform(data_valid), columns=data_valid.columns, index=data_valid.index)\n",
        "  # # data_test_norm = pd.DataFrame(scaler.transform(data_test), columns=data_test.columns, index=data_test.index)\n",
        "\n",
        "  # predictions_list = []\n",
        "\n",
        "  # for i in range(n):\n",
        "  #   # Creo los Windowed Datasets\n",
        "  #   data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "  #   data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "  #   # Parmetros del modelo\n",
        "  #   loss = 'mse'\n",
        "  #   optimizer = 'adam'\n",
        "  #   patience = 20\n",
        "  #   epochs = 500\n",
        "  #   n_features = data_train.shape[1]\n",
        "  #   callbacks = MyCallbacks(patience)\n",
        "\n",
        "  #   # Modelo\n",
        "  #   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "  #   history = model.fit(\n",
        "  #       data_train_windowed,\n",
        "  #       validation_data=data_valid_windowed,\n",
        "  #       callbacks=callbacks,\n",
        "  #       verbose=0, # Poner en 0 si se hacen bucles largos\n",
        "  #       epochs=epochs\n",
        "  #   )\n",
        "  #   plot_history(history) # Comentar si se hacen bucles largos\n",
        "\n",
        "  #   # Genero la prediccion, y la agrego al vector\n",
        "  #   prediction = generate_predictions(model, data_valid_norm, scaler)\n",
        "  #   predictions_list.append(prediction)\n",
        "\n",
        "  #   # Liberar memoria\n",
        "  #   del model\n",
        "  #   del data_train_windowed\n",
        "  #   del data_valid_windowed\n",
        "  #   clear_session()\n",
        "  #   gc.collect()\n",
        "\n",
        "  # # display(data_grouped.loc['2019-12'])\n",
        "  # # print('¨Prediccion:', np.mean(predictions_list))\n",
        "  # product_predictions.loc[producto, 'Robust'] = np.mean(predictions_list)"
      ],
      "metadata": {
        "id": "BaHO5vDhdSTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # # Lo mismo pero con MinMax Scaler\n",
        "  # ##########################################################################\n",
        "  # # MinMax Scaler segundo\n",
        "  # print('MinMax')\n",
        "  # scaler = MinMaxScaler() # Probar con transformaciones no-lineales\n",
        "  # data_train_norm = pd.DataFrame(scaler.fit_transform(data_train), columns=data_train.columns, index=data_train.index)\n",
        "  # data_valid_norm = pd.DataFrame(scaler.transform(data_valid), columns=data_valid.columns, index=data_valid.index)\n",
        "  # # data_test_norm = pd.DataFrame(scaler.transform(data_test), columns=data_test.columns, index=data_test.index) # No lo usamos\n",
        "\n",
        "  # predictions_list = []\n",
        "\n",
        "  # for i in range(n):\n",
        "  #   # Creo los Windowed Datasets\n",
        "  #   data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "  #   data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "  #   # Parmetros del modelo\n",
        "  #   loss = 'mse'\n",
        "  #   optimizer = 'adam'\n",
        "  #   patience = 20\n",
        "  #   epochs = 500\n",
        "  #   n_features = data_train.shape[1]\n",
        "  #   callbacks = MyCallbacks(patience)\n",
        "\n",
        "  #   # Modelo\n",
        "  #   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "  #   history = model.fit(\n",
        "  #       data_train_windowed,\n",
        "  #       validation_data=data_valid_windowed,\n",
        "  #       callbacks=callbacks,\n",
        "  #       verbose=1, # Poner en 0 si se hacen bucles largos\n",
        "  #       epochs=epochs\n",
        "  #   )\n",
        "  #   plot_history(history)\n",
        "\n",
        "  #   # Genero la prediccion, y la agrego al vector\n",
        "  #   prediction = generate_predictions(model, data_valid_norm, scaler)\n",
        "  #   predictions_list.append(prediction)\n",
        "\n",
        "  #   # Liberar memoria\n",
        "  #   del model\n",
        "  #   del data_train_windowed\n",
        "  #   del data_valid_windowed\n",
        "  #   clear_session()\n",
        "  #   gc.collect()\n",
        "\n",
        "  # # display(data_grouped.loc['2019-12'])\n",
        "  # # print('¨Prediccion:', np.mean(predictions_list))\n",
        "  # product_predictions.loc[producto, 'MinMax'] = np.mean(predictions_list)\n",
        "\n",
        "  # # # Exporta solo cuando el ultimo digito del ultimo producto coincide con el que ultimo del producto actual\n",
        "  # # # Para no exportar con cada producto, esto exporta 1 cada 10\n",
        "  # # if producto[-1] == product_id_end[-1]:\n",
        "  # #   filename = f\"{producto}.csv\"\n",
        "  # #   product_predictions.to_csv(path_data + filename)\n",
        "\n",
        "  # display(product_predictions.loc[producto].to_frame().T)"
      ],
      "metadata": {
        "id": "puPnpl9Cl55n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 07/07 - Walk-Forward Validation (TimeSeriesSplit)"
      ],
      "metadata": {
        "id": "mqoKvkm0kBw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # OJO QUE PUEDE FUNCIONAR, PROBAR DESPUES\n",
        "# # Se rompe el EarlyStopping, pero corre\n",
        "# # No lo pudimos hacer funcionar, asi que no continuamos con esta idea\n",
        "# # Pero esto junto con mejorar el FE seria lo primero a resolver en futuras iteraciones\n",
        "\n",
        "# n_splits = 3\n",
        "# n = 3 # Cantidad de experimentos por split\n",
        "\n",
        "# # Genero un backup del Dataframe sin agrupar\n",
        "# data = data_bkp.copy()\n",
        "\n",
        "# # En caso de que quiera ver un producto en particular\n",
        "# producto = '20001'\n",
        "# data_product = data.query(\"product_id == @producto\")\n",
        "\n",
        "# # Agrupo el Dataframe\n",
        "# data_grouped = group_data(data_product, 'product_id')\n",
        "\n",
        "# # Modelo naif, así que llenamos con bfill para atrás, y 0 para adelante\n",
        "# data_grouped.bfill(inplace=True)\n",
        "# data_grouped.fillna(0, inplace=True)\n",
        "\n",
        "# # Arreglo Agosto 2019\n",
        "# data_grouped = fix_aug2019(data_grouped, 'mean')\n",
        "\n",
        "# # Parámetros para el Data Preprocessing\n",
        "# horizon = 2\n",
        "# batch_size = 32\n",
        "# seed = 151\n",
        "# window_size = 3\n",
        "\n",
        "# # Separo en Train y Test usando TimeSeriesSplit\n",
        "# splits = split_data_timeseriessplit(data_grouped, n_splits)\n",
        "\n",
        "# # Normalizo\n",
        "# scaler = MinMaxScaler() # Probar con StandardScaler\n",
        "# data_grouped_norm = pd.DataFrame(scaler.fit_transform(data_grouped), columns=data_grouped.columns, index=data_grouped.index)\n",
        "\n",
        "# predictions_list = []\n",
        "\n",
        "# for train_index, test_index in splits:\n",
        "#     df_train = data_grouped_norm.iloc[train_index]\n",
        "#     df_test = data_grouped_norm.iloc[test_index]\n",
        "#     # Ajustar para asegurar suficientes datos en validación\n",
        "#     df_valid = df_test.iloc[:int(len(df_test) * 0.7)]  # Aumenta el tamaño del conjunto de validación\n",
        "#     df_test = df_test.iloc[int(len(df_test) * 0.7):]  # Reduce el tamaño del conjunto de prueba\n",
        "\n",
        "#     for i in range(n):\n",
        "#         # Creo los Windowed Datasets\n",
        "#         data_train_windowed = windowed_dataset(df_train, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#         data_valid_windowed = windowed_dataset(df_valid, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#         # Parámetros del modelo\n",
        "#         loss = 'mse'\n",
        "#         optimizer = 'adam'\n",
        "#         patience = 5\n",
        "#         epochs = 10 # Segun el valor del window_size, se rompe el EarlyStopping. Hay que poner un valor bajo en epochs\n",
        "#         n_features = df_train.shape[1]\n",
        "#         callbacks = MyCallbacks(patience) # Ojo con el window_size\n",
        "\n",
        "#         # Modelo\n",
        "#         model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "#         history = model.fit(\n",
        "#             data_train_windowed,\n",
        "#             validation_data=data_valid_windowed,\n",
        "#             callbacks=callbacks,\n",
        "#             verbose=0,\n",
        "#             epochs=epochs\n",
        "#         )\n",
        "\n",
        "#         # Genero la predicción, y la agrego al vector\n",
        "#         prediction = generate_predictions(model, df_valid, scaler)\n",
        "#         predictions_list.append(prediction)\n",
        "\n",
        "#         # Liberar memoria\n",
        "#         del model\n",
        "#         del data_train_windowed\n",
        "#         del data_valid_windowed\n",
        "#         clear_session()\n",
        "#         gc.collect()\n",
        "\n",
        "# display(f'Producto {producto}:', np.mean(predictions_list))"
      ],
      "metadata": {
        "id": "_0k4v257gCeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/30 - Por Cliente, Producto, Cat1, Cat2"
      ],
      "metadata": {
        "id": "JmYlG2l7rven"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index_start = 471\n",
        "# index_end = None\n",
        "\n",
        "# horizon = 2\n",
        "# window_size = 6\n",
        "# batch_size = 32\n",
        "\n",
        "# customers = group_data(data, 'customer_id').loc['2019'].sum().index\n",
        "\n",
        "# # Corte por Cliente\n",
        "# for customer in customers[index_start:index_end]:\n",
        "\n",
        "#     # Inicializo el Dataframe con las ventas del cliente\n",
        "#     predicciones_by_customer = data_productos_a_predecir.copy()\n",
        "#     predicciones_by_customer['tn'] = 0\n",
        "#     predicciones_by_customer_serie = predicciones_by_customer.squeeze()\n",
        "\n",
        "#     # Filtro el DataFrame con solo las ventas del Cliente\n",
        "#     data_customer = data.loc[data['customer_id'] == customer]\n",
        "\n",
        "#     # Corte por Cat1\n",
        "#     for cat1 in data_customer['cat1'].unique():\n",
        "#       # Corte por Cat2\n",
        "#       for cat2 in data_customer.loc[data_customer['cat1'] == cat1]['cat2'].unique():\n",
        "#         print(customer, cat1, cat2)\n",
        "\n",
        "#         # Filtro el DataFrame por Cat1 y Cat2\n",
        "#         data_customer_cat1_cat2 = data_customer.loc[(data_customer['cat1'] == cat1) & (data_customer['cat2'] == cat2)]\n",
        "\n",
        "#         # Agrupo los datos filtrados Cliente, Cat1, Cat2\n",
        "#         gruoped_data = group_data(data_customer_cat1_cat2, 'product_id')\n",
        "\n",
        "#         # Para los meses sin venta en los datos agrupados, los completo con NaN\n",
        "#         gruoped_data = fix_grouped_data(gruoped_data)\n",
        "\n",
        "#         # Fill los meses con NaN\n",
        "#         gruoped_data = fill_nulls(gruoped_data)\n",
        "\n",
        "#         # Arreglo Agosto 2019\n",
        "#         grouped_data = fix_aug2019(gruoped_data, 'mean')\n",
        "\n",
        "#         # Separo en Train y Test, aca hay que jugar con TimeSeriesSplit\n",
        "#         data_train, data_valid = split_data(grouped_data, window_size)\n",
        "#         # print(data_train.shape, data_valid.shape)\n",
        "\n",
        "#         # Normalizo\n",
        "#         scaler = MinMaxScaler() # Probar con StandardScaler\n",
        "#         data_train = pd.DataFrame(scaler.fit_transform(data_train), columns=data_train.columns, index=data_train.index)\n",
        "#         data_valid = pd.DataFrame(scaler.transform(data_valid), columns=data_valid.columns, index=data_valid.index)\n",
        "\n",
        "#         # Creo los Windowed Datasets\n",
        "#         data_train_windowed = windowed_dataset(data_train, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#         data_valid_windowed = windowed_dataset(data_valid, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#         # Parmetros del modelo\n",
        "#         loss = 'mse'\n",
        "#         optimizer = 'adam'\n",
        "#         patience = 30\n",
        "#         epochs = 500\n",
        "#         n_features = data_train.shape[1]\n",
        "#         callbacks = MyCallbacks(patience)\n",
        "\n",
        "#         model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "#         history = model.fit(\n",
        "#             data_train_windowed,\n",
        "#             validation_data=data_valid_windowed,\n",
        "#             callbacks=callbacks,\n",
        "#             verbose=0,\n",
        "#             epochs=epochs\n",
        "#         )\n",
        "\n",
        "#         plot_history(history)\n",
        "\n",
        "#         #generate_predictions_6(model_trained, data_valid, scaler):\n",
        "\n",
        "#         predictions = generate_predictions(model, data_valid, scaler)\n",
        "#         predicciones_by_customer_serie = sumar_series(predicciones_by_customer_serie, predictions)\n",
        "\n",
        "#         # Liberar memoria\n",
        "#         del model\n",
        "#         del data_train\n",
        "#         del data_valid\n",
        "#         del data_train_windowed\n",
        "#         del data_valid_windowed\n",
        "#         clear_session()\n",
        "#         gc.collect()\n",
        "\n",
        "#     filename = f\"{customer}.csv\"\n",
        "#     predicciones_by_customer_serie.to_csv(path_data + filename)"
      ],
      "metadata": {
        "id": "AbJLVwG54Jws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/29 - Predicciones por Cliente\n"
      ],
      "metadata": {
        "id": "bdAWZg5_kG8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones para el manejo de archivos\n",
        "\n",
        "# Que predicciones me falta calcular\n",
        "# check_preddictions_files(path_data + 'Predicciones_cat1_cat2//')\n",
        "\n",
        "# Genera un unico archivo de predicciones, concatenando las predicciones indifiduales\n",
        "# predictions = generate_predictions_from_files(path_predicciones)\n",
        "\n",
        "# to_kaggle_2(predictions.sum(axis=1).sort_index(), \"por_cliente_win6\")"
      ],
      "metadata": {
        "id": "e7d7EYiAkKJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/25 - Categorias Extendidas"
      ],
      "metadata": {
        "id": "RHIZ85HQoj14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# customers = group_data(data, 'customer_id').loc['2019'].sum().index\n",
        "\n",
        "# for customer in customers[0:]:\n",
        "#     predicciones_by_customer = data_productos_a_predecir.copy()\n",
        "#     predicciones_by_customer['tn'] = 0\n",
        "#     predicciones_by_customer_serie = predicciones_by_customer.squeeze()\n",
        "\n",
        "#     data_customer = data.loc[data['customer_id'] == customer]\n",
        "#     for customer_cat1 in data_customer['cat1'].unique():\n",
        "#         for customer_cat1_cat2 in data_customer.loc[data_customer['cat1'] == customer_cat1]['cat2'].unique():\n",
        "#             for customer_cat1_cat2_cat3 in data_customer.loc[(data_customer['cat1'] == customer_cat1) & (data_customer['cat2'] == customer_cat1_cat2)]['cat3'].unique():\n",
        "#                 data_customer_cat1_cat2_cat3 = data_customer.loc[(data_customer['cat1'] == customer_cat1) & (data_customer['cat2'] == customer_cat1_cat2) & (data_customer['cat3'] == customer_cat1_cat2_cat3)]\n",
        "#                 data_customer_cat1_cat2_cat3_grouped = group_data(data_customer_cat1_cat2_cat3, 'product_id')\n",
        "#                 data_customer_cat1_cat2_cat3_grouped_fillna = fill_nulls(data_customer_cat1_cat2_cat3_grouped)\n",
        "#                 # data_customer_cat1_cat2_cat3_grouped_fillna_fixaug2019 = fix_aug2019(data_customer_cat1_cat2_cat3_grouped_fillna, 'julplus10') # ojo con esto\n",
        "\n",
        "#                 split_strategy = 'S1'\n",
        "#                 normalization = 'MinMax'\n",
        "#                 horizon = 2\n",
        "#                 window_size = 6\n",
        "#                 batch_size = 32\n",
        "\n",
        "#                 data_train, data_valid = split_data(data_customer_cat1_cat2_cat3_grouped_fillna, window_size)\n",
        "#                 print(f'{customer}, {customer_cat1}, {customer_cat1_cat2}, {customer_cat1_cat2_cat3}')\n",
        "#                 print(data_train.shape[0], data_valid.shape[0])\n",
        "#                 if data_train.shape[0] >= (horizon + window_size) and data_valid.shape[0] >= (horizon + window_size):\n",
        "#                     data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "\n",
        "#                     data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#                     data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#                     model_name = 'M1'\n",
        "#                     loss = 'mse'\n",
        "#                     optimizer = 'adam'\n",
        "#                     patience = 30\n",
        "#                     epochs = 1000\n",
        "#                     n_features = data_train.shape[1]\n",
        "\n",
        "#                     callbacks = MyCallbacks(patience)\n",
        "#                     model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#                     history = model.fit(\n",
        "#                         data_train_windowed,\n",
        "#                         validation_data=data_valid_windowed,\n",
        "#                         callbacks=callbacks,\n",
        "#                         verbose=0,\n",
        "#                         epochs=epochs\n",
        "#                     )\n",
        "\n",
        "#                     # plot_history(history)\n",
        "\n",
        "#                     predictions = generate_predictions_4(model, data_valid_norm, norm_params)\n",
        "#                     predicciones_by_customer_serie = sumar_series(predicciones_by_customer_serie, predictions)\n",
        "\n",
        "#                     # Liberar memoria\n",
        "#                     del model\n",
        "#                     del data_train\n",
        "#                     del data_valid\n",
        "#                     del data_train_windowed\n",
        "#                     del data_valid_windowed\n",
        "#                     clear_session()\n",
        "#                     gc.collect()\n",
        "\n",
        "#     filename = f\"{customer}\"\n",
        "#     to_kaggle_2(predicciones_by_customer_serie, filename)\n",
        "#     print(filename)\n"
      ],
      "metadata": {
        "id": "V_0ln6r6My9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/24 - Entrenamiento a 2018-05"
      ],
      "metadata": {
        "id": "9j2ZF_Jixctg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Todos los productos\n",
        "# data_201805 = data_extended.loc[:'2018-05']\n",
        "# data_201807 = data_extended.loc['2018-07']\n",
        "\n",
        "# # # Filtando por categorias\n",
        "# # data_201805 = data_extended.loc[:'2018-05'].query('cat1 == \"FOODS\"') # PC, HC, FOODS\n",
        "# # data_201807 = data_extended.loc['2018-07'].query('cat1 == \"FOODS\"') # PC, HC, FOODS\n",
        "\n",
        "# print('Cantidad productos al 201805:', data_201805['product_id'].unique().shape)\n",
        "# print('Cantidad productos en 201807:', data_201807['product_id'].unique().shape)\n",
        "\n",
        "# ytrue = data_201807.groupby('product_id')['tn'].sum()\n",
        "# # print(ytrue.head())\n",
        "\n",
        "# data_201805_grouped = group_data(data_201805, 'product_id')\n",
        "# data_201805_grouped_fillna = fill_nulls(data_201805_grouped)\n",
        "# # display(data_201805_grouped_fillna)\n",
        "\n",
        "# # Ojo que en Mayo tene,ps 976 prodictos, pero en Julio solo 846"
      ],
      "metadata": {
        "id": "-dcoz3xigFFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split_strategy = 'S1'\n",
        "# normalization = 'MinMax'\n",
        "# window_size = None\n",
        "# horizon = 2\n",
        "# batch_size = None\n",
        "\n",
        "\n",
        "# # Model Variables: Dentro de cada Experimento, no son generales\n",
        "# n_features = None  # Esto va a depender de cada modelo, es el data_train.shape[1]\n",
        "# n_splits = None # No mas, la usabamos con el TimeSeriesSplit\n",
        "# # model_name = 'CAT1'\n",
        "# # loss = 'mse'\n",
        "# # optimizer = 'adam'\n",
        "# # patience = 30\n",
        "# # epochs = 10\n",
        "\n",
        "\n",
        "# window_size = 3\n",
        "# batch_size = 32\n",
        "\n",
        "# # Para el primer pipeline, entrenamiento correcto hasta 201805\n",
        "# data_train, data_valid = split_data_201805(data_201805_grouped_fillna, window_size)\n",
        "\n",
        "# data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "# print(data_train_norm.shape)\n",
        "\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "# n_features = data_train.shape[1]\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data=data_valid_windowed,\n",
        "#     callbacks=callbacks,\n",
        "#     verbose=0,\n",
        "#     epochs=epochs\n",
        "#     )\n",
        "\n",
        "# plot_history(history)\n",
        "# predictions = generate_predictions_3(model, data_valid_norm, norm_params)\n",
        "\n",
        "# # Filtrar para obtener solo los índices que están en ambas series\n",
        "# common_indices = ytrue.index.intersection(predictions.index)\n",
        "\n",
        "# # Crear un DataFrame concatenando las dos series usando solo los índices comunes\n",
        "# predictions_df = pd.concat([ytrue[common_indices], predictions[common_indices]], axis=1)\n",
        "# predictions_df.columns = ['true', 'predicted']\n",
        "\n",
        "# calcular_error(predictions_df)"
      ],
      "metadata": {
        "id": "2Lc3n6sULjSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/24 - Actividad en Clase: Entrenamiento con Datos del futuro (2018-07)"
      ],
      "metadata": {
        "id": "0hHUz3DaH72Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # OJO QUE ESTE ENTRENA HASTA 201807\n",
        "# # # no le cambio los nombres a la variable para que sea mas sencillo de correr\n",
        "\n",
        "# # Todos los productos\n",
        "# data_201805 = data_extended.loc[:'2018-07']\n",
        "# data_201807 = data_extended.loc['2018-07']\n",
        "\n",
        "# # # Filtando por categorias\n",
        "# # data_201805 = data_extended.loc[:'2018-07'].query('cat1 == \"PC\"') # PC, HC, FOODS\n",
        "# # data_201807 = data_extended.loc['2018-07'].query('cat1 == \"PC\"') # PC, HC, FOODS\n",
        "\n",
        "# print('Cantidad productos al 201805:', data_201805['product_id'].unique().shape)\n",
        "# print('Cantidad productos en 201807:', data_201807['product_id'].unique().shape)\n",
        "\n",
        "# ytrue = data_201807.groupby('product_id')['tn'].sum()\n",
        "# # print(ytrue.head())\n",
        "\n",
        "# data_201805_grouped = group_data(data_201805, 'product_id')\n",
        "# data_201805_grouped_fillna = fill_nulls(data_201805_grouped)\n",
        "# # display(data_201805_grouped_fillna)\n",
        "\n",
        "# # Ojo que en Mayo tene,ps 976 prodictos, pero en Julio solo 846"
      ],
      "metadata": {
        "id": "lFDcFWY5gLNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split_strategy = 'S1'\n",
        "# normalization = 'MinMax'\n",
        "# window_size = None\n",
        "# horizon = 2\n",
        "# batch_size = None\n",
        "\n",
        "\n",
        "# # Model Variables: Dentro de cada Experimento, no son generales\n",
        "# n_features = None  # Esto va a depender de cada modelo, es el data_train.shape[1]\n",
        "# n_splits = None # No mas, la usabamos con el TimeSeriesSplit\n",
        "# # model_name = 'CAT1'\n",
        "# # loss = 'mse'\n",
        "# # optimizer = 'adam'\n",
        "# # patience = 30\n",
        "# # epochs = 10\n",
        "\n",
        "\n",
        "\n",
        "# window_size = 3\n",
        "# batch_size = 32\n",
        "\n",
        "# # Segudno Pipeline, validacion con datos del futuro hasta 201807\n",
        "# data_train, data_valid = split_data_201807(data_201805_grouped_fillna, window_size)\n",
        "\n",
        "# data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "# print(data_train_norm.shape)\n",
        "\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "# n_features = data_train.shape[1]\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data=data_valid_windowed,\n",
        "#     callbacks=callbacks,\n",
        "#     verbose=0,\n",
        "#     epochs=epochs\n",
        "#     )\n",
        "\n",
        "# plot_history(history)\n",
        "# predictions = generate_predictions_3(model, data_valid_norm, norm_params)\n",
        "\n",
        "# # Filtrar para obtener solo los índices que están en ambas series\n",
        "# common_indices = ytrue.index.intersection(predictions.index)\n",
        "\n",
        "# # Crear un DataFrame concatenando las dos series usando solo los índices comunes\n",
        "# predictions_df = pd.concat([ytrue[common_indices], predictions[common_indices]], axis=1)\n",
        "# predictions_df.columns = ['true', 'predicted']\n",
        "\n",
        "# calcular_error(predictions_df)"
      ],
      "metadata": {
        "id": "uo1QU5XWHW9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/20 - Prediccion de Producto, Multivariada por Clientes"
      ],
      "metadata": {
        "id": "mKJiBByKaaXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Inicializamos el vector de predicciones\n",
        "# vector_predictions = data_productos_a_predecir.copy()\n",
        "# vector_predictions['tn'] = 0\n",
        "# # vector_predictions['nulls'] = 0\n",
        "\n",
        "# # Eliminamos los productos que no nos interesan\n",
        "# data_products_filtered = filter_products_data(data_bkp, data_productos_a_predecir)\n",
        "\n",
        "# # Iteramos por cada producto\n",
        "# for product in data_productos_a_predecir.index[1:2]:\n",
        "#   # print(f'Producto {product}')\n",
        "\n",
        "#   data_product_grouped = group_data(data_products_filtered, 'customer_id')\n",
        "#   # print(data_product_grouped.shape)\n",
        "\n",
        "#   data_product_grouped_customer = data_product_grouped.columns\n",
        "#   # print(data_product_grouped_customer)\n",
        "\n",
        "#   # Inicializamos el vector predicciones por cliente, para ese producto\n",
        "#   vector_clientes = pd.DataFrame(pd.Series(data_product_grouped_customer.copy(), name='customer'))\n",
        "#   vector_clientes.set_index('customer', inplace=True)\n",
        "#   vector_clientes['tn'] = 0\n",
        "#   vector_clientes['nulls'] = 0\n",
        "\n",
        "#   # print(vector_clientes.head())\n",
        "\n",
        "#   # Recorremos las ventas de cada customer\n",
        "\n",
        "#   for customer in data_product_grouped_customer[3:5]:\n",
        "#     print(f'Producto {product}')\n",
        "#     print(f'Customer {customer}')\n",
        "#     data_product = data_product_grouped[customer].copy()\n",
        "#     cant_nulos = data_product.isnull().sum()\n",
        "#     data_product = data_product.to_frame()\n",
        "#     data_product.fillna(0, inplace=True)\n",
        "#     # print(data_product)\n",
        "\n",
        "\n",
        "#     # Añadir indicador de disponibilidad al producto\n",
        "#     # first_sale_date = data_product[data_product > 0].index[0]\n",
        "#     data_product['available'] = (data_product > 0).astype(int)\n",
        "#     # print(data_product)\n",
        "\n",
        "#     # Jugamos con esto acá, para ver el tema de la validación en el entrenamiento\n",
        "#     window_size = 6\n",
        "#     batch_size = 1\n",
        "\n",
        "#     data_train, data_valid = split_data(data_product, window_size)\n",
        "#     data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "\n",
        "#     # Convertir DataFrame a numpy array\n",
        "#     train_sequence = data_train_norm.values\n",
        "#     valid_sequence = data_valid_norm.values\n",
        "\n",
        "#     data_train_windowed = windowed_dataset(train_sequence, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#     data_valid_windowed = windowed_dataset(valid_sequence, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "#       # Model Variables\n",
        "#     model_name = 'xx'\n",
        "#     loss = 'mse'\n",
        "#     optimizer = 'adam'\n",
        "#     patience = 30\n",
        "#     epochs = 300\n",
        "#     n_features = data_train.shape[1]\n",
        "\n",
        "#     callbacks = MyCallbacks(patience)\n",
        "#     model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#     history = model.fit(\n",
        "#         data_train_windowed,\n",
        "#         validation_data=data_valid_windowed,\n",
        "#         callbacks=callbacks,\n",
        "#         verbose=1,\n",
        "#         epochs=epochs\n",
        "#     )\n",
        "\n",
        "#     plot_history(history)\n",
        "#     predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "\n",
        "#     # Vamos sumando en el vector de acum\n",
        "#     value_to_add = predictions.loc['2020-02'].values[0]\n",
        "#     vector_clientes.loc[customer, 'nulls'] = cant_nulos\n",
        "#     vector_clientes.loc[customer, 'tn'] += value_to_add\n",
        "#     # print(vector_clientes)\n",
        "\n",
        "#   # print(vector_predictions.head())\n",
        "#   filename_cliente = f\"{product}_win{window_size}.csv\"\n",
        "#   # print(filename)\n",
        "#   vector_clientes.to_csv(filename_cliente, header=True, index=True)\n",
        "\n",
        "#   vector_predictions.loc[product, 'tn'] += vector_clientes['tn'].sum()\n",
        "\n",
        "# filename_predicciones = f\"SumaPredicciones_win{window_size}.csv\"\n",
        "# # print(filename)\n",
        "# vector_predictions.to_csv(filename_predicciones, header=True, index=True)\n"
      ],
      "metadata": {
        "id": "oJRVKBJnHV2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Analizamos la prediccion de un producto en particular, generanzo una matriz multivariada con columnas por cliente\n",
        "\n",
        "# # Producto 20001\n",
        "# data_20001 = data_bkp.query('product_id == \"20001\"')[['customer_id', 'tn']].copy()\n",
        "\n",
        "# print(data_20001.shape)\n",
        "# display(data_20001.head())\n",
        "\n",
        "# # plot_grouped_data_heatmap(data_20001, 'customer_id')\n",
        "\n",
        "# # No siempre se vende el producto estrella, inclusve as los clientes mas importantes\n",
        "\n",
        "# data_products_filtered = filter_products_data(data_bkp, data_productos_a_predecir)\n",
        "# print(data_products_filtered.shape)\n",
        "\n",
        "\n",
        "# # Creamos el Dataframe de ventas agrupadas por cliente/producto seleccionado\n",
        "# data_20001_grouped = group_data(data_20001, 'customer_id')\n",
        "\n",
        "# # Creamos el vector de clientes, pero solo los que tuvieron ventas del producto 20001\n",
        "# data_20001_customers = data_20001_grouped.columns\n",
        "\n",
        "# # Inicializamos el vector de predicciones\n",
        "# vector_clientes_20001 = pd.DataFrame(pd.Series(data_20001_customers.copy(), name='customer'))\n",
        "# vector_clientes_20001.set_index('customer', inplace=True)\n",
        "# vector_clientes_20001['tn'] = 0\n",
        "# vector_clientes_20001['nulls'] = 0\n",
        "# vector_clientes_20001.head()\n",
        "\n",
        "\n",
        "\n",
        "# for customer in data_20001_customers:\n",
        "#   # print(customer)\n",
        "#   data_product = data_20001_grouped[customer].copy()\n",
        "#   cant_nulos = data_product.isnull().sum()\n",
        "#   data_product = data_product.to_frame()\n",
        "#   data_product.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "#     # Añadir indicador de disponibilidad al producto\n",
        "#   first_sale_date = data_product[data_product > 0].index[0]\n",
        "#   data_product['available'] = (data_product > 0).astype(int)\n",
        "\n",
        "#   # Jugamos con esto acá, para ver el tema de la validación en el entrenamiento\n",
        "#   window_size = 6\n",
        "#   batch_size = 1\n",
        "\n",
        "#   data_train, data_valid = split_data(data_product, window_size)\n",
        "#   data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "\n",
        "#   # Convertir DataFrame a numpy array\n",
        "#   train_sequence = data_train_norm.values\n",
        "#   valid_sequence = data_valid_norm.values\n",
        "\n",
        "#   data_train_windowed = windowed_dataset(train_sequence, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(valid_sequence, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'xx'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 30\n",
        "#   epochs = 300\n",
        "#   n_features = data_train.shape[1]\n",
        "\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       validation_data=data_valid_windowed,\n",
        "#       callbacks=callbacks,\n",
        "#       verbose=1,\n",
        "#       epochs=epochs\n",
        "#   )\n",
        "\n",
        "#   plot_history(history)\n",
        "\n",
        "#   predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "\n",
        "#   # Vamos sumando en el vector de acum\n",
        "#   value_to_add = predictions.loc['2020-02'].values[0]\n",
        "#   vector_clientes_20001.loc[customer, 'nulls'] = cant_nulos\n",
        "#   vector_clientes_20001.loc[customer, 'tn'] += value_to_add\n",
        "#   # vector_predictions.loc[predictions.name] += value_to_add\n",
        "#   # vector_predictions.loc[predictions.name, 'nulls'] = cant_nulos\n",
        "\n",
        "# # print(vector_predictions.head())\n",
        "# filename = f\"PrediccionesIndividuales{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "# # print(filename)\n",
        "# vector_clientes_20001.to_csv(filename, header=True, index=True)\n"
      ],
      "metadata": {
        "id": "uw9gbDzXf0Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/19 - Union de Distintas Predicciones"
      ],
      "metadata": {
        "id": "z8l2uyPVM23-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # #########################################################################\n",
        "# # # Decomisionado                                                         #\n",
        "# # #########################################################################\n",
        "\n",
        "\n",
        "# BASELINE.set_index('product_id', inplace=True)\n",
        "# completos = pd.read_csv('completos.csv')\n",
        "# completos.set_index('product_id', inplace=True)\n",
        "# completos.rename(columns={'tn': 'tn_comp'}, inplace=True)\n",
        "# incompletos = pd.read_csv('incompletos.csv')\n",
        "# incompletos.set_index('product_id', inplace=True)\n",
        "# incompletos.rename(columns={'tn': 'tn_inco'}, inplace=True)\n",
        "# data = BASELINE.join(completos, on='product_id', how='left').join(incompletos, on='product_id', how='left')\n",
        "\n",
        "# # Crear la columna tn_cand con las condiciones especificadas\n",
        "# data['tn_cand'] = np.where(data['tn_inco'].isna(), data['tn_comp'],\n",
        "#                            np.where(data['tn_comp'].isna(), data['tn_inco'], np.nan))\n",
        "\n",
        "\n",
        "\n",
        "# data['diff'] = abs((data['tn'] - data['tn_cand']))\n",
        "# data.loc[data['nulls'] > 24, 'tn_cand'] = data['tn']\n",
        "\n",
        "# casos_particualres = [20022, 20001, 20071]\n",
        "# data.loc[casos_particualres, 'tn_cand'] = data.loc[casos_particualres, 'tn']"
      ],
      "metadata": {
        "id": "o1XKtVI8LUUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/18 - Pruebas contra Diciembre 2019, nos olvidamos de Kaggle"
      ],
      "metadata": {
        "id": "-ORvfte2nIee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # #########################################################################\n",
        "# # # Decomisionado                                                         #\n",
        "# # #########################################################################\n",
        "\n",
        "\n",
        "# # Separamos el Dataframe,\n",
        "# mask_productos_completos = data.isnull().sum() == 0\n",
        "# data_productos_completos = data.loc[:, mask_productos_completos]\n",
        "# data_productos_incompletos = data.loc[:, ~mask_productos_completos]\n",
        "# print(data_productos_completos.shape)\n",
        "# print(data_productos_incompletos.shape)\n",
        "\n",
        "# # Productos Incompletos\n",
        "# # Inicializamos el vector de predicciones\n",
        "# vector_predictions = data_productos_a_predecir[~mask_productos_completos].copy()\n",
        "# vector_predictions['tn'] = 0\n",
        "# vector_predictions['nulls'] = 0\n",
        "\n",
        "# for product in data_productos_incompletos.columns:\n",
        "#     print(product)\n",
        "\n",
        "#     # Modificar el valor del filtro, para analizar otros Productos. Ojo que hay productos que no tenemos todos los años y pincha\n",
        "#     data_product = data_productos_incompletos[product].copy()\n",
        "#     cant_nulos = data_product.isnull().sum()\n",
        "#     data_product = data_product.to_frame()\n",
        "#     data_product.fillna(0, inplace=True)\n",
        "\n",
        "#     # Añadir indicador de disponibilidad al producto\n",
        "#     first_sale_date = data_product[data_product > 0].index[0]\n",
        "#     data_product['available'] = (data_product.index >= first_sale_date).astype(int)\n",
        "#     # display(data_product.head())\n",
        "\n",
        "#     # Jugamos con esto acá, para ver el tema de la validación en el entrenamiento\n",
        "#     window_size = 6\n",
        "#     batch_size = 1\n",
        "\n",
        "#     data_train, data_valid = split_data(data_product, window_size)\n",
        "#     data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "\n",
        "#     # Convertir DataFrame a numpy array\n",
        "#     train_sequence = data_train_norm.values\n",
        "#     valid_sequence = data_valid_norm.values\n",
        "\n",
        "#     data_train_windowed = windowed_dataset(train_sequence, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#     data_valid_windowed = windowed_dataset(valid_sequence, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#     # Model Variables\n",
        "#     model_name = 'xx'\n",
        "#     loss = 'mse'\n",
        "#     optimizer = 'adam'\n",
        "#     patience = 30\n",
        "#     epochs = 300\n",
        "#     n_features = data_train.shape[1]\n",
        "\n",
        "#     callbacks = MyCallbacks(patience)\n",
        "#     model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#     history = model.fit(\n",
        "#         data_train_windowed,\n",
        "#         validation_data=data_valid_windowed,\n",
        "#         callbacks=callbacks,\n",
        "#         verbose=1,\n",
        "#         epochs=epochs\n",
        "#     )\n",
        "\n",
        "#     plot_history(history)\n",
        "\n",
        "#     predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "\n",
        "#     # Vamos sumando en el vector de acum\n",
        "#     value_to_add = predictions.loc['2020-02'].values[0]\n",
        "#     vector_predictions.loc[predictions.name] += value_to_add\n",
        "#     vector_predictions.loc[predictions.name, 'nulls'] = cant_nulos\n",
        "\n",
        "# print(vector_predictions.head())\n",
        "# filename = f\"ProductosIncompletos2_{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "# print(filename)\n",
        "# vector_predictions.to_csv(filename, header=True, index=True)\n",
        "\n",
        "\n",
        "# # Inicializamos el vector de predicciones\n",
        "# vector_predictions = data_productos_a_predecir[mask_productos_completos].copy()\n",
        "# vector_predictions['tn'] =0\n",
        "# # vector_predictions = vector_predictions.squeeze()\n",
        "\n",
        "# for product in data_productos_completos.columns:\n",
        "#   print(product)\n",
        "\n",
        "#   # Modificar el valor del filtro, para analizar otros Productos. Ojo que hay productos que no tenemos todos los anos y pincha\n",
        "#   data_product = data_productos_completos[product]\n",
        "\n",
        "#   # Jugamos con esto aca, para ver el tema de la validacion en el entrenamiento\n",
        "#   window_size = 6\n",
        "#   batch_size = 1\n",
        "\n",
        "\n",
        "#   data_train, data_valid = split_data(data_product, window_size)\n",
        "#   data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "#   # print(data_train.index)\n",
        "#   # print(data_valid.index)\n",
        "#   data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# #   # Model Variables\n",
        "#   model_name = 'xx'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 30\n",
        "#   epochs = 300\n",
        "#   n_features = data_train.shape[1]\n",
        "\n",
        "\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       validation_data = data_valid_windowed,\n",
        "#       callbacks = callbacks,\n",
        "#       verbose=0,\n",
        "#       epochs=epochs)\n",
        "\n",
        "#   # plot_history(history)\n",
        "\n",
        "#   predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "#   # plot_predictions_dec2019(data_product, predictions)\n",
        "\n",
        "#   # Vamos sumando en el vector de acum\n",
        "#   value_to_add = predictions.loc['2020-02'].values[0]\n",
        "#   vector_predictions.loc[predictions.name]+= value_to_add\n",
        "\n",
        "# print(vector_predictions.head())\n",
        "# filename = f\"ProductosCompletos_{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "# print(filename)\n",
        "# vector_predictions.to_csv(filename, header=True, index=False)"
      ],
      "metadata": {
        "id": "Jf49KKPC_uGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/17 - Ventas por dia"
      ],
      "metadata": {
        "id": "1HbokWwRHZEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data = data_bkp.copy()\n",
        "# data.sort_values(by=['periodo', 'customer_id', 'product_id'])\n",
        "# data = data[['customer_id', 'product_id', 'tn']]\n",
        "# data = data.merge(data_productos_a_predecir_con_categorias, left_on='product_id', right_on='product_id', how='left')\n",
        "# data.set_index(data_extended.index, inplace=True)\n",
        "# data.sort_index(inplace=True)\n",
        "# data.sort_values(by=['periodo', 'customer_id', 'product_id'], inplace=True)\n",
        "\n",
        "\n",
        "# # # Vemos los nulos por variable\n",
        "# # print(data_merged.isnull().sum())\n",
        "\n",
        "\n",
        "# mask_cat1_null = data['cat1'].isnull()\n",
        "# # display(data_merged[mask_cat1_null])\n",
        "\n",
        "# # # Ninguno de estos productos nos interesa, no estan en nuestas categorias\n",
        "# # print(data_productos_a_predecir_con_categorias.index.isin(data_merged[mask_cat1_null]['product_id']).sum())\n",
        "# # print(data_merged.shape[0])\n",
        "\n",
        "# # Elimino los nulos\n",
        "# data.dropna(subset=['cat1'], inplace=True)\n",
        "# # print(\"Nulos:\", data.isnull().sum())\n",
        "# # print(data.shape)\n",
        "\n",
        "\n",
        "# # Contar las ocurrencias de cada combinación de periodo, customer_id y product_id\n",
        "# repeated_entries = data.groupby(['periodo', 'customer_id', 'product_id']).size()\n",
        "\n",
        "# # Filtrar combinaciones que aparecen más de una vez\n",
        "# repeated_entries = repeated_entries[repeated_entries > 1]\n",
        "\n",
        "# # Mostrar las combinaciones repetidas\n",
        "# print(repeated_entries)\n",
        "\n",
        "# # sin combinaciones repetidas\n",
        "\n",
        "# print(data.shape)\n",
        "# display(data.head())\n",
        "\n",
        "# # No hay ventas por dias, todos los registros son mensuales. Una perdida de tiempo\n",
        "\n",
        "# # # restoreo\n",
        "# # data = data_filtered.copy()"
      ],
      "metadata": {
        "id": "N6XR0zgQTFJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ############################################################################################################\n",
        "# # # Verificamos los clientes Activos\n",
        "# ############################################################################################################\n",
        "# # Generamos la matriz de presencia\n",
        "# presencia_clientes = data.groupby(['periodo', 'customer_id']).size().unstack(fill_value=0)\n",
        "# presencia_clientes[presencia_clientes > 0] = 1  # Convertimos las cantidades en 1 para indicar presencia\n",
        "# mask_active_clients = presencia_clientes.loc['2019-10':'2019-12'].sum(axis=0) == 3\n",
        "\n",
        "# # Configuramos el tamaño de la figura\n",
        "# plt.figure(figsize=(15, 10))\n",
        "\n",
        "# # Creamos el heatmap\n",
        "# sns.heatmap(presencia_clientes, cmap='viridis', cbar=False, linewidths=.5)\n",
        "\n",
        "# # Añadimos los títulos y etiquetas\n",
        "# plt.title('Presencia de Clientes por Mes')\n",
        "# plt.xlabel('Clientes')\n",
        "# plt.ylabel('Mes')\n",
        "\n",
        "# # Mostramos el gráfico\n",
        "# plt.show()\n",
        "# ############################################################################################################\n",
        "\n",
        "# # El cliente 10039 no compro en el ultimo semestre. Realizo 3 ordenes de compra en Junio 2019 por ultima vez\n",
        "# # Maybe podemos removerlos del experimento, ya que sabemos que no van a volver a comprar\n",
        "# # Cantidad de compras por mes para el cliente 10039\n",
        "# # display(data.groupby(['customer_id', 'periodo']).count()[['product_id']].loc[['10039']].tail())\n",
        "\n",
        "\n",
        "# presencia_clientes = data.groupby(['periodo', 'customer_id']).size().unstack(fill_value=0)\n",
        "# presencia_clientes[presencia_clientes > 0] = 1\n",
        "\n",
        "# # Vemos 173 clientes, no compraron en diciembre\n",
        "# print(presencia_clientes.loc['2019-12'].sum(axis=0).value_counts())\n",
        "\n",
        "\n",
        "# # Y que de esos 173, 111 tampoco compraron en noviembre\n",
        "# print(presencia_clientes.loc['2019-11':'2019-12'].sum(axis=0).value_counts())\n",
        "\n",
        "# # y de esos 111, 98 tampoco compraron en Octubre\n",
        "# print(presencia_clientes.loc['2019-10':'2019-12'].sum(axis=0).value_counts())\n",
        "\n",
        "\n",
        "# # print(mask_active_clients.shape)\n",
        "# # display(mask_active_clients.head())\n",
        "\n",
        "# # Concluimos que los 111 clientes que no nos compraron en Octibre, Noviembre ni Diciembre\n",
        "# # no son mas nuestros clietnes, y damos de baja sus ventas, ya que es muy probable\n",
        "# # que no nos vuelvan a comprar, aunque si un porcentje sera absosrvido por la competencia"
      ],
      "metadata": {
        "id": "P0D6gjK_54l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ###################################################################################################################\n",
        "# # 2024-06-07: Excluyo las ventas de los clientes marcados como desactivados\n",
        "# # Ventas de clientes que no compraron en los ultimos dos meses, eliminadas\n",
        "# # active_data = data[data['customer_id'].isin(mask_active_clients[mask_active_clients].index)]\n",
        "# active_data = data_filtered.copy()\n",
        "\n",
        "# # Pivotear los datos\n",
        "# data_grouped_active_clients = active_data.pivot_table(\n",
        "#     index='periodo',\n",
        "#     columns='product_id',\n",
        "#     values='tn',\n",
        "#     aggfunc='sum'\n",
        "# )\n",
        "\n",
        "# # Asegurarnos de que el índice esté en el formato correcto\n",
        "# data_grouped_active_clients.index = pd.to_datetime(data_grouped_active_clients.index)\n",
        "\n",
        "# # Ordenar el índice si fuera necesario\n",
        "# data_grouped_active_clients = data_grouped_active_clients.sort_index()\n",
        "# ####################################################################################################################\n",
        "# # Si quiero usar el Dataset con las ventas de los clientes desactivados excluidas\n",
        "# # Y trasca le completo los nulos\n",
        "# data_grouped = fill_nulls(data_grouped_active_clients)\n",
        "\n",
        "# # Relacion entre el promedio anual anterior, y el febrero siguiente\n",
        "# data_2017_mean = pd.Series(data_grouped.loc['2017'].mean(axis=0), name='2017_mean')\n",
        "# data_2018_mean = pd.Series(data_grouped.loc['2018'].mean(axis=0), name='2018_mean')\n",
        "# data_2019_mean = pd.Series(data_grouped.loc['2019'].mean(axis=0), name='2019_mean')\n",
        "# feb_2018 = pd.Series(data_grouped.loc['2018-02'].T.squeeze(), name='Feb_2018')\n",
        "# feb_2019 = pd.Series(data_grouped.loc['2019-02'].T.squeeze(), name='Feb_2019')\n",
        "\n",
        "# data = pd.concat([data_2017_mean, feb_2018, data_2018_mean, feb_2019, data_2019_mean], axis=1)\n",
        "\n",
        "# # display(data.head())\n",
        "\n",
        "# # Calculamos cuanto represetan las ventas de Febrero, con respecto al Mean de todo el ano anterior\n",
        "# ratio_2017 = pd.Series(data['Feb_2018'] / data['2017_mean'], name='Ratio_2017')\n",
        "# ratio_2018 = pd.Series(data['Feb_2019'] / data['2018_mean'], name='Ratio_2018')\n",
        "# data_ratios = pd.concat([ratio_2017, ratio_2018], axis=1)\n",
        "\n",
        "# # Nos enfocamos solo en los primeros 40 productos, los mas importantes\n",
        "# data_ratios[:40].plot(figsize=(25, 6))\n",
        "# plt.show()\n",
        "\n",
        "# # display(data_ratios.head())\n",
        "# # Vemos el huevo del producto #32, ya que no tenemos ventas todos los meses\n",
        "# # Vemos como son bastantes parecidas\n",
        "# # Se ve que el producto 19 fue suplementado por el 22 en las ventas del 2018\n",
        "\n",
        "\n",
        "# # Si el ratio del producto vendido entre el promedo del Ano y Febrero del ano siguiente es menor al 3%, sigue manteniendo ese ratio\n",
        "# data_ratios['diff3'] = (abs(data_ratios['Ratio_2017'] - data_ratios['Ratio_2018']) <= 0.03)\n",
        "# data_final = pd.concat([data_ratios, data_2019_mean], axis=1)\n",
        "\n",
        "# # Inflation Multiplier, dejar en 1 para que no modifique las predicciones\n",
        "# inflation = .92\n",
        "# data_final['tn'] = data_final.apply(lambda row: row['2019_mean'] * row['Ratio_2018'] * inflation if row['diff3'] else row['2019_mean'], axis=1)\n",
        "\n",
        "# # display(data_final.head())\n",
        "\n",
        "# # Formateo y exporto a Kaggle.\n",
        "# to_kaggle(data_final['tn'].reset_index(), name='modelo_loco_3diff_inlf92_active3')\n",
        "\n",
        "# # Guardo los productos que tengo que mejorar, los que los ratio de ventas con Febrero sobrepasan el 3%\n",
        "# mask_mejorar = ~ data_final['diff3']\n",
        "# ####################################################################################################################\n",
        "# # Comentar esta linea para que no se excluyan los clientes desactivados\n",
        "# data_grouped = data_grouped_active_clients\n",
        "\n",
        "# # Igual que Diciembre 2020 (ultimos datos)\n",
        "# pred_202012 = data_grouped.loc['2019-12'].T.reset_index()\n",
        "# pred_202012.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_202012.to_csv('BASELINE-pred_202012.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 3 meses\n",
        "# pred_mean3 = data_grouped.loc['2019-10':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean3.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean3.to_csv('BASELINE-pred_mean3.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 6 meses\n",
        "# pred_mean6 = data_grouped.loc['2019-07':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean6.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean6.to_csv('BASELINE-pred_mean6.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 12 meses\n",
        "# pred_mean12 = data_grouped.loc['2019-01':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean12.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean12.to_csv('BASELINE-pred_mean12_act3.csv', header=True, index=False)\n",
        "# pred_mean12.head()\n",
        "\n",
        "# # restoreo\n",
        "# data = data_filtered.copy()"
      ],
      "metadata": {
        "id": "Zg8foW6XMcMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/17 - Concatenamos Estadistica con Redes Neuronales"
      ],
      "metadata": {
        "id": "-sB43EDztBui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # De la noebook anterior\n",
        "\n",
        "# # Relacion entre el promedio anual anterior, y el febrero siguiente\n",
        "# data_2017_mean = pd.Series(data_grouped.loc['2017'].mean(axis=0), name='2017_mean')\n",
        "# data_2018_mean = pd.Series(data_grouped.loc['2018'].mean(axis=0), name='2018_mean')\n",
        "# data_2019_mean = pd.Series(data_grouped.loc['2019'].mean(axis=0), name='2019_mean')\n",
        "# feb_2018 = pd.Series(data_grouped.loc['2018-02'].T.squeeze(), name='Feb_2018')\n",
        "# feb_2019 = pd.Series(data_grouped.loc['2019-02'].T.squeeze(), name='Feb_2019')\n",
        "\n",
        "# data = pd.concat([data_2017_mean, feb_2018, data_2018_mean, feb_2019, data_2019_mean], axis=1)\n",
        "\n",
        "# # display(data.head())\n",
        "\n",
        "# # Calculamos cuanto represetan las ventas de Febrero, con respecto al Mean de todo el ano anterior\n",
        "# ratio_2017 = pd.Series(data['Feb_2018'] / data['2017_mean'], name='Ratio_2017')\n",
        "# ratio_2018 = pd.Series(data['Feb_2019'] / data['2018_mean'], name='Ratio_2018')\n",
        "# data_ratios = pd.concat([ratio_2017, ratio_2018], axis=1)\n",
        "\n",
        "# # # Nos enfocamos solo en los primeros 40 productos, los mas importantes\n",
        "# # data_ratios[:40].plot(figsize=(25, 6))\n",
        "# # plt.show()\n",
        "\n",
        "# # display(data_ratios.head())\n",
        "# # Vemos el huevo del producto #32, ya que no tenemos ventas todos los meses\n",
        "# # Vemos como son bastantes parecidas\n",
        "# # Se ve que el producto 19 fue suplementado por el 22 en las ventas del 2018\n",
        "\n",
        "\n",
        "# # Si el ratio del producto vendido entre el promedo del Ano y Febrero del ano siguiente es menor al 3%, sigue manteniendo ese ratio\n",
        "# data_ratios['diff3'] = (abs(data_ratios['Ratio_2017'] - data_ratios['Ratio_2018']) <= 0.03)\n",
        "# data_final = pd.concat([data_ratios, data_2019_mean], axis=1)\n",
        "\n",
        "# # Inflation Multiplier, dejar en 1 para que no modifique las predicciones\n",
        "# inflation = 0.90\n",
        "# data_final['tn'] = data_final.apply(lambda row: row['2019_mean'] * row['Ratio_2018'] * inflation if row['diff3'] else row['2019_mean'], axis=1)\n",
        "\n",
        "# # display(data_final.head())\n",
        "\n",
        "# # # Formateo y exporto a Kaggle.\n",
        "# # to_kaggle(data_final['tn'].reset_index(), name='modelo_loco_3diff_inlf90')\n",
        "\n",
        "# # Guardo los productos que tengo que mejorar, los que los ratio de ventas con Febrero sobrepasan el 3%\n",
        "# mask_mejorar = ~ data_final['diff3']\n",
        "\n",
        "# #########################################################################################\n",
        "# # Predecimos las ventas de productos que no fueron buenos usando la estadistica clasica\n",
        "# #########################################################################################\n",
        "# # # Viene de otro EDA, si da error hay que ejecutar ese analisis primero que genera esta variable\n",
        "# # mask_mejorar = ~ data_final['diff3']\n",
        "\n",
        "# products_list = mask_mejorar[mask_mejorar == True].index\n",
        "\n",
        "# # Probamos con los datos de los clientes activos only\n",
        "# data_grouped_filled = fill_nulls(data_grouped_active_clients)\n",
        "\n",
        "# # Probamos reemplazando la crisis de Agosto 2019 por Julio + 10% (visto en el EDA)\n",
        "# data_grouped_filled.drop(index='2019-08', axis=1, inplace=True)\n",
        "# data_agosto_2019_jul_plus10 = data_grouped_filled.loc['2019-07']*1.1\n",
        "# data_agosto_2019_jul_plus10.index = pd.to_datetime(['2019-08-01'])\n",
        "# data_grouped_filled = pd.concat([data_grouped_filled, data_agosto_2019_jul_plus10]).sort_index()\n",
        "# #########################################################################################\n",
        "\n",
        "# # Inicializamos el vector de predicciones\n",
        "# vector_predictions = data_productos_a_predecir.copy()\n",
        "# vector_predictions['tn'] =0\n",
        "# # vector_predictions = vector_predictions.squeeze()\n",
        "\n",
        "\n",
        "# for product in products_list[:3]:\n",
        "#   print(product)\n",
        "\n",
        "#   # Modificar el valor del filtro, para analizar otros Productos. Ojo que hay productos que no tenemos todos los anos y pincha\n",
        "#   data_product = data_grouped[product]\n",
        "\n",
        "#   # Jugamos con esto aca, para ver el tema de la validacion en el entrenamiento\n",
        "#   window_size = 12\n",
        "#   batch_size = 1\n",
        "\n",
        "\n",
        "#   data_train, data_valid = split_data(data_product, window_size)\n",
        "#   data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "#   # print(data_train.index)\n",
        "#   # print(data_valid.index)\n",
        "#   data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'xx'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 30\n",
        "#   epochs = 300\n",
        "#   n_features = data_train.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       validation_data = data_valid_windowed,\n",
        "#       callbacks = callbacks,\n",
        "#       verbose=0,\n",
        "#       epochs=epochs)\n",
        "\n",
        "#   # plot_history(history)\n",
        "\n",
        "#   predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "#   # plot_predictions_dec2019(data_product, predictions)\n",
        "\n",
        "#   # Vamos sumando en el vector de acum\n",
        "#   value_to_add = predictions.loc['2020-02'].values[0]\n",
        "#   vector_predictions.loc[predictions.name]+= value_to_add\n",
        "\n",
        "# display(vector_predictions.head(5))\n",
        "# vector_predictions.to_csv('predicciones_win12.csv')\n",
        "\n",
        "# # restoreo el Dataframe filtrado\n",
        "# data = data_filtered.copy()"
      ],
      "metadata": {
        "id": "xPSg0lhQtM6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/17 - Errores contra Diciembre 2019"
      ],
      "metadata": {
        "id": "70vs0Lkdlgsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analizamos las series de los productos mas vendidos, pero esta vez testeamos contra los ultimos dos meses del dataset Nov y Dev 2019\n",
        "\n",
        "# # # Inicializamos el vector de predicciones\n",
        "# # vector_predictions = data_productos_a_predecir.copy()\n",
        "# # vector_predictions['tn'] =0\n",
        "\n",
        "# # Modificar el valor del filtro, para analizar otros Productos. Ojo que hay productos que no tenemos todos los anos y pincha\n",
        "# data_product = data_grouped['20003']\n",
        "# data_product = group_data(data.query('product_id == \"20003\"'), 'product_id')\n",
        "\n",
        "# # Jugamos con esto aca, para ver el tema de la validacion en el entrenamiento\n",
        "# window_size = 6\n",
        "# batch_size = 1\n",
        "\n",
        "\n",
        "# data_train, data_valid, data_test = split_data_dec2019(data_product, window_size)\n",
        "# data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "# # print(data_train.index)\n",
        "# # print(data_valid.index)\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'xx'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 300\n",
        "# n_features = data_train.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=1,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# plot_history(history)\n",
        "\n",
        "# predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "# # plot_predictions_dec2019(data_product, predictions)\n",
        "# display(predictions)"
      ],
      "metadata": {
        "id": "ENuMHEl_nL-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/17 - Ratio Febrero con Promedio de anos anteriores (Modelo Loco Inception)"
      ],
      "metadata": {
        "id": "_a6ldKf-vb-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Ventas de clientes que no compraron en los ultimos dos meses, eliminadas\n",
        "# # active_data = data[data['customer_id'].isin(mask_active_clients[mask_active_clients].index)]\n",
        "# active_data = data.copy()\n",
        "\n",
        "# # Pivotear los datos\n",
        "# data_grouped_active_clients = active_data.pivot_table(\n",
        "#     index='periodo',\n",
        "#     columns='product_id',\n",
        "#     values='tn',\n",
        "#     aggfunc='sum'\n",
        "# )\n",
        "\n",
        "# # Si quiero usar el Dataset con las ventas de los clientes desactivados excluidas\n",
        "# # Y trasca le completo los nulos\n",
        "# data_grouped = fill_nulls(data_grouped_active_clients)\n",
        "\n",
        "# # Relacion entre el promedio anual anterior, y el febrero siguiente\n",
        "# data_2017_mean = pd.Series(data_grouped.loc['2017'].mean(axis=0), name='2017_mean')\n",
        "# data_2018_mean = pd.Series(data_grouped.loc['2018'].mean(axis=0), name='2018_mean')\n",
        "# data_2019_mean = pd.Series(data_grouped.loc['2019'].mean(axis=0), name='2019_mean')\n",
        "# feb_2018 = pd.Series(data_grouped.loc['2018-02'].T.squeeze(), name='Feb_2018')\n",
        "# feb_2019 = pd.Series(data_grouped.loc['2019-02'].T.squeeze(), name='Feb_2019')\n",
        "\n",
        "# data = pd.concat([data_2017_mean, feb_2018, data_2018_mean, feb_2019, data_2019_mean], axis=1)\n",
        "\n",
        "# # display(data.head())\n",
        "\n",
        "# # Calculamos cuanto represetan las ventas de Febrero, con respecto al Mean de todo el ano anterior\n",
        "# ratio_2017 = pd.Series(data['Feb_2018'] / data['2017_mean'], name='Ratio_2017')\n",
        "# ratio_2018 = pd.Series(data['Feb_2019'] / data['2018_mean'], name='Ratio_2018')\n",
        "# data_ratios = pd.concat([ratio_2017, ratio_2018], axis=1)\n",
        "\n",
        "# # Nos enfocamos solo en los primeros 40 productos, los mas importantes\n",
        "# data_ratios[:40].plot(figsize=(25, 6))\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # Si el ratio del producto vendido entre el promedo del Ano y Febrero del ano siguiente es menor al 3%, sigue manteniendo ese ratio\n",
        "# data_ratios['diff3'] = (abs(data_ratios['Ratio_2017'] - data_ratios['Ratio_2018']) <= 0.03)\n",
        "# data_final = pd.concat([data_ratios, data_2019_mean], axis=1)\n",
        "\n",
        "# # Inflation Multiplier, dejar en 1 para que no modifique las predicciones\n",
        "# # Esto lo probe bastante, y no logra mejoras. Lo excluimos dejando el multuplicador en 1\n",
        "# inflation = 1\n",
        "# data_final['tn'] = data_final.apply(lambda row: row['2019_mean'] * row['Ratio_2018'] * inflation if row['diff3'] else row['2019_mean'], axis=1)\n",
        "\n",
        "# display(data_final.head())\n",
        "\n",
        "\n",
        "# # # Formateo y exporto a Kaggle.\n",
        "# # to_kaggle(data_final['tn'].reset_index(), name='modelo_loco_3diff_inlf92_active3')\n",
        "\n",
        "# # # Guardo los productos que tengo que mejorar, los que los ratio de ventas con Febrero sobrepasan el 3%\n",
        "# # mask_mejorar = ~ data_final['diff3']\n",
        "\n",
        "# # Restoreo el Dataframe filtrado original\n",
        "# data = data_filtered.copy()"
      ],
      "metadata": {
        "id": "dcF-bI-2vgIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/16 - Error Analysis"
      ],
      "metadata": {
        "id": "GiO91wV2MWuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analizamos las series de los productos mas vendidos, vemos si las redes neuronales pueden entenderlas\n",
        "\n",
        "# Modificar el valor del filtro, para analizar otros Productos. Ojo que hay productos que no tenemos todos los anos y pincha\n",
        "# data_product = data_grouped['20024']\n",
        "data_product = group_data(data.query('product_id == \"20024\"'), 'product_id')\n",
        "\n",
        "\n",
        "data_train, data_valid = split_data_test(data_product)\n",
        "data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "# Model Variables\n",
        "model_name = 'xx'\n",
        "loss = 'mse'\n",
        "optimizer = 'adam'\n",
        "patience = 20\n",
        "epochs = 200\n",
        "n_features = data_train.shape[1]\n",
        "\n",
        "\n",
        "# Jugamos con esto aca, para ver el tema de la validacion en el entrenamiento\n",
        "window_size = 3\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "callbacks = MyCallbacks(patience)\n",
        "model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "history = model.fit(\n",
        "    data_train_windowed,\n",
        "    validation_data = data_valid_windowed,\n",
        "    callbacks = callbacks,\n",
        "    verbose=0,\n",
        "    epochs=epochs)\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "plot_predictions(data_product, predictions)"
      ],
      "metadata": {
        "id": "2Fr3EDZnMbQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/15 - BASELINE + TOP Productos"
      ],
      "metadata": {
        "id": "Waoyxx3TzKh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # PARA LOCAL ONLY, DEMORA MUCHO\n",
        "\n",
        "# #############################################################################################\n",
        "# #############################################################################################\n",
        "#  # El EDA tiene que estar arriba\n",
        "# mask_product_id_sold36 = (data_grouped > 0).sum(axis=0)==36\n",
        "# # print('Productos vendidos los 36 meses:', mask_product_id_sold36.sum())\n",
        "# # mask_product_id_sold12 = (data_grouped > 0).sum(axis=0)<=12\n",
        "# # print('Productos vendidos en 12 meses o menos:', mask_product_id_sold12.sum())\n",
        "\n",
        "# # Marco los que se vendieron los 36 meses\n",
        "# mask_products_id_top74 = mask_product_id_sold36\n",
        "\n",
        "# # No me interesan los +75\n",
        "# mask_products_id_top74.loc['20085':] = False\n",
        "# mask_products_id_top74.sum()\n",
        "# #############################################################################################\n",
        "# #############################################################################################\n",
        "# top_productos = data_productos_a_predecir[mask_products_id_top74].index\n",
        "\n",
        "# # # Prueba con los dos primeros\n",
        "# # top_productos = top_productos[:2]\n",
        "\n",
        "\n",
        "# # # Continuacion a mano desde producto 20017\n",
        "# # mask_17 = mask_products_id_top74.loc['20017':]\n",
        "# # data_17 = data_productos_a_predecir.loc['20017':]\n",
        "# # data_17[mask_17]\n",
        "# # top_productos = data_17[mask_17].index\n",
        "\n",
        "# data_prod_pred = data_productos_a_predecir.copy()\n",
        "# data_prod_pred['mean'] = 0\n",
        "# data_prod_pred['median'] = 0\n",
        "\n",
        "# for producto in top_productos:\n",
        "#   # print(f'Producto: {producto}')\n",
        "#   data_grouped_loc = group_data(data, 'product_id')[producto]\n",
        "#   # display(data_grouped_loc)\n",
        "\n",
        "#   data_train, data_valid = split_data(data_grouped_loc)\n",
        "#   data_train = pd.DataFrame(data_train)\n",
        "#   data_valid = pd.DataFrame(data_valid)\n",
        "#   # display(data_train.head())\n",
        "\n",
        "#   data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "#   # display(data_valid_norm.head())\n",
        "\n",
        "#   data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'M1'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 20\n",
        "#   epochs = 100\n",
        "#   interaciones = 10\n",
        "#   n_features = data_train.shape[1]\n",
        "#   # print(n_features)\n",
        "\n",
        "\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   # Promedio 810\n",
        "#   mean, median = model_train(epochs, interaciones)\n",
        "#   data_prod_pred.loc[producto, ['mean', 'median']] = mean, median\n",
        "\n",
        "# # data_prod_pred.to_csv('predicciones_top74.csv', header=True, index=True)"
      ],
      "metadata": {
        "id": "Rce7LpdHi__X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/15 - Nuevos BASELINES"
      ],
      "metadata": {
        "id": "J6t7e7l9bEeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #########################################################################\n",
        "# # Decomisionado                                                         #\n",
        "# #########################################################################\n",
        "\n",
        "\n",
        "# ####################################################################################################################\n",
        "# # 2024-06-07: Excluyo las ventas de los clientes marcados como desactivados\n",
        "# # Ventas de clientes que no compraron en los ultimos dos meses, eliminadas\n",
        "# active_data = data[data['customer_id'].isin(mask_active_clients[mask_active_clients].index)]\n",
        "\n",
        "# # Pivotear los datos\n",
        "# data_grouped_active_clients = active_data.pivot_table(\n",
        "#     index='periodo',\n",
        "#     columns='product_id',\n",
        "#     values='tn',\n",
        "#     aggfunc='sum'\n",
        "# )\n",
        "\n",
        "# # Asegurarnos de que el índice esté en el formato correcto\n",
        "# data_grouped_active_clients.index = pd.to_datetime(data_grouped_active_clients.index)\n",
        "\n",
        "# # Ordenar el índice si fuera necesario\n",
        "# data_grouped_active_clients = data_grouped_active_clients.sort_index()\n",
        "# ####################################################################################################################\n",
        "\n",
        "# # Comentar esta linea para que no se excluyan los clientes desactivados\n",
        "# data_grouped = data_grouped_active_clients\n",
        "\n",
        "# # Igual que Diciembre 2020 (ultimos datos)\n",
        "# pred_202012 = data_grouped.loc['2019-12'].T.reset_index()\n",
        "# pred_202012.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_202012.to_csv('BASELINE-pred_202012.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 3 meses\n",
        "# pred_mean3 = data_grouped.loc['2019-10':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean3.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean3.to_csv('BASELINE-pred_mean3.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 6 meses\n",
        "# pred_mean6 = data_grouped.loc['2019-07':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean6.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean6.to_csv('BASELINE-pred_mean6.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 12 meses\n",
        "# pred_mean12 = data_grouped.loc['2019-01':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean12.columns = ['product_id', 'tn']\n",
        "\n",
        "# pred_mean12.to_csv('BASELINE-pred_mean12_act4.csv', header=True, index=False)\n"
      ],
      "metadata": {
        "id": "Sy0QnKMklwzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/12 - Walk Forward Validation\n",
        "- Son muy pocos datos los que tenemos, no funciona bien"
      ],
      "metadata": {
        "id": "PCQSAqstQinS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # #########################################################################\n",
        "# # # Decomisionado                                                         #\n",
        "# # #########################################################################\n",
        "\n",
        "\n",
        "# # #########################################################################\n",
        "# # TimeSeriesSplit\n",
        "# # #########################################################################\n",
        "# n_splits = 3\n",
        "\n",
        "# scaler = MinMaxScaler() # Probar con transformaciones no-lineales\n",
        "# data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
        "# # data_norm = data_grouped(data, 'product_id')\n",
        "\n",
        "\n",
        "# # # TimeSeriesSplit: 3 splits para ejemplo\n",
        "# # tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# # # Almacenar las pérdidas para cada split\n",
        "# # split_losses = []\n",
        "\n",
        "# # # Inicializo el vector de predicciones\n",
        "# # predicciones_all = data_productos_a_predecir.copy()\n",
        "# # predicciones_all['tn'] = 0\n",
        "\n",
        "# # # Probar si esto se puede sacar del bucle\n",
        "# # model_name = 'M1'\n",
        "# # loss = 'mse'\n",
        "# # optimizer = 'adam'\n",
        "# # patience = 30\n",
        "# # epochs = 500\n",
        "# # callbacks = MyCallbacks(patience)\n",
        "# # model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "\n",
        "# # # Iterar sobre cada split\n",
        "# # for i, (train_index, test_index) in enumerate(tscv.split(data_norm)):\n",
        "# #     train_tscv = data_norm.iloc[train_index]\n",
        "# #     test_tscv = data_norm.iloc[test_index]\n",
        "# #     print('Train:\\n', train_tscv.shape[0])\n",
        "# #     print('Test:\\n', test_tscv.shape[0])\n",
        "\n",
        "\n",
        "# #     # Crear datasets de ventanas\n",
        "# #     data_train_wrangled = windowed_dataset(train_tscv.values, 'train', window_size, horizon, batch_size)\n",
        "# #     data_valid_wrangled = windowed_dataset(test_tscv.values, 'valid', window_size, horizon, batch_size)\n",
        "\n",
        "# #     # Check if datasets are empty and adjust if necessary\n",
        "# #     if len(list(data_train_wrangled)) == 0 or len(list(data_valid_wrangled)) == 0:\n",
        "# #       print(f\"Warning: Empty dataset encountered for split {i+1}. Skipping this split.\")\n",
        "# #       continue  # Skip to the next split\n",
        "\n",
        "# #     history = model.fit(\n",
        "# #     data_train_wrangled,\n",
        "# #     validation_data = data_valid_wrangled,\n",
        "# #     epochs=epochs,\n",
        "# #     verbose=2,\n",
        "# #     callbacks = callbacks)\n",
        "\n",
        "# #     # Evaluar el modelo en el conjunto de validación\n",
        "# #     val_loss = model.evaluate(data_valid_wrangled)\n",
        "# #     print(f'Split {i+1} - Loss: {val_loss}')\n",
        "# #     split_losses.append(val_loss)\n",
        "# #     plot_history(history)\n",
        "\n",
        "# #     predicciones_all = sumar_predicciones(predicciones_all, generate_predictions(False))\n",
        "\n",
        "\n",
        "# # # Promedio de las pérdidas en todos los splits. El axis es por si analizamos mas de una metrica\n",
        "# # avg_loss = np.mean(split_losses, axis=0)\n",
        "# # print(f'Average Loss across all splits: {avg_loss}')\n",
        "\n",
        "# # # Promedio las predicciones\n",
        "# # predicciones_final = data_productos_a_predecir.copy()\n",
        "# # predicciones_final['tn'] = predicciones_all['tn']/n_splits\n",
        "\n",
        "# # # Exporto el CSV para Kaggle\n",
        "# # filename = f\"{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "# # predicciones_final.to_csv(filename, header=True, index=False)\n",
        "# # print(filename)"
      ],
      "metadata": {
        "id": "LA0pXJB_7OD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/11 - Agrupando por Categoria 1"
      ],
      "metadata": {
        "id": "Mj0hs54Uc8_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #########################################################################\n",
        "# # Decomisionado                                                         #\n",
        "# #########################################################################\n",
        "\n",
        "\n",
        "# # Inicializo el vector de predicciones\n",
        "# predictions_acum = data_productos_a_predecir.copy()\n",
        "# predictions_acum['tn'] = 0\n",
        "# # predictions_acum.set_index('product_id', inplace=True)\n",
        "# predictions_acum = predictions_acum.squeeze()\n",
        "# predictions_acum\n",
        "\n",
        "# # Genero el vector de categorias\n",
        "# categorias = data_productos_a_predecir_con_categorias.cat1.unique()\n",
        "\n",
        "# # Creo un modelo para cada categoria 1\n",
        "# for cat in categorias:\n",
        "#   data_cat1 = filter_data_por_categoria(data_grouped, cat, 'cat1')\n",
        "#   n_features = data_cat1.shape[1]\n",
        "#   # display(data_cat1)\n",
        "#   data_train_cat1, data_valid_cat1 = split_data(data_cat1)\n",
        "#   print(f'Categoria {cat}: {data_train_cat1.shape}, {data_valid_cat1.shape}')\n",
        "#   data_train_cat1_norm, data_valid_cat1_norm, data_norm_params = normalize_data(data_train_cat1, data_valid_cat1, normalization)\n",
        "#   data_train_windowed = windowed_dataset(data_train_cat1_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_train_cat1, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#   # Creo que tengo que llenar los huecos\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'CAT1'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 50\n",
        "#   epochs = 500\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       validation_data = data_valid_windowed,\n",
        "#       callbacks = callbacks,\n",
        "#       verbose=0,\n",
        "#       epochs=epochs)\n",
        "\n",
        "#   plot_history(history)\n",
        "\n",
        "# #   # Seleccionar los últimos x meses de data_train\n",
        "# #   data_for_prediction = data_train_cat1[-window_size:]\n",
        "# #   # Convierte los datos a un formato compatible con la función window_dataset\n",
        "# #   data_for_prediction = data_for_prediction.values.reshape((1, window_size, n_features))\n",
        "# #   predictions = model.predict(data_for_prediction)\n",
        "\n",
        "# #   # # Convertir las predicciones a un DataFrame para desnormalizar\n",
        "# #   predictions_df = pd.DataFrame(predictions[0], columns=data_train_cat1.columns)\n",
        "\n",
        "# #   # # Desnormalizar las predicciones\n",
        "# #   predictions_denorm_cat1 = denormalize_series(predictions_df, data_norm_params, normalization=normalization).iloc[1]\n",
        "\n",
        "# #   # Voy sumando las predicciones de cada categoria\n",
        "# #   predictions_acum = predictions_acum.add(predictions_denorm_cat1, fill_value=0)\n",
        "\n",
        "# # # Exporto a formato Kaggle\n",
        "# # predictions_acum_df = pd.DataFrame(predictions_acum).reset_index()\n",
        "# # predictions_acum_df.columns = ['product_id', 'tn']\n",
        "# # filename = f\"{model_name}_{split_strategy}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "# # predictions_acum_df.to_csv(filename, header=True, index=False)\n",
        "# # print(filename)"
      ],
      "metadata": {
        "id": "4i5vHk16uOl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/11 - Analisis del Error por Producto"
      ],
      "metadata": {
        "id": "OisFvYA0feHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #########################################################################\n",
        "# # Decomisionado                                                         #\n",
        "# #########################################################################\n",
        "\n",
        "\n",
        "# #########################################################################\n",
        "# # Train hasta 2019-10, para predecir 2019-12\n",
        "# #########################################################################\n",
        "\n",
        "# # split_data_2019\n",
        "# data_filled = fill_nulls(data_grouped)\n",
        "# data_train, data_valid, data_test = split_data_2019(data_filled)\n",
        "# data_train = pd.DataFrame(data_train)\n",
        "# data_valid = pd.DataFrame(data_valid)\n",
        "# data_test = pd.DataFrame(data_test)\n",
        "# print(data_train.shape, data_valid.shape, data_test.shape)\n",
        "\n",
        "\n",
        "# data_train_norm, data_valid_norm, data_norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size,)\n",
        "# display(data_test)\n",
        "\n",
        "# # Al hacer el split disitnto, hay que modificar esto sino me voy de rango creando las window\n",
        "# # split_strategy = 'S1'\n",
        "# # window_size = 3\n",
        "# # horizon = 2\n",
        "# # batch_size = 1\n",
        "# # normalization = 'MinMax'\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'ErrorAnalysis'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 20\n",
        "# epochs = 200\n",
        "# n_features = data_train.shape[1]\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# plot_history(history)\n",
        "\n",
        "# # # No sirve para este caso, esta hardcodeada a Febrero 2020. Tengo que actualizar esta funcion\n",
        "# # # generate_predictions(data_norm, data_norm_params)\n",
        "\n",
        "# # # Seleccionar los últimos x meses de data_train\n",
        "# # data_for_prediction = data_train[-window_size:]\n",
        "# # # Convierte los datos a un formato compatible con la función window_dataset\n",
        "# # data_for_prediction = data_for_prediction.values.reshape((1, window_size, n_features))\n",
        "# # predictions = model.predict(data_for_prediction)\n",
        "\n",
        "# # # Convertir las predicciones a un DataFrame para desnormalizar\n",
        "# # predictions_df = pd.DataFrame(predictions[0], columns=data_train.columns)\n",
        "\n",
        "# # # Desnormalizar las predicciones\n",
        "# # predictions_denorm = denormalize_series(predictions_df, data_norm_params, normalization=normalization)\n",
        "\n",
        "# # # Imprimir las predicciones desnormalizadas\n",
        "# # # display(predictions_denorm)\n",
        "\n",
        "\n",
        "# # # Genero las Series para plotear el error entre predicho y real. Armo un Dataframe\n",
        "# # data_dec2019_pred = pd.Series(predictions_denorm.iloc[1], name='Pred')\n",
        "# # data_dec2019_true = pd.Series(data_grouped.fillna(0).loc['2019-12-01'], name='True')\n",
        "# # data_dec2019_error = pd.concat([data_dec2019_pred, data_dec2019_true], axis=1)\n",
        "\n",
        "# # # Clusterizo por Categorias 1 y 2 de productos\n",
        "# # data_productos_indexed = data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()\n",
        "# # data_dec2019_error_detail = data_dec2019_error.join(data_productos_indexed[['cat1', 'cat2', 'cat3']])\n",
        "\n",
        "# # # Ploteo los Errores, con sus clusters\n",
        "# # sns.scatterplot(data=data_dec2019_error_detail, x='Pred', y='True', hue='cat1')\n",
        "# # plt.axline((0, 0), slope=1, color='r', linestyle='--')\n",
        "# # plt.show()\n",
        "\n",
        "# # # Verificamos que Categoria es la que engloba mas productos\n",
        "# # print(data_productos['cat1'].value_counts())\n",
        "\n",
        "# # # Productos con mayores diferencias en la prediccion\n",
        "# # predictions_worst10 = abs(data_dec2019_pred - data_dec2019_true).sort_values(ascending=False).head(10)\n",
        "# # display(pd.DataFrame(predictions_worst10).join(data_productos[['product_id', 'cat1']].set_index('product_id').sort_index()).rename(columns={0: 'tn_diff'}).sort_values(by='tn_diff', ascending=False))\n",
        "\n",
        "# # # Ploteamos el error en las predicciones acumulaod por Categoria\n",
        "# # pd.DataFrame(abs(data_dec2019_pred - data_dec2019_true).sort_values(ascending=False)).join(data_productos[['product_id', 'cat1']].set_index('product_id').sort_index()).rename(columns={0: 'tn_diff'}).groupby('cat1').sum().sort_values(by='tn_diff', ascending=False).plot(kind='bar')\n",
        "# # plt.title('Diferencia en toneladas por Categoria')\n",
        "# # plt.show()"
      ],
      "metadata": {
        "id": "8tw6MlALfgrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vemos claramente como estsamos prediciendo mal los productos de Health Care, mas que nada los que predice entre 350 y 600, esta prediciendo bastante de menos."
      ],
      "metadata": {
        "id": "7hM59Wm47VGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - Cada cliente por separado"
      ],
      "metadata": {
        "id": "fPq4wBabw7gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Esto lo corro en mi maquina local, en Colab se cuelga antes de terminar con los casi 600 clientes\n",
        "\n",
        "# # Inicializo el vector de predicciones\n",
        "# predicciones_all = data_productos_a_predecir.copy()\n",
        "# predicciones_all['tn'] = 0\n",
        "\n",
        "# # Probamos solo con los 3 primeros clientes\n",
        "# # customers = data['customer_id'].unique()\n",
        "# customers = ['10001', '10002', '10003']\n",
        "# customers.sort()\n",
        "\n",
        "# i = 0\n",
        "\n",
        "# for customer in customers:\n",
        "#   print('Vuelta: ', i)\n",
        "#   i += 1\n",
        "#   data_customer = data_filter.query('customer_id == @customer')\n",
        "#   data_customer_grouped = group_data(data_customer, 'product_id')\n",
        "#   data_customer_grouped_fixed = complete_sales(data_customer_grouped, data_productos_a_predecir)\n",
        "#   # display(data_customer_grouped_fixed)\n",
        "#   data_customer_filled = fill_nulls(data_customer_grouped_fixed) # Probar cual funciona mejor\n",
        "\n",
        "#     # Probamos reemplazando la crisis de Agosto 2019 por el promedio en Julio y Septiembre. Maybe no funciona\n",
        "#   data_customer_filled.drop(index='2019-08', axis=1, inplace=True)\n",
        "#   data_agosto_2019 = data_customer_filled.loc[['2019-07', '2019-09']].mean().to_frame().transpose()\n",
        "#   data_agosto_2019.index = pd.to_datetime(['2019-08-01'])\n",
        "#   data_customer_filled = pd.concat([data_customer_filled, data_agosto_2019]).sort_index()\n",
        "\n",
        "#   data_customer_norm, data_customer_norm_params = normalize_data(data_customer_filled, normalization=normalization)\n",
        "#   data_customer_norm_train, data_customer_norm_valid = split_data(data_customer_norm)\n",
        "#   data_train_windowed = windowed_dataset(data_customer_norm_train, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_customer_norm_valid, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'M1'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 30\n",
        "#   epochs = 500\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       validation_data = data_valid_windowed,\n",
        "#       callbacks = callbacks,\n",
        "#       verbose=0,\n",
        "#       epochs=epochs)\n",
        "\n",
        "#   plot_history(history)\n",
        "\n",
        "\n",
        "#   predicciones = generate_predictions(data_customer_norm, data_customer_norm_params, False)\n",
        "#   predicciones_all = sumar_predicciones(predicciones_all, predicciones)\n",
        "\n",
        "# predicciones_all.to_csv('predicciones_local.csv', header=True, index=False)"
      ],
      "metadata": {
        "id": "TvvXLKmpyS5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - No Split"
      ],
      "metadata": {
        "id": "MFRKlhKimrIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #########################################################################\n",
        "# # Decomisionado                                                         #\n",
        "# #########################################################################\n",
        "\n",
        "\n",
        "# #########################################################################\n",
        "# # Sin Splitear los datos, usando todo para entrenar\n",
        "# #########################################################################\n",
        "\n",
        "# # data_train, data_valid = split_data(group_data(data, 'product_id'), window_size)\n",
        "# data_train_windowed = windowed_dataset(data_train, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# # data_valid_windowed = windowed_dataset(data_valid, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# # loss = 'mse'\n",
        "# # optimizer = 'adam'\n",
        "# # patience = 30\n",
        "# epochs = 10\n",
        "\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     # validation_data = data_valid_windowed,\n",
        "#     # callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# plot_history(history)\n",
        "# predicciones = generate_predictions(True)\n",
        "# predicciones"
      ],
      "metadata": {
        "id": "JFTI6TUpmrRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - Split #2"
      ],
      "metadata": {
        "id": "56uvfCGZfKek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #########################################################################\n",
        "# # Decomisionado                                                         #\n",
        "# #########################################################################\n",
        "\n",
        "\n",
        "# #########################################################################\n",
        "# Train desde 2017-01 hasta 2019-06\n",
        "# #########################################################################\n",
        "\n",
        "# data_train, data_valid = split_data(group_data(data, 'product_id'), window_size)\n",
        "# data_train_windowed = windowed_dataset(data_train, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)"
      ],
      "metadata": {
        "id": "kMCpoMFHfMgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - Split #1"
      ],
      "metadata": {
        "id": "Z2xfLSMdU96y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #########################################################################\n",
        "# # Decomisionado                                                         #\n",
        "# #########################################################################\n",
        "\n",
        "\n",
        "# # #########################################################################\n",
        "# # Train 2018 & 2018, Validation 2019 (10/06)\n",
        "# # #########################################################################\n",
        "\n",
        "# window_size = 6\n",
        "\n",
        "# data_train, data_valid = split_data(group_data(data, 'product_id'), window_size)\n",
        "# data_train_windowed = windowed_dataset(data_train, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "# n_features = data_train.shape[1]\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# plot_history(history)\n",
        "# preddicciones = generate_predictions(True)\n",
        "# preddicciones"
      ],
      "metadata": {
        "id": "I4NrpvT1VgNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proof of Concept"
      ],
      "metadata": {
        "id": "y28Ak-HH3TEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #########################################################################\n",
        "# Decomisionado                                                           #\n",
        "# #########################################################################\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "# New Pipeline (09/06)\n",
        "# #########################################################################\n",
        "# data_norm, data_norm_params = normalize_data(data_filled, normalization=normalization)\n",
        "# # data_train, data_valid = split_data(data_norm) # Split pendiente\n",
        "# data_train = data_norm\n",
        "# print(data_train.shape)\n",
        "# # print(data_valid.shape)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# # data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "\n",
        "# #########################################################################\n",
        "# # Old Pipeline (08/06)\n",
        "# #########################################################################\n",
        "# # data_all = group_data(data, data_productos_a_predecir)\n",
        "# # data_all_norm, data_all_norm_params = normalize_data(data_all, normalization=normalization)\n",
        "# # data_all_norm['20001'].describe()\n",
        "# # data_train, data_valid = split_data_all(data_all_norm)\n",
        "# # print(data_train.shape)\n",
        "# # print(data_valid.shape)\n",
        "# # data_train = data_all_norm\n",
        "# # data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# # data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "\n",
        "# #########################################################################\n",
        "# # Modelo\n",
        "# #########################################################################\n",
        "# data_train, data_valid = split_data_1(data_norm)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "\n",
        "# callbacks = MyCallbacks(model_name, patience)\n",
        "# model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# generate_predictions()"
      ],
      "metadata": {
        "id": "bCx0jH7rFU-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA Inicial"
      ],
      "metadata": {
        "id": "CRuRNJCI8J7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivos Preeliminares\n",
        "* [x] Algunos productos por promedio, otros por red neuronal. Identificar cuales\n",
        " (los que no tengo ventas todos o la mayoria de los meses)\n",
        "*[x] Saber como fueron las ventas en cada Agosto, comparado con Julio y Septiembre del mismo ano. Esto para poder sortear el problema de la no ventas en Agosto 2018\n",
        "*[x] Entender la distribucion de ventas por categoria\n",
        "*[ ] Entender la distribucion de los errores por categoria\n",
        "*[ ] entrenar en 2017 y 2018, y predecir feb 2019, teniendo en cuenta el trend negativo de alguna manera\n",
        "*[ ] Saber si la serie de todos los productos por categoria, son similares\n",
        "*[ ] Dividr las ventas por tipo de calidad de producto (Alto, Media Bajo), pedirlo si no esta\n",
        "*[ ] Ver el error por producto, para saber cual analizar individualmente\n",
        "*[ ] Identificar los productos mas importantes, separarlos por categoria, y fijarse si las series son similares. No con todos, maybe los 150 orimeros solametne\n",
        "*[ ] Probar Unvariado en cada producto?\n",
        "\n"
      ],
      "metadata": {
        "id": "oX5UG0OxbRrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot de Ventas General\n",
        "# data.groupby(['periodo'])['tn'].sum().plot()\n",
        "# plt.show()\n",
        "\n",
        "# # Deidentificacionde los productos y clientes\n",
        "# # Evidentemente cuando el profe deidentifico los customers, lo hizo asignandoles ID secuenciales al listado ordenado por la suma de ventas(tn)\n",
        "# print('Listado de Clientes, ordenados por la sumatoria de ventas en tn:\\n', group_data(data, 'customer_id').sum(), '\\n')\n",
        "\n",
        "# # Lo mismo cuando deidentifico a los productos, solo que esta vez empezo desde 20000\n",
        "# print('Listado de Productos, ordenados por la sumatoria de ventas en tn:\\n', group_data(data, 'product_id').sum())"
      ],
      "metadata": {
        "id": "-PQsvGzWAIMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vemos la Tendencia Negativa\n",
        "* Tambien vemos la seasonalidad anual, y mas chica puede que tabien\n",
        "* Tambien se ve la caida en 2019-08, ya que decidimos no vender ese mes"
      ],
      "metadata": {
        "id": "talT1z4hc8jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 74"
      ],
      "metadata": {
        "id": "tEP9EZ77NM46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mask_product_id_sold36 = (group_data(data, 'product_id') > 0).sum(axis=0)==36\n",
        "# print('Productos vendidos los 36 meses:', mask_product_id_sold36.sum())\n",
        "# mask_product_id_sold12 = (group_data(data, 'product_id') > 0).sum(axis=0)<=12\n",
        "# print('Productos vendidos en 12 meses o menos:', mask_product_id_sold12.sum())\n"
      ],
      "metadata": {
        "id": "tjN0AgGnNL7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vamos a ver de el TOP 74 (hasta ID 84) de los productos mas vendidos, y solo los product_id 20032 y 20089 no se vendieron los 36 meses\n"
      ],
      "metadata": {
        "id": "Zn2bX_L8VB5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group_data(data.query(\"product_id == '20032' or product_id == '20089'\"), 'product_id').plot(figsize=(15, 5), title='Productos Top75 que no se vendieron los 36 meses')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "s6sy-Bt3nfyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Uno se empezo a vender en 2019-02, y el otro 2018-07"
      ],
      "metadata": {
        "id": "x-fmExFCWE1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisis Agosto 2019"
      ],
      "metadata": {
        "id": "ePaHFgdPb0VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group_data(data, 'product_id').loc['2017-06':'2017-10'].sum(axis=1).plot(title='Agosto crece-crece en 2017')\n",
        "# plt.show()\n",
        "\n",
        "# group_data(data, 'product_id').loc['2018-06':'2018-10'].sum(axis=1).plot(title='Agosto crece-decrece en 2018')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "SPJsHIyQdkhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pareciera ser que siempre sube un 10% con respecto al Julio anterior"
      ],
      "metadata": {
        "id": "hiN9yUs1vtTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis por Categoria"
      ],
      "metadata": {
        "id": "AoScqaZOIrIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_productos_con_categorias[data_productos_con_categorias['product_id'].isin(data_productos_a_predecir.index)]['cat1'].value_counts().plot(kind='bar')\n",
        "# plt.title('Productos por Categoria 1')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "gCbG7TIQqpQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* La categoria con mas productos es PC con 444"
      ],
      "metadata": {
        "id": "dD5QG3mnLjKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisis Ventas en los Febreros"
      ],
      "metadata": {
        "id": "9M0IKSYdSrzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # En 2017\n",
        "\n",
        "# data_2017 =group_data(data, 'product_id').loc['2017'].mean(axis=0)\n",
        "# feb_2017 = group_data(data, 'product_id').loc['2017-02']\n",
        "\n",
        "# # Ploteamos 2017\n",
        "# fig, ax = plt.subplots(figsize=(10, 6))\n",
        "# # Ploteamos promedio del 2017\n",
        "# data_2017[:50].plot(ax=ax, label='Promedio 2017')\n",
        "# # Ploteamos Febrero 2017\n",
        "# feb_2017.T[:50].plot(ax=ax, label='Febrero 2017')\n",
        "# ax.legend()\n",
        "# plt.show()\n",
        "# # Vemos como el promedio anual y las ventas de Feb tienen una cierta relacion\n",
        "\n",
        "# # Calculamos el ratio que Febrero corresponde al promedio anual\n",
        "# diff_2017 = pd.Series(((data_2017- feb_2017)/feb_2017).T.squeeze(), name='diff')\n",
        "\n",
        "# # Lo joineamos a las categorias\n",
        "# diff_2017_cat = pd.concat([diff_2017, data_productos_con_categorias[data_productos_con_categorias['product_id'].isin(data_productos_a_predecir.index)][['cat1']]], axis=1)\n",
        "\n",
        "\n",
        "# # # Analizamos (REVISAR)\n",
        "# # diff_2017_cat.groupby('cat1')['diff'].mean().plot(kind='bar')\n",
        "# # plt.title('Ratio ventas Proemdio Anual sobre Febrero en 2017')\n",
        "# # plt.show()\n",
        "\n",
        "# # display(diff_cat.sort_values(by='diff', ascending=False).head(30))\n",
        "# # Vemos que ciertos productos de Personal Care no siguen el patron\n",
        "# # Esto habra sido un problema puntual en 2017, el profe habia comentaso lo de las cremas?\n",
        "# # Lo que si nos interesa es ver que HC y FOODS estan cerca del 0.5"
      ],
      "metadata": {
        "id": "PaHvU0QWSs4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Que productos se vendieron los 36 meses"
      ],
      "metadata": {
        "id": "GEJxmKPZ-bZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Verificamos como algunos productos se dejaron de comercializar antes de Diciembre 2019\n",
        "# # Tambien vemos como otros empezaron a comecializarse despues de Enero 2017\n",
        "# plot_products_sales(data_bkp, start=0, end=50)"
      ],
      "metadata": {
        "id": "MKaQu--V7p4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Verificamos los productos con menos tiempo de exposicion en el mercado, se venden en menos cantidad\n",
        "# # Agregar esta variable al FE\n",
        "\n",
        "# # Verificamos si hay valores nulos en el DataFrame\n",
        "# null_data = group_data(data, 'product_id').isnull()\n",
        "\n",
        "# # Creamos el heatmap para visualizar los valores nulos\n",
        "# plt.figure(figsize=(15, 10))\n",
        "# sns.heatmap(null_data, cmap='viridis', cbar=False)\n",
        "# plt.gca().set_yticklabels(null_data.index.strftime('%Y-%m')) # Formateo de las etiquetas del eje y\n",
        "# plt.title('Productos sin ventas en algunos meses (en amarillo)')\n",
        "# plt.xlabel('Product ID')\n",
        "# plt.ylabel('Fecha')\n",
        "# plt.show()\n",
        "\n",
        "# # Se observan que algunos productos que no se vendieron en meses puntuales, se habra roto la maquina?"
      ],
      "metadata": {
        "id": "4D8PDrMLv_FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tambien pareciera ser que el producto 21074 se dejo de vender en Dic 2019. Deberiamos predecir 0 para Feb 2020?\n",
        "# display(null_data.loc['2019-12'].T[null_data.loc['2019-12'].T == True].dropna())\n",
        "\n",
        "# # Aunque en la grafica no lo parece. Se abra roto la maquina en Dec 2019?\n",
        "# group_data(data.query('product_id == \"21074\"'), \"product_id\").plot(figsize=(15, 5), title='Ventas de 21074')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "-230XsPmVIYD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}