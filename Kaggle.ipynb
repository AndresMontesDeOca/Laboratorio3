{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLjBbtpeVUYuIs4f3m814u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresMontesDeOca/Laboratorio3/blob/main/Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modulos"
      ],
      "metadata": {
        "id": "crj3eqSV_WvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "eL3soxEh_cgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ColabNotebook = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# if ColabNotebook: # maquina virtual colab\n",
        "#     # monta G-drive en entorno COLAB\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "HkJAzqZfivZx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "import warnings\n",
        "# warnings.filterwarnings('ignore', category=ValueWarning)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Asegurarte de que Pandas muestre los valores con la máxima precisión\n",
        "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
        "\n",
        "# Ajustar la opción para mostrar más filas\n",
        "# pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Si también quieres mostrar más columnas\n",
        "# pd.set_option('display.max_columns', None)\n",
        "\n",
        "\n",
        "# Vamos a suprimir la notacion cientifica\n",
        "pd.set_option(\"display.float_format\", lambda x:\"%.2f\" %x)\n"
      ],
      "metadata": {
        "id": "4dKLvveO9QDs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga Datos"
      ],
      "metadata": {
        "id": "CCBiyQvD_nC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "# !pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "################################# Datasets ###################################\n",
        "# # Ventas\n",
        "id = \"158aOjqxaNO8l97yA6VWJkek_15YVLMhs\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('sell-in.txt')\n",
        "data_ventas = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
        "data_ventas['periodo'] = pd.to_datetime(data_ventas['periodo'], format='%Y%m')\n",
        "data_ventas['customer_id'] = data_ventas['customer_id'].astype(str)\n",
        "data_ventas['product_id'] = data_ventas['product_id'].astype(str)\n",
        "data = data_ventas.copy()\n",
        "\n",
        "# # Productos\n",
        "id = \"15JS_k86LS0sgJXma7BOVXWlyNcMwxdhE\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('tb_productos.txt')\n",
        "data_productos = pd.read_csv(\"tb_productos.txt\", sep=\"\\t\")\n",
        "data_productos['product_id'] = data_productos['product_id'].astype(str)\n",
        "\n",
        "# # Stocks\n",
        "id = \"15EV-8f_U7onpA1AcTxxXeD-z8yVR4fQu\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('tb_stocks.txt')\n",
        "data_stocks = pd.read_csv(\"tb_stocks.txt\", sep=\"\\t\")\n",
        "data_stocks['periodo'] = pd.to_datetime(data_stocks['periodo'], format='%Y%m')\n",
        "data_stocks['product_id'] = data_stocks['product_id'].astype(str)\n",
        "\n",
        "# # Productos a predecir\n",
        "id = \"15LjADctFVwjzQFJvfJGFTEdgZx9xCoId\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('productos_a_predecir.txt')\n",
        "data_productos_a_predecir = pd.read_csv(\"productos_a_predecir.txt\", sep=\"\\t\")\n",
        "data_productos_a_predecir['product_id'] = data_productos_a_predecir['product_id'].astype(str)\n",
        "data_productos_a_predecir_con_categorias = data_productos_a_predecir.set_index('product_id').join(data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()[['cat1', 'cat2', 'cat3']])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8GISdopF_obd",
        "outputId": "189e0aad-6ced-4666-8b1a-43f1b3253bff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Data"
      ],
      "metadata": {
        "id": "dJckQiyL08r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "def filter_data(data_all, data_filter):\n",
        "    # Filtrar el DataFrame 'data_all' para que solo contenga los 'product_id' presentes en 'data_filter'\n",
        "    data_filtered = data_all[data_all['product_id'].isin(data_filter['product_id'])]\n",
        "\n",
        "    return data_filtered\n",
        "##############################################################################\n",
        "def filter_data_por_categoria(df, categoria, categoria_columna):\n",
        "    \"\"\"\n",
        "    Filtra los productos de un DataFrame dado una categoría y el DataFrame de productos con categorías.\n",
        "\n",
        "    Args:\n",
        "    dataframe (pd.DataFrame): DataFrame con las ventas de productos (cada columna es un product_id).\n",
        "    categoria (str): Categoría a filtrar (valor de cat1, cat2 o cat3).\n",
        "    categoria_columna (str): Nombre de la columna de categoría ('cat1', 'cat2' o 'cat3').\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame filtrado con solo los productos de la categoría especificada.\n",
        "    \"\"\"\n",
        "    # Filtrar los productos que pertenecen a la categoría especificada\n",
        "    productos_filtrados = data_productos_a_predecir_con_categorias[data_productos_a_predecir_con_categorias[categoria_columna] == categoria].index\n",
        "\n",
        "    # Filtrar el DataFrame de ventas usando los product_ids de los productos filtrados\n",
        "    productos_en_data = [col for col in df.columns if col in productos_filtrados]\n",
        "    df_filtrado = df[productos_en_data]\n",
        "\n",
        "    return df_filtrado\n",
        "##############################################################################"
      ],
      "metadata": {
        "id": "nnSdLDk60-bO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group Data"
      ],
      "metadata": {
        "id": "5L-kvH-J_7SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Siempre como values toma las toneladas vendidas\n",
        "def group_data(data, column):\n",
        "  grouped_data = data.groupby([column, 'periodo']).sum().reset_index()\n",
        "\n",
        "  # Crea un DataFrame pivoteado donde las filas son las fechas y las columnas son los product_id\n",
        "  pivot_data = grouped_data.pivot(index='periodo', columns=column, values='tn')\n",
        "\n",
        "  # Asegúrate de que los nombres de las columnas sean strings\n",
        "  pivot_data.columns = pivot_data.columns.astype(str)\n",
        "\n",
        "  # Restablece el índice para asegurarse de que 'product_id' no sea un índice compuesto\n",
        "  pivot_data.columns.name = None\n",
        "\n",
        "  return pivot_data"
      ],
      "metadata": {
        "id": "Mrkkv5Kd0yCG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill Nulls"
      ],
      "metadata": {
        "id": "p_XRfvor4fhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Jugar con esto, no se si esta bien\n",
        "def fill_nulls(df):\n",
        "  # Primero usamos bfill para completar las ordenes mas viejas con los valores de las ordenes mas recientes\n",
        "  df = df.bfill()\n",
        "  # Luego completamos con ceros los productos que dejamos de vender, o se discontinuaron\n",
        "  df = df.fillna(0)\n",
        "\n",
        "  return df\n",
        "###########################################################################\n",
        "def fill_nulls_customer(df):\n",
        "  df = df.fillna(0)\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "WPghARRT5HE3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize Data"
      ],
      "metadata": {
        "id": "a2ctIs9UCRRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "def normalize_data(df, normalization=\"MinMax\"):\n",
        "    \"\"\"\n",
        "    Normaliza cada serie de tiempo (columna) de manera individual usando MinMax o Zscore.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame con series de tiempo de distintos productos, cada columna es un producto.\n",
        "        normalization (str): Tipo de normalización a aplicar. Opciones: \"MinMax\" o \"Zscore\". Default es \"MinMax\".\n",
        "\n",
        "    Returns:\n",
        "        normalized_df (pd.DataFrame): DataFrame con las series normalizadas.\n",
        "        normalization_params (pd.DataFrame): DataFrame con los parámetros necesarios para desnormalizar cada columna.\n",
        "            - Para \"MinMax\": valores min y max de cada columna.\n",
        "            - Para \"Zscore\": valores mean y std de cada columna.\n",
        "    \"\"\"\n",
        "    normalization_params = pd.DataFrame(columns=[\"product_id\", \"min\", \"max\", \"mean\", \"std\"])\n",
        "    normalized_df = pd.DataFrame(index=df.index)\n",
        "\n",
        "    for column in df.columns:\n",
        "        if normalization == \"MinMax\":\n",
        "            scaler = MinMaxScaler()\n",
        "            normalized_values = scaler.fit_transform(df[[column]]).flatten()\n",
        "            new_params = pd.DataFrame({\n",
        "                \"product_id\": [column],\n",
        "                \"min\": [scaler.data_min_[0]],\n",
        "                \"max\": [scaler.data_max_[0]],\n",
        "                \"mean\": [None],\n",
        "                \"std\": [None]\n",
        "            })\n",
        "            normalization_params = pd.concat([normalization_params, new_params], ignore_index=True)\n",
        "            normalized_df[column] = normalized_values\n",
        "\n",
        "        elif normalization == \"ZScore\":\n",
        "            scaler = StandardScaler()\n",
        "            normalized_values = scaler.fit_transform(df[[column]]).flatten()\n",
        "            new_params = pd.DataFrame({\n",
        "                \"product_id\": [column],\n",
        "                \"min\": [None],\n",
        "                \"max\": [None],\n",
        "                \"mean\": [scaler.mean_[0]],\n",
        "                \"std\": [scaler.scale_[0]]\n",
        "            })\n",
        "            normalization_params = pd.concat([normalization_params, new_params], ignore_index=True)\n",
        "            normalized_df[column] = normalized_values\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid normalization method. Choose 'MinMax' or 'ZScore'.\")\n",
        "\n",
        "    return normalized_df, normalization_params\n",
        "\n",
        "def denormalize_series(normalized_series, normalization_params, normalization=\"MinMax\"):\n",
        "    \"\"\"\n",
        "    Desnormaliza una serie de tiempo usando los valores almacenados.\n",
        "\n",
        "    Args:\n",
        "        normalized_series (pd.Series or pd.DataFrame): Serie o DataFrame con los datos normalizados.\n",
        "        normalization_params (pd.DataFrame): DataFrame con los parámetros necesarios para desnormalizar cada serie o columna.\n",
        "            - Para \"MinMax\": valores min y max de cada serie o columna.\n",
        "            - Para \"Zscore\": valores mean y std de cada serie o columna.\n",
        "        normalization (str): Tipo de normalización a deshacer. Opciones: \"MinMax\" o \"Zscore\". Default es \"MinMax\".\n",
        "\n",
        "    Returns:\n",
        "        denormalized_series (pd.Series or pd.DataFrame): Serie o DataFrame con los datos desnormalizados.\n",
        "    \"\"\"\n",
        "    if isinstance(normalized_series, pd.DataFrame):\n",
        "        denormalized_df = pd.DataFrame(index=normalized_series.index)\n",
        "        for column in normalized_series.columns:\n",
        "            params = normalization_params[normalization_params[\"product_id\"] == column]\n",
        "            if normalization == \"MinMax\":\n",
        "                min_value = params[\"min\"].values[0]\n",
        "                max_value = params[\"max\"].values[0]\n",
        "                denormalized_values = normalized_series[column] * (max_value - min_value) + min_value\n",
        "            elif normalization == \"ZScore\":\n",
        "                mean_value = params[\"mean\"].values[0]\n",
        "                std_value = params[\"std\"].values[0]\n",
        "                denormalized_values = normalized_series[column] * std_value + mean_value\n",
        "            else:\n",
        "                raise ValueError(\"Invalid normalization method. Choose 'MinMax' or 'ZScore'.\")\n",
        "            denormalized_df[column] = denormalized_values\n",
        "        return denormalized_df\n",
        "    elif isinstance(normalized_series, pd.Series):\n",
        "        product_ids = normalized_series.index\n",
        "        denormalized_values = []\n",
        "        for product_id in product_ids:\n",
        "            params = normalization_params[normalization_params[\"product_id\"] == product_id]\n",
        "            if normalization == \"MinMax\":\n",
        "                min_value = params[\"min\"].values[0]\n",
        "                max_value = params[\"max\"].values[0]\n",
        "                denormalized_value = normalized_series[product_id] * (max_value - min_value) + min_value\n",
        "            elif normalization == \"ZScore\":\n",
        "                mean_value = params[\"mean\"].values[0]\n",
        "                std_value = params[\"std\"].values[0]\n",
        "                denormalized_value = normalized_series[product_id] * std_value + mean_value\n",
        "            else:\n",
        "                raise ValueError(\"Invalid normalization method. Choose 'MinMax' or 'ZScore'.\")\n",
        "            denormalized_values.append(denormalized_value)\n",
        "        denormalized_series = pd.Series(denormalized_values, index=product_ids, name=normalized_series.name)\n",
        "        return denormalized_series\n",
        "    else:\n",
        "        raise TypeError(\"normalized_series should be either a pandas Series or DataFrame\")\n"
      ],
      "metadata": {
        "id": "jTr6HsM8CTL2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data"
      ],
      "metadata": {
        "id": "6z54dcDlIyHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Hay que ponerse a invstigar sobre como hacer el Split en Series de Tiempo. Walk Fwd Validation?\n",
        "def split_data_1(df):\n",
        "  df_train = df.loc['2017-01':'2018-12']\n",
        "  df_valid = df.loc['2019-01':'2019-12']\n",
        "  return df_train, df_valid\n",
        "###############################################################################\n",
        "# No me deja un data_valid mas chico que esto, tira erro el earlystopping\n",
        "def split_data_2(df):\n",
        "  df_train = df.loc['2017-01':'2019-04']\n",
        "  df_valid = df.loc['2019-05':]\n",
        "  return df_train, df_valid\n",
        "#############################################################################\n",
        "def split_data_dec2019(df):\n",
        "  df_train = df.loc['2017-01':'2019-10']\n",
        "  df_valid = df.loc['2019-11':'2019-12']\n",
        "  return df_train, df_valid"
      ],
      "metadata": {
        "id": "zZUKfILzC6Un"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Window Data"
      ],
      "metadata": {
        "id": "xbYFP2j6EiEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def windowed_dataset(sequence, data_split, window_size, horizon, batch_size, shuffle_buffer=1000):\n",
        "    \"\"\"Generates dataset windows.\n",
        "\n",
        "    Args:\n",
        "      sequence (array-like): Contains the values of the time series.\n",
        "      data_split (str): Specifies if the dataset is for training or validation/test.\n",
        "      window_size (int): The number of time steps to include in the feature.\n",
        "      horizon (int): The number of future time steps to predict.\n",
        "      batch_size (int): The batch size.\n",
        "      shuffle_buffer (int): Buffer size to use for the shuffle method.\n",
        "\n",
        "    Returns:\n",
        "      tf.data.Dataset: TF Dataset containing time windows.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "\n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size + horizon, shift=1, drop_remainder=True)\n",
        "\n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + horizon))\n",
        "\n",
        "    # Create tuples with features and labels\n",
        "    dataset = dataset.map(lambda window: (window[:-horizon], window[-horizon:]))\n",
        "\n",
        "    if data_split == 'train':\n",
        "        # Shuffle the training data to improve generalization\n",
        "        dataset = dataset.shuffle(shuffle_buffer)\n",
        "    else:\n",
        "        # Cache the validation/test data for improved performance\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "    # Create batches of windows and prefetch for performance\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "OlImIwoE3Mh-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# # Viejos\n",
        "##############################################\n",
        "# def window_dataset(sequence, data_split, window_size, batch_size, n_future, shuffle_buffer=1000, seed=None):\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "#     dataset = dataset.window(window_size + n_future, shift=1, drop_remainder=True)\n",
        "#     dataset = dataset.flat_map(lambda window: window.batch(window_size + n_future))\n",
        "#     dataset = dataset.map(lambda window: (window[:window_size], window[window_size:]))\n",
        "\n",
        "#     if data_split == 'train':\n",
        "#         dataset = dataset.shuffle(shuffle_buffer, seed=seed)\n",
        "#     else:\n",
        "#         dataset = dataset.cache()\n",
        "\n",
        "#     dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#     return dataset\n",
        "##############################################\n",
        "# def window_dataset(sequence, data_split, window_size, batch_size, n_future, shuffle_buffer=1000, seed=None):\n",
        "#     \"\"\"Generates dataset windows for multi-step forecasting in a multivariable context.\n",
        "\n",
        "#     Args:\n",
        "#       sequence (array-like): Contains the values of the time series, where each element is an array of feature values.\n",
        "#       data_split (str): Specifies if the dataset is for training or validation/test.\n",
        "#       window_size (int): The number of time steps to include in the feature.\n",
        "#       batch_size (int): The batch size.\n",
        "#       n_future (int): The number of future steps to predict.\n",
        "#       shuffle_buffer (int): Buffer size to use for the shuffle method.\n",
        "#       seed (int, optional): Random seed for reproducibility.\n",
        "\n",
        "#     Returns:\n",
        "#       tf.data.Dataset: TF Dataset containing time windows.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Generate a TF Dataset from the series values\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "\n",
        "#     # Window the data but only take those with the specified size\n",
        "#     dataset = dataset.window(window_size + n_future, shift=1, drop_remainder=True)\n",
        "\n",
        "#     # Flatten the windows by putting its elements in a single batch\n",
        "#     dataset = dataset.flat_map(lambda window: window.batch(window_size + n_future))\n",
        "\n",
        "#     # Create tuples with features and labels\n",
        "#     dataset = dataset.map(lambda window: (window[:window_size], window[window_size:]))\n",
        "\n",
        "#     if data_split == 'train':\n",
        "#         # Shuffle the training data to improve generalization\n",
        "#         dataset = dataset.shuffle(shuffle_buffer, seed=seed)\n",
        "#     else:\n",
        "#         # Cache the validation/test data for improved performance\n",
        "#         dataset = dataset.cache()\n",
        "\n",
        "#     # Create batches of windows and prefetch for performance\n",
        "#     dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#     return dataset\n"
      ],
      "metadata": {
        "id": "c5g4WjakEjcM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Functions"
      ],
      "metadata": {
        "id": "x_Zm-EgneATh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################################\n",
        "def generate_predictions():\n",
        "  data_norm_array = data_norm.values\n",
        "  column_names = data_norm.columns\n",
        "  input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "  pred = model.predict(input_data)\n",
        "  pred = pred.reshape((1, horizon, n_features))\n",
        "  pred_df = pd.DataFrame(pred[0], columns=column_names)\n",
        "  pred_df.index = pd.date_range(start='2020-01-01', periods=horizon, freq='MS')\n",
        "  pred_feb = pred_df.loc['2020-02-01']\n",
        "  pred_1_denorm = denormalize_series(pred_feb, data_norm_params, normalization=normalization)\n",
        "  data_pred1_denorm = pred_1_denorm.reset_index()\n",
        "  data_pred1_denorm.columns = ['product_id', 'tn']\n",
        "  # Esto no creo que sea necesario\n",
        "  predicciones = filter_data(data_pred1_denorm, data_productos_a_predecir)\n",
        "\n",
        "  # Ojo con esto, caja negra. Vuelve a predecir usando lo predicho antes.\n",
        "  input_data2 = np.append(input_data[:, 1:, :], pred[:, 0, :].reshape(1, 1, n_features), axis=1)\n",
        "  pred2 = model.predict(input_data2)\n",
        "  pred2 = pred2.reshape((1, horizon, n_features))\n",
        "  pred2_df = pd.DataFrame(pred2[0], columns=column_names)\n",
        "  pred2_df.index = pd.date_range(start='2020-02-01', periods=horizon, freq='MS')\n",
        "  pred2_feb = pred2_df.loc['2020-02-01']\n",
        "  pred_2_denorm = denormalize_series(pred2_feb, data_norm_params, normalization=normalization)\n",
        "  data_pred2_denorm = pred_2_denorm.reset_index()\n",
        "  data_pred2_denorm.columns = ['product_id', 'tn']\n",
        "  predicciones2 = filter_data(data_pred2_denorm, data_productos_a_predecir)\n",
        "\n",
        "  filename = f\"{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "  filename2 = f\"RECURRENTE_{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "\n",
        "  predicciones.to_csv(filename, header=True, index=False)\n",
        "  predicciones2.to_csv(filename2, header=True, index=False)\n",
        "  print(filename)\n",
        "##########################################################################\n",
        "def sumar_predicciones(df1, df2):\n",
        "    # Asegúrate de que las columnas necesarias estén en los DataFrames\n",
        "    if 'product_id' not in df1.columns or 'tn' not in df1.columns:\n",
        "        raise ValueError(\"df1 debe contener las columnas 'product_id' y 'tn'\")\n",
        "    if 'product_id' not in df2.columns or 'tn' not in df2.columns:\n",
        "        raise ValueError(\"df2 debe contener las columnas 'product_id' y 'tn'\")\n",
        "\n",
        "    # Suma los valores de 'tn' para cada 'product_id' de ambos DataFrames\n",
        "    result = df1.set_index('product_id').add(df2.set_index('product_id'), fill_value=0).reset_index()\n",
        "\n",
        "    return result\n",
        "# ##########################################################################\n",
        "# # Boostea los productos de Health Care cuya predicciones tienen mucho error\n",
        "# def HC_boost(predicciones_serie, boost=-0.5):\n",
        "#     boost_mask = ['20006', '20007', '20008', '20009', '20010', '20012', '20014', '20015']\n",
        "\n",
        "#     # Convertir el índice a string si no lo es\n",
        "#     predicciones_serie.index = predicciones_serie.index.astype(str)\n",
        "\n",
        "#     # Aplicar el boost a los valores en la máscara\n",
        "#     predicciones_serie.loc[boost_mask] = predicciones_serie.loc[boost_mask] * (1 + boost)\n",
        "\n",
        "#     return predicciones_serie\n",
        "# ##########################################################################"
      ],
      "metadata": {
        "id": "jmLuXgOGeC57"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "### OLD\n",
        "###############################################################################\n",
        "# def generate_predictions(data_norm, data_norm_params):\n",
        "\n",
        "#     # Convertir el DataFrame a un array de NumPy\n",
        "#     data_norm_array = data_norm.values\n",
        "\n",
        "#     # Extraer la última ventana de datos de 2019 para predecir enero de 2020\n",
        "#     column_names = data_norm.columns  # Obtener los nombres de las columnas\n",
        "\n",
        "#     # Extraer los últimos `window_size` meses de 2019\n",
        "#     input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "\n",
        "#     # Predecir enero de 2020\n",
        "#     pred_january = model.predict(input_data)\n",
        "\n",
        "#     # Asegurarse de que la predicción tenga la forma correcta\n",
        "#     pred_january = pred_january.reshape((1, n_future, n_features))\n",
        "\n",
        "#     # Crear un DataFrame para la predicción de enero de 2020\n",
        "#     pred_january_df = pd.DataFrame(pred_january[0], columns=column_names)\n",
        "#     pred_january_df.index = pd.date_range(start='2020-01-01', periods=n_future, freq='MS')\n",
        "\n",
        "#     # Actualizar la ventana de entrada para predecir febrero de 2020\n",
        "#     input_data = np.append(input_data[:, 1:, :], pred_january[:, 0, :].reshape(1, 1, n_features), axis=1)\n",
        "\n",
        "#     # Predecir febrero de 2020\n",
        "#     pred_february = model.predict(input_data)\n",
        "\n",
        "#     # Asegurarse de que la predicción tenga la forma correcta\n",
        "#     pred_february = pred_february.reshape((1, n_future, n_features))\n",
        "\n",
        "#     # Crear un DataFrame para la predicción de febrero de 2020\n",
        "#     pred_february_df = pd.DataFrame(pred_february[0], columns=column_names)\n",
        "#     pred_february_df.index = pd.date_range(start='2020-02-01', periods=n_future, freq='MS')\n",
        "\n",
        "#     # Obtener la predicción de febrero de 2020\n",
        "#     pred_1 = pred_january_df.loc['2020-02-01']\n",
        "\n",
        "#     # Desnormalizar la predicción\n",
        "#     pred_1_denorm = denormalize_series(pred_1, data_norm_params, normalization=normalization)\n",
        "#     data_pred1_denorm = pred_1_denorm.reset_index()\n",
        "#     data_pred1_denorm.columns = ['product_id', 'tn']\n",
        "#     predicciones = filter_data(data_pred1_denorm, data_productos_a_predecir)\n",
        "\n",
        "#     # Crear el nombre del archivo\n",
        "#     filename = f\"{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}_epochs{epochs}.csv\"\n",
        "#     predicciones.to_csv(filename, header=True, index=False)\n",
        "\n",
        "#     print(filename)\n",
        "\n",
        "#     return predicciones\n",
        "##########################################################################"
      ],
      "metadata": {
        "id": "G1YhW2wUNh6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Sales"
      ],
      "metadata": {
        "id": "Tsi7tkBEgtWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_sales(df1, df2):\n",
        "    # Crear un rango de fechas desde enero 2017 hasta diciembre 2019\n",
        "    fechas_completas = pd.date_range(start='2017-01-01', end='2019-12-01', freq='MS')\n",
        "\n",
        "    # Reindexar el DataFrame para asegurar que todas las fechas estén presentes\n",
        "    df1 = df1.reindex(fechas_completas, fill_value=pd.NA)\n",
        "\n",
        "    # Obtener los product_id del primer DataFrame (nombres de las columnas)\n",
        "    product_ids_df1 = df1.columns.tolist()\n",
        "\n",
        "    # Obtener los product_id del segundo DataFrame (valores en la columna 'product_id')\n",
        "    product_ids_df2 = df2['product_id'].tolist()\n",
        "\n",
        "    # Identificar los product_id que faltan en df1\n",
        "    product_ids_faltantes = [pid for pid in product_ids_df2 if pid not in product_ids_df1]\n",
        "\n",
        "    # Crear un DataFrame con las columnas faltantes y valores NaN\n",
        "    df_faltantes = pd.DataFrame(index=df1.index, columns=product_ids_faltantes)\n",
        "\n",
        "    # Concatenar el DataFrame original con el DataFrame de faltantes\n",
        "    df_resultante = pd.concat([df1, df_faltantes], axis=1)\n",
        "\n",
        "    return df_resultante\n",
        "\n"
      ],
      "metadata": {
        "id": "aSHUuvHXgtf8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks"
      ],
      "metadata": {
        "id": "brfkj2XkXena"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def MyCallbacks(patience):\n",
        "    \"\"\"\n",
        "    Devuelve una lista de callbacks para el entrenamiento del modelo.\n",
        "\n",
        "    Parameters:\n",
        "    patience (int): Número de épocas a esperar para ver una mejora en 'val_loss' antes de detener el entrenamiento.\n",
        "\n",
        "    Returns:\n",
        "    list: Lista de callbacks de Keras.\n",
        "    \"\"\"\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "    return [early_stop]"
      ],
      "metadata": {
        "id": "fcfAKyAS_iD2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "#### VIEJO\n",
        "# #############################################################################\n",
        "# class MAEThresholdCallback(Callback):\n",
        "#     def __init__(self, threshold=0.15):\n",
        "#         super(MAEThresholdCallback, self).__init__()\n",
        "#         self.threshold = threshold\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "#         val_mae = logs.get('val_mae')\n",
        "#         if val_mae is not None and val_mae <= self.threshold:\n",
        "#             print(f'\\nEpoch {epoch+1}: Validation MAE has reached {val_mae:.4f}, stopping training.')\n",
        "#             self.model.stop_training = True\n",
        "\n",
        "# def MyCallbacks(model_name, patience):\n",
        "#     earlystop = tf.keras.callbacks.EarlyStopping('val_loss', patience=patience, restore_best_weights=True)\n",
        "#     # checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=f'ckpts/{model_name}-' + '{epoch:02d}-{val_loss:.4f}.h5', monitor='val_loss')\n",
        "#     # mae_threshold_callback = MAEThresholdCallback(threshold=0.015)\n",
        "#     return [earlystop] #, checkpoint] #, mae_threshold_callback]\n",
        "\n",
        "# #############################################################################"
      ],
      "metadata": {
        "id": "ETzh0JyBXgRt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Design"
      ],
      "metadata": {
        "id": "cGAz7W4mXqO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def compile_model(new_model, loss, optimizer):\n",
        "    new_model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
        "    print(new_model.summary())\n",
        "    return new_model\n",
        "#############################################################################\n",
        "def MyModel(loss, optimizer, window_size, horizon, n_features):\n",
        "    new_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer((window_size, n_features)),\n",
        "        tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='causal'),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=False)),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(n_features * horizon, activation='relu'),\n",
        "        tf.keras.layers.Reshape((horizon, n_features)),\n",
        "    ])\n",
        "    return compile_model(new_model, loss, optimizer)\n",
        "#############################################################################"
      ],
      "metadata": {
        "id": "v1XojStQ3FQw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################################################\n",
        "# def compile_model(new_model, loss, optimizer):\n",
        "#   new_model.compile(optimizer=optimizer, loss=loss, metrics=['mse'])\n",
        "#   print(new_model.summary())\n",
        "#   return new_model\n",
        "# #############################################################################\n",
        "# def MyModel(loss, optimizer, window_size, n_future, n_features):\n",
        "#     new_model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.InputLayer((window_size, n_features)),\n",
        "#         tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='causal'),\n",
        "#         tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "#         tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "#         tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=False)),\n",
        "#         tf.keras.layers.Dropout(0.4),\n",
        "#         tf.keras.layers.Dense(n_features * n_future, activation='relu'),\n",
        "#         tf.keras.layers.Reshape((n_future, n_features)),\n",
        "#         ])\n",
        "#     return compile_model(new_model, loss, optimizer)"
      ],
      "metadata": {
        "id": "eCESYECOXr45"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines"
      ],
      "metadata": {
        "id": "F4yiWWu8FJZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "HWg00hIFX64c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # data\n",
        "# # data_productos\n",
        "# # data_stocks\n",
        "# # data_productos_a_predecir\n",
        "\n",
        "# Pre-Processing Variables\n",
        "split_strategy = 'AgostoAvg'\n",
        "window_size = 3\n",
        "horizon = 2\n",
        "n_features = len(data_productos_a_predecir)\n",
        "batch_size = 1\n",
        "normalization = 'MinMax'\n",
        "n_splits = 5\n",
        "\n",
        "data_filter = filter_data(data, data_productos_a_predecir)\n",
        "data_grouped = group_data(data_filter, 'product_id')\n",
        "\n",
        "# Probamos reemplazando la crisis de Agosto 2019 por el promedio en Julio y Septiembre\n",
        "data_grouped.drop(index='2019-08', axis=1, inplace=True)\n",
        "data_agosto_2019 = data_grouped.loc[['2019-07', '2019-09']].mean().to_frame().transpose()\n",
        "data_agosto_2019.index = pd.to_datetime(['2019-08-01'])\n",
        "data_grouped = pd.concat([data_grouped, data_agosto_2019]).sort_index()\n",
        "\n",
        "data_filled = fill_nulls(data_grouped)\n",
        "data_norm, data_norm_params = normalize_data(data_filled, normalization=normalization)\n",
        "data_norm.index.freq = pd.infer_freq(data_norm.index) # No se si es necesario\n",
        "# print(data_norm.info())"
      ],
      "metadata": {
        "id": "vLrQTz5YEkPk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimentos Individuales"
      ],
      "metadata": {
        "id": "SaiJuaUP3JUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/12 - Walk Forward Validation"
      ],
      "metadata": {
        "id": "PCQSAqstQinS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # #########################################################################\n",
        "# # # TimeSeriesSplit\n",
        "# # # #########################################################################\n",
        "\n",
        "# TimeSeriesSplit: 3 splits para ejemplo\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Almacenar las pérdidas para cada split\n",
        "split_losses = []\n",
        "\n",
        "# Probar si esto se puede sacar del bucle\n",
        "model_name = 'M1'\n",
        "loss = 'mse'\n",
        "optimizer = 'adam'\n",
        "patience = 5\n",
        "epochs = 100\n",
        "callbacks = MyCallbacks(patience)\n",
        "model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# Iterar sobre cada split\n",
        "for i, (train_index, test_index) in enumerate(tscv.split(data_norm)):\n",
        "    train_tscv = data_norm.iloc[train_index]\n",
        "    test_tscv = data_norm.iloc[test_index]\n",
        "\n",
        "    # Crear datasets de ventanas\n",
        "    # data_norm_train, data_norm_valid = split_data(data_norm)\n",
        "    data_train_wrangled = windowed_dataset(train_tscv.values, 'train', window_size, horizon, batch_size)\n",
        "    data_valid_wrangled = windowed_dataset(test_tscv.values, 'valid', window_size, horizon, batch_size)\n",
        "\n",
        "    # Check if datasets are empty and adjust if necessary\n",
        "    if len(list(data_train_wrangled)) == 0 or len(list(data_valid_wrangled)) == 0:\n",
        "      print(f\"Warning: Empty dataset encountered for split {i+1}. Skipping this split.\")\n",
        "      continue  # Skip to the next split\n",
        "\n",
        "    history = model.fit(\n",
        "    data_train_wrangled,\n",
        "    validation_data = data_valid_wrangled,\n",
        "    epochs=epochs,\n",
        "    verbose=2,\n",
        "    callbacks = callbacks)\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de validación\n",
        "    val_loss = model.evaluate(data_valid_wrangled)\n",
        "    # print(f'Split {i+1} - Loss: {val_loss}')\n",
        "    split_losses.append(val_loss)\n",
        "\n",
        "# Promedio de las pérdidas en todos los splits. El axis es por si analizamos mas de una metrica\n",
        "avg_loss = np.mean(split_losses, axis=0)\n",
        "print(f'Average Loss across all splits: {avg_loss}')\n",
        "\n",
        "genereate_predictions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA0pXJB_7OD7",
        "outputId": "eb59095a-a7f8-4168-f9bc-60d324d766ee"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_3 (Conv1D)           (None, 3, 64)             149824    \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPoolin  (None, 1, 64)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirecti  (None, 1, 64)             24832     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1560)              51480     \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 2, 780)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 236504 (923.84 KB)\n",
            "Trainable params: 236504 (923.84 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "2/2 - 11s - loss: 0.3883 - mae: 0.5408 - val_loss: 0.3457 - val_mae: 0.4998 - 11s/epoch - 6s/step\n",
            "Epoch 2/100\n",
            "2/2 - 0s - loss: 0.3843 - mae: 0.5376 - val_loss: 0.3418 - val_mae: 0.4963 - 47ms/epoch - 23ms/step\n",
            "Epoch 3/100\n",
            "2/2 - 0s - loss: 0.3788 - mae: 0.5330 - val_loss: 0.3371 - val_mae: 0.4921 - 45ms/epoch - 23ms/step\n",
            "Epoch 4/100\n",
            "2/2 - 0s - loss: 0.3710 - mae: 0.5264 - val_loss: 0.3315 - val_mae: 0.4869 - 46ms/epoch - 23ms/step\n",
            "Epoch 5/100\n",
            "2/2 - 0s - loss: 0.3642 - mae: 0.5201 - val_loss: 0.3247 - val_mae: 0.4804 - 60ms/epoch - 30ms/step\n",
            "Epoch 6/100\n",
            "2/2 - 0s - loss: 0.3589 - mae: 0.5155 - val_loss: 0.3166 - val_mae: 0.4728 - 63ms/epoch - 32ms/step\n",
            "Epoch 7/100\n",
            "2/2 - 0s - loss: 0.3493 - mae: 0.5070 - val_loss: 0.3076 - val_mae: 0.4644 - 45ms/epoch - 23ms/step\n",
            "Epoch 8/100\n",
            "2/2 - 0s - loss: 0.3378 - mae: 0.4968 - val_loss: 0.2976 - val_mae: 0.4549 - 48ms/epoch - 24ms/step\n",
            "Epoch 9/100\n",
            "2/2 - 0s - loss: 0.3252 - mae: 0.4844 - val_loss: 0.2860 - val_mae: 0.4439 - 44ms/epoch - 22ms/step\n",
            "Epoch 10/100\n",
            "2/2 - 0s - loss: 0.3077 - mae: 0.4683 - val_loss: 0.2734 - val_mae: 0.4319 - 44ms/epoch - 22ms/step\n",
            "Epoch 11/100\n",
            "2/2 - 0s - loss: 0.2950 - mae: 0.4556 - val_loss: 0.2599 - val_mae: 0.4187 - 45ms/epoch - 23ms/step\n",
            "Epoch 12/100\n",
            "2/2 - 0s - loss: 0.2814 - mae: 0.4417 - val_loss: 0.2456 - val_mae: 0.4045 - 46ms/epoch - 23ms/step\n",
            "Epoch 13/100\n",
            "2/2 - 0s - loss: 0.2805 - mae: 0.4410 - val_loss: 0.2307 - val_mae: 0.3896 - 46ms/epoch - 23ms/step\n",
            "Epoch 14/100\n",
            "2/2 - 0s - loss: 0.2643 - mae: 0.4250 - val_loss: 0.2157 - val_mae: 0.3741 - 64ms/epoch - 32ms/step\n",
            "Epoch 15/100\n",
            "2/2 - 0s - loss: 0.2271 - mae: 0.3833 - val_loss: 0.2004 - val_mae: 0.3579 - 44ms/epoch - 22ms/step\n",
            "Epoch 16/100\n",
            "2/2 - 0s - loss: 0.2223 - mae: 0.3790 - val_loss: 0.1849 - val_mae: 0.3409 - 46ms/epoch - 23ms/step\n",
            "Epoch 17/100\n",
            "2/2 - 0s - loss: 0.1911 - mae: 0.3427 - val_loss: 0.1699 - val_mae: 0.3237 - 49ms/epoch - 25ms/step\n",
            "Epoch 18/100\n",
            "2/2 - 0s - loss: 0.1698 - mae: 0.3174 - val_loss: 0.1559 - val_mae: 0.3070 - 47ms/epoch - 23ms/step\n",
            "Epoch 19/100\n",
            "2/2 - 0s - loss: 0.1628 - mae: 0.3086 - val_loss: 0.1433 - val_mae: 0.2916 - 46ms/epoch - 23ms/step\n",
            "Epoch 20/100\n",
            "2/2 - 0s - loss: 0.1479 - mae: 0.2894 - val_loss: 0.1324 - val_mae: 0.2779 - 47ms/epoch - 24ms/step\n",
            "Epoch 21/100\n",
            "2/2 - 0s - loss: 0.1197 - mae: 0.2527 - val_loss: 0.1234 - val_mae: 0.2664 - 49ms/epoch - 25ms/step\n",
            "Epoch 22/100\n",
            "2/2 - 0s - loss: 0.1187 - mae: 0.2522 - val_loss: 0.1164 - val_mae: 0.2570 - 48ms/epoch - 24ms/step\n",
            "Epoch 23/100\n",
            "2/2 - 0s - loss: 0.1161 - mae: 0.2470 - val_loss: 0.1112 - val_mae: 0.2497 - 49ms/epoch - 24ms/step\n",
            "Epoch 24/100\n",
            "2/2 - 0s - loss: 0.0952 - mae: 0.2264 - val_loss: 0.1077 - val_mae: 0.2441 - 46ms/epoch - 23ms/step\n",
            "Epoch 25/100\n",
            "2/2 - 0s - loss: 0.1052 - mae: 0.2394 - val_loss: 0.1054 - val_mae: 0.2396 - 46ms/epoch - 23ms/step\n",
            "Epoch 26/100\n",
            "2/2 - 0s - loss: 0.0849 - mae: 0.2055 - val_loss: 0.1039 - val_mae: 0.2362 - 45ms/epoch - 22ms/step\n",
            "Epoch 27/100\n",
            "2/2 - 0s - loss: 0.0769 - mae: 0.1915 - val_loss: 0.1032 - val_mae: 0.2339 - 46ms/epoch - 23ms/step\n",
            "Epoch 28/100\n",
            "2/2 - 0s - loss: 0.0696 - mae: 0.1822 - val_loss: 0.1030 - val_mae: 0.2320 - 44ms/epoch - 22ms/step\n",
            "Epoch 29/100\n",
            "2/2 - 0s - loss: 0.0809 - mae: 0.2088 - val_loss: 0.1031 - val_mae: 0.2306 - 42ms/epoch - 21ms/step\n",
            "Epoch 30/100\n",
            "2/2 - 0s - loss: 0.0667 - mae: 0.1777 - val_loss: 0.1031 - val_mae: 0.2291 - 43ms/epoch - 21ms/step\n",
            "Epoch 31/100\n",
            "2/2 - 0s - loss: 0.0657 - mae: 0.1771 - val_loss: 0.1035 - val_mae: 0.2281 - 47ms/epoch - 24ms/step\n",
            "Epoch 32/100\n",
            "2/2 - 0s - loss: 0.0627 - mae: 0.1709 - val_loss: 0.1041 - val_mae: 0.2277 - 43ms/epoch - 22ms/step\n",
            "Epoch 33/100\n",
            "2/2 - 0s - loss: 0.0533 - mae: 0.1544 - val_loss: 0.1048 - val_mae: 0.2275 - 52ms/epoch - 26ms/step\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1030 - mae: 0.2320\n",
            "Epoch 1/100\n",
            "8/8 - 0s - loss: 0.0889 - mae: 0.2170 - val_loss: 0.0838 - val_mae: 0.2089 - 136ms/epoch - 17ms/step\n",
            "Epoch 2/100\n",
            "8/8 - 0s - loss: 0.0755 - mae: 0.1951 - val_loss: 0.0748 - val_mae: 0.1934 - 82ms/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "8/8 - 0s - loss: 0.0644 - mae: 0.1822 - val_loss: 0.0701 - val_mae: 0.1855 - 82ms/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "8/8 - 0s - loss: 0.0600 - mae: 0.1748 - val_loss: 0.0675 - val_mae: 0.1813 - 89ms/epoch - 11ms/step\n",
            "Epoch 5/100\n",
            "8/8 - 0s - loss: 0.0622 - mae: 0.1803 - val_loss: 0.0668 - val_mae: 0.1814 - 91ms/epoch - 11ms/step\n",
            "Epoch 6/100\n",
            "8/8 - 0s - loss: 0.0548 - mae: 0.1674 - val_loss: 0.0659 - val_mae: 0.1818 - 83ms/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "8/8 - 0s - loss: 0.0529 - mae: 0.1630 - val_loss: 0.0654 - val_mae: 0.1794 - 82ms/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "8/8 - 0s - loss: 0.0525 - mae: 0.1625 - val_loss: 0.0651 - val_mae: 0.1764 - 89ms/epoch - 11ms/step\n",
            "Epoch 9/100\n",
            "8/8 - 0s - loss: 0.0480 - mae: 0.1547 - val_loss: 0.0650 - val_mae: 0.1742 - 89ms/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "8/8 - 0s - loss: 0.0472 - mae: 0.1533 - val_loss: 0.0654 - val_mae: 0.1756 - 84ms/epoch - 10ms/step\n",
            "Epoch 11/100\n",
            "8/8 - 0s - loss: 0.0482 - mae: 0.1572 - val_loss: 0.0659 - val_mae: 0.1786 - 87ms/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "8/8 - 0s - loss: 0.0504 - mae: 0.1619 - val_loss: 0.0647 - val_mae: 0.1747 - 85ms/epoch - 11ms/step\n",
            "Epoch 13/100\n",
            "8/8 - 0s - loss: 0.0484 - mae: 0.1563 - val_loss: 0.0623 - val_mae: 0.1723 - 89ms/epoch - 11ms/step\n",
            "Epoch 14/100\n",
            "8/8 - 0s - loss: 0.0514 - mae: 0.1637 - val_loss: 0.0625 - val_mae: 0.1720 - 85ms/epoch - 11ms/step\n",
            "Epoch 15/100\n",
            "8/8 - 0s - loss: 0.0486 - mae: 0.1563 - val_loss: 0.0617 - val_mae: 0.1700 - 83ms/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "8/8 - 0s - loss: 0.0518 - mae: 0.1639 - val_loss: 0.0623 - val_mae: 0.1701 - 83ms/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "8/8 - 0s - loss: 0.0466 - mae: 0.1551 - val_loss: 0.0621 - val_mae: 0.1699 - 81ms/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "8/8 - 0s - loss: 0.0442 - mae: 0.1495 - val_loss: 0.0614 - val_mae: 0.1694 - 101ms/epoch - 13ms/step\n",
            "Epoch 19/100\n",
            "8/8 - 0s - loss: 0.0501 - mae: 0.1602 - val_loss: 0.0607 - val_mae: 0.1693 - 87ms/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "8/8 - 0s - loss: 0.0464 - mae: 0.1547 - val_loss: 0.0607 - val_mae: 0.1702 - 81ms/epoch - 10ms/step\n",
            "Epoch 21/100\n",
            "8/8 - 0s - loss: 0.0500 - mae: 0.1590 - val_loss: 0.0597 - val_mae: 0.1714 - 82ms/epoch - 10ms/step\n",
            "Epoch 22/100\n",
            "8/8 - 0s - loss: 0.0498 - mae: 0.1587 - val_loss: 0.0597 - val_mae: 0.1689 - 90ms/epoch - 11ms/step\n",
            "Epoch 23/100\n",
            "8/8 - 0s - loss: 0.0444 - mae: 0.1506 - val_loss: 0.0611 - val_mae: 0.1683 - 83ms/epoch - 10ms/step\n",
            "Epoch 24/100\n",
            "8/8 - 0s - loss: 0.0602 - mae: 0.1786 - val_loss: 0.0631 - val_mae: 0.1733 - 85ms/epoch - 11ms/step\n",
            "Epoch 25/100\n",
            "8/8 - 0s - loss: 0.0482 - mae: 0.1583 - val_loss: 0.0641 - val_mae: 0.1770 - 85ms/epoch - 11ms/step\n",
            "Epoch 26/100\n",
            "8/8 - 0s - loss: 0.0434 - mae: 0.1487 - val_loss: 0.0640 - val_mae: 0.1780 - 84ms/epoch - 11ms/step\n",
            "Epoch 27/100\n",
            "8/8 - 0s - loss: 0.0421 - mae: 0.1464 - val_loss: 0.0628 - val_mae: 0.1737 - 104ms/epoch - 13ms/step\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0597 - mae: 0.1689\n",
            "Epoch 1/100\n",
            "14/14 - 0s - loss: 0.0550 - mae: 0.1698 - val_loss: 0.0930 - val_mae: 0.2336 - 157ms/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "14/14 - 0s - loss: 0.0536 - mae: 0.1660 - val_loss: 0.0813 - val_mae: 0.2185 - 132ms/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "14/14 - 0s - loss: 0.0492 - mae: 0.1602 - val_loss: 0.0756 - val_mae: 0.2113 - 126ms/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "14/14 - 0s - loss: 0.0498 - mae: 0.1602 - val_loss: 0.0864 - val_mae: 0.2248 - 131ms/epoch - 9ms/step\n",
            "Epoch 5/100\n",
            "14/14 - 0s - loss: 0.0467 - mae: 0.1551 - val_loss: 0.0853 - val_mae: 0.2228 - 123ms/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "14/14 - 0s - loss: 0.0519 - mae: 0.1650 - val_loss: 0.0740 - val_mae: 0.2092 - 192ms/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "14/14 - 0s - loss: 0.0507 - mae: 0.1604 - val_loss: 0.0788 - val_mae: 0.2149 - 212ms/epoch - 15ms/step\n",
            "Epoch 8/100\n",
            "14/14 - 0s - loss: 0.0543 - mae: 0.1723 - val_loss: 0.0898 - val_mae: 0.2290 - 209ms/epoch - 15ms/step\n",
            "Epoch 9/100\n",
            "14/14 - 0s - loss: 0.0505 - mae: 0.1625 - val_loss: 0.0811 - val_mae: 0.2189 - 211ms/epoch - 15ms/step\n",
            "Epoch 10/100\n",
            "14/14 - 0s - loss: 0.0525 - mae: 0.1655 - val_loss: 0.0763 - val_mae: 0.2116 - 206ms/epoch - 15ms/step\n",
            "Epoch 11/100\n",
            "14/14 - 0s - loss: 0.0486 - mae: 0.1587 - val_loss: 0.0893 - val_mae: 0.2283 - 216ms/epoch - 15ms/step\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0740 - mae: 0.2092\n",
            "Epoch 1/100\n",
            "20/20 - 0s - loss: 0.0600 - mae: 0.1811 - val_loss: 0.1025 - val_mae: 0.2365 - 387ms/epoch - 19ms/step\n",
            "Epoch 2/100\n",
            "20/20 - 0s - loss: 0.0534 - mae: 0.1696 - val_loss: 0.1085 - val_mae: 0.2428 - 291ms/epoch - 15ms/step\n",
            "Epoch 3/100\n",
            "20/20 - 0s - loss: 0.0545 - mae: 0.1716 - val_loss: 0.1048 - val_mae: 0.2363 - 332ms/epoch - 17ms/step\n",
            "Epoch 4/100\n",
            "20/20 - 0s - loss: 0.0490 - mae: 0.1604 - val_loss: 0.0991 - val_mae: 0.2319 - 333ms/epoch - 17ms/step\n",
            "Epoch 5/100\n",
            "20/20 - 0s - loss: 0.0485 - mae: 0.1606 - val_loss: 0.0957 - val_mae: 0.2276 - 318ms/epoch - 16ms/step\n",
            "Epoch 6/100\n",
            "20/20 - 0s - loss: 0.0547 - mae: 0.1705 - val_loss: 0.0963 - val_mae: 0.2268 - 167ms/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "20/20 - 0s - loss: 0.0540 - mae: 0.1705 - val_loss: 0.0894 - val_mae: 0.2202 - 163ms/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "20/20 - 0s - loss: 0.0464 - mae: 0.1553 - val_loss: 0.0904 - val_mae: 0.2211 - 159ms/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "20/20 - 0s - loss: 0.0535 - mae: 0.1695 - val_loss: 0.0928 - val_mae: 0.2248 - 175ms/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "20/20 - 0s - loss: 0.0501 - mae: 0.1625 - val_loss: 0.0983 - val_mae: 0.2292 - 152ms/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "20/20 - 0s - loss: 0.0478 - mae: 0.1594 - val_loss: 0.0853 - val_mae: 0.2162 - 168ms/epoch - 8ms/step\n",
            "Epoch 12/100\n",
            "20/20 - 0s - loss: 0.0497 - mae: 0.1611 - val_loss: 0.0897 - val_mae: 0.2210 - 161ms/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "20/20 - 0s - loss: 0.0487 - mae: 0.1607 - val_loss: 0.0937 - val_mae: 0.2257 - 170ms/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "20/20 - 0s - loss: 0.0510 - mae: 0.1639 - val_loss: 0.0883 - val_mae: 0.2201 - 179ms/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "20/20 - 0s - loss: 0.0520 - mae: 0.1666 - val_loss: 0.0906 - val_mae: 0.2214 - 165ms/epoch - 8ms/step\n",
            "Epoch 16/100\n",
            "20/20 - 0s - loss: 0.0533 - mae: 0.1692 - val_loss: 0.1037 - val_mae: 0.2352 - 160ms/epoch - 8ms/step\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0853 - mae: 0.2162\n",
            "Epoch 1/100\n",
            "26/26 - 0s - loss: 0.0552 - mae: 0.1699 - val_loss: 0.1096 - val_mae: 0.2513 - 245ms/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "26/26 - 0s - loss: 0.0530 - mae: 0.1682 - val_loss: 0.1159 - val_mae: 0.2597 - 202ms/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "26/26 - 0s - loss: 0.0513 - mae: 0.1655 - val_loss: 0.1206 - val_mae: 0.2668 - 193ms/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "26/26 - 0s - loss: 0.0516 - mae: 0.1655 - val_loss: 0.1092 - val_mae: 0.2505 - 202ms/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "26/26 - 0s - loss: 0.0539 - mae: 0.1704 - val_loss: 0.1134 - val_mae: 0.2568 - 209ms/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "26/26 - 0s - loss: 0.0496 - mae: 0.1625 - val_loss: 0.1149 - val_mae: 0.2584 - 215ms/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "26/26 - 0s - loss: 0.0503 - mae: 0.1636 - val_loss: 0.1210 - val_mae: 0.2672 - 206ms/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "26/26 - 0s - loss: 0.0510 - mae: 0.1653 - val_loss: 0.1152 - val_mae: 0.2588 - 206ms/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "26/26 - 0s - loss: 0.0481 - mae: 0.1607 - val_loss: 0.1047 - val_mae: 0.2439 - 215ms/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "26/26 - 0s - loss: 0.0529 - mae: 0.1689 - val_loss: 0.1065 - val_mae: 0.2467 - 223ms/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "26/26 - 0s - loss: 0.0483 - mae: 0.1613 - val_loss: 0.1133 - val_mae: 0.2547 - 201ms/epoch - 8ms/step\n",
            "Epoch 12/100\n",
            "26/26 - 0s - loss: 0.0486 - mae: 0.1597 - val_loss: 0.1061 - val_mae: 0.2446 - 202ms/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "26/26 - 0s - loss: 0.0507 - mae: 0.1641 - val_loss: 0.1102 - val_mae: 0.2500 - 214ms/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "26/26 - 0s - loss: 0.0473 - mae: 0.1574 - val_loss: 0.1080 - val_mae: 0.2453 - 232ms/epoch - 9ms/step\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.1047 - mae: 0.2439\n",
            "Average Loss across all splits: [0.08532271 0.21405484]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # #########################################################################\n",
        "# # # Sin Splitear los datos, usando todo para entrenar\n",
        "# # # #########################################################################\n",
        "\n",
        "#  data_train_windowed = window_dataset(data_norm, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# # data_train_windowed = window_dataset(data_norm, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 10\n",
        "\n",
        "# model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     # validation_data = data_valid_windowed,\n",
        "#     # callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# predicciones = generate_predictions()"
      ],
      "metadata": {
        "id": "VBcLFxbYss0G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/11 - Agrupando por Categoria 1"
      ],
      "metadata": {
        "id": "Mj0hs54Uc8_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Inicializo el vector de predicciones\n",
        "# predictions_acum = data_productos_a_predecir.copy()\n",
        "# predictions_acum['tn'] = 0\n",
        "# predictions_acum.set_index('product_id', inplace=True)\n",
        "# predictions_acum = predictions_acum.squeeze()\n",
        "# predictions_acum\n",
        "\n",
        "# # Genero el vector de categorias\n",
        "# categorias = data_productos_a_predecir_con_categorias.cat1.unique()\n",
        "\n",
        "# # Creo un modelo para cada categoria 1\n",
        "# for cat in categorias:\n",
        "#   print(cat)\n",
        "#   data_cat1 = filter_data_por_categoria(data_norm, cat, 'cat1')\n",
        "#   n_features = data_cat1.shape[1]\n",
        "#   data_train = data_cat1\n",
        "#   data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'CAT1'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 30\n",
        "#   epochs = 10\n",
        "\n",
        "#   # callbacks = MyCallbacks(model_name, patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       # validation_data = data_valid_windowed,\n",
        "#       # callbacks = callbacks,\n",
        "#       verbose=2,\n",
        "#       epochs=epochs)\n",
        "\n",
        "#   # Seleccionar los últimos x meses de data_train\n",
        "#   data_for_prediction = data_train[-window_size:]\n",
        "#   # Convierte los datos a un formato compatible con la función window_dataset\n",
        "#   data_for_prediction = data_for_prediction.values.reshape((1, window_size, n_features))\n",
        "#   predictions = model.predict(data_for_prediction)\n",
        "\n",
        "#   # # Convertir las predicciones a un DataFrame para desnormalizar\n",
        "#   predictions_df = pd.DataFrame(predictions[0], columns=data_train.columns)\n",
        "\n",
        "#   # # Desnormalizar las predicciones\n",
        "#   predictions_denorm_cat1 = denormalize_series(predictions_df, data_norm_params, normalization=normalization).iloc[1]\n",
        "\n",
        "#   # Voy sumando las predicciones de cada categoria\n",
        "#   predictions_acum = predictions_acum.add(predictions_denorm_cat1, fill_value=0)\n",
        "\n",
        "# # Exporto a formato Kaggle\n",
        "# predictions_acum_df = pd.DataFrame(predictions_acum).reset_index()\n",
        "# predictions_acum_df.columns = ['product_id', 'tn']\n",
        "# filename = f\"{model_name}_{split_strategy}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "# predictions_acum_df.to_csv(filename, header=True, index=False)\n",
        "# print(filename)"
      ],
      "metadata": {
        "id": "4i5vHk16uOl8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/11 - Analisis del Error por Producto"
      ],
      "metadata": {
        "id": "OisFvYA0feHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # #########################################################################\n",
        "# # # # # Train hasta 2019-10, para predecir 2019-12\n",
        "# # # # # #########################################################################\n",
        "# data_train, data_valid = split_data_dec2019(data_norm)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# # data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'Clase3'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 10\n",
        "\n",
        "# callbacks = MyCallbacks(model_name, patience)\n",
        "# model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     # validation_data = data_valid_windowed,\n",
        "#     # callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# # No sirve para este caso, esta hardcodeada a Febrero 2020. Tengo que actualizar esta funcion\n",
        "# # generate_predictions(data_norm, data_norm_params)\n",
        "\n",
        "# # Seleccionar los últimos 6 meses de data_train\n",
        "# data_for_prediction = data_train[-window_size:]\n",
        "# # Convierte los datos a un formato compatible con la función window_dataset\n",
        "# data_for_prediction = data_for_prediction.values.reshape((1, window_size, n_features))\n",
        "# predictions = model.predict(data_for_prediction)\n",
        "\n",
        "# # Convertir las predicciones a un DataFrame para desnormalizar\n",
        "# predictions_df = pd.DataFrame(predictions[0], columns=data_train.columns)\n",
        "\n",
        "# # Desnormalizar las predicciones\n",
        "# predictions_denorm = denormalize_series(predictions_df, data_norm_params, normalization=normalization)\n",
        "\n",
        "# # Imprimir las predicciones desnormalizadas\n",
        "# # display(predictions_denorm)\n",
        "\n",
        "# # Exporto a formato Kaggle, para probar un tirito (no sirve porque Kaggle valida a Febrero 2020)\n",
        "# predictions_denorm.iloc[1].to_csv('predictions_dec2019.csv', header=True, index=True)\n",
        "\n",
        "# # Genero las Series para plotear el error entre predicho y real. Armo un Dataframe\n",
        "# data_dec2019_pred = pd.Series(predictions_denorm.iloc[1], name='Pred')\n",
        "# data_dec2019_true = pd.Series(data_grouped.fillna(0).loc['2019-12-01'], name='True')\n",
        "# data_dec2019_error = pd.concat([data_dec2019_pred, data_dec2019_true], axis=1)\n",
        "\n",
        "# # Clusterizo por Categorias 1 y 2 de productos\n",
        "# data_productos_indexed = data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()\n",
        "# data_dec2019_error_detail = data_dec2019_error.join(data_productos_indexed[['cat1', 'cat2', 'cat3']])\n",
        "\n",
        "# # Ploteo los Errores, con sus clusters\n",
        "# sns.scatterplot(data=data_dec2019_error_detail, x='Pred', y='True', hue='cat1')\n",
        "# plt.axline((0, 0), slope=1, color='r', linestyle='--')\n",
        "# plt.show()\n",
        "\n",
        "# # Verificamos que Categoria es la que engloba mas productos\n",
        "# print(data_productos['cat1'].value_counts())\n",
        "\n",
        "# # Index de los productos que suelen predecirse de menos en HC\n",
        "# # HC_boost_index = data_dec2019_error_detail.query('cat1 == \"HC\" and Pred > 350 and Pred < 600').index\n",
        "\n",
        "# # Productos con mayores diferencias en la prediccion. No me sirve de mucho ver este listado\n",
        "# # print('Productos con peores predicciones:\\n', abs(data_dec2019_pred - data_dec2019_true).sort_values(ascending=False).head(10))\n"
      ],
      "metadata": {
        "id": "8tw6MlALfgrx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vemos claramente como estsamos prediciendo mal los productos de Health Care, mas que nada los que predice entre 350 y 600, esta prediciendo bastante de menos."
      ],
      "metadata": {
        "id": "7hM59Wm47VGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - Cada cliente por separado"
      ],
      "metadata": {
        "id": "fPq4wBabw7gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esto lo corro en mi maquina local, en Colab se cuelga antes de terminar con los casi 600 clientes\n",
        "\n",
        "# # Inicializo el vector de predicciones\n",
        "# predicciones_all = data_productos_a_predecir.copy()\n",
        "# predicciones_all['tn'] = 0\n",
        "\n",
        "# # Probamos solo con los 3 primeros clientes\n",
        "# customers = data['customer_id'].unique()\n",
        "# i = 0\n",
        "\n",
        "# for customer in customers:\n",
        "#   print('Vuelta: ', i)\n",
        "#   i += 1\n",
        "#   data_customer = data_filter.query('customer_id == @customer')\n",
        "#   data_customer_grouped = group_data(data_customer, 'product_id')\n",
        "#   data_customer_grouped_fixed = complete_sales(data_customer_grouped, data_productos_a_predecir)\n",
        "#   data_customer_filled = fill_nulls(data_customer_grouped_fixed) # Probar cual funciona mejor\n",
        "#   # data_customer_filled = fill_nulls_customer(data_customer_grouped_fixed) # Probar cual funciona mejor\n",
        "#   data_customer_norm, data_customer_norm_params = normalize_data(data_customer_filled, normalization=normalization)\n",
        "\n",
        "#   data_train_windowed = window_dataset(data_customer_norm, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'M1'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 30\n",
        "#   epochs = 10\n",
        "\n",
        "#   model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       # validation_data = data_valid_windowed,\n",
        "#       # callbacks = callbacks,\n",
        "#       verbose=2,\n",
        "#       epochs=epochs)\n",
        "\n",
        "#   predicciones = generate_predictions(data_customer_norm, data_customer_norm_params)\n",
        "\n",
        "#   predicciones_all = sumar_predicciones(predicciones_all, predicciones)\n",
        "\n",
        "# predicciones_all.to_csv('/content/drive/MyDrive/Universidad Austral (Resumenes)/predicciones2.csv', header=True, index=False)"
      ],
      "metadata": {
        "id": "TvvXLKmpyS5s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - No Split"
      ],
      "metadata": {
        "id": "MFRKlhKimrIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # #########################################################################\n",
        "# # # Sin Splitear los datos, usando todo para entrenar\n",
        "# # # #########################################################################\n",
        "\n",
        "# data_train_windowed = windowed_dataset(data_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# # data_train_windowed = window_dataset(data_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# # loss = 'mse'\n",
        "# # optimizer = 'adam'\n",
        "# # patience = 30\n",
        "# epochs = 10\n",
        "\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     # validation_data = data_valid_windowed,\n",
        "#     # callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# # predicciones = generate_predictions()"
      ],
      "metadata": {
        "id": "JFTI6TUpmrRP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "outputId": "970f872f-c65c-4353-80a5-f566cafd35d2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_1 (Conv1D)           (None, 6, 64)             149824    \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 3, 64)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 3, 64)             24832     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 780)               25740     \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 1, 780)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 210764 (823.30 KB)\n",
            "Trainable params: 210764 (823.30 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "30/30 - 8s - loss: 0.1818 - mae: 0.3303 - 8s/epoch - 282ms/step\n",
            "Epoch 2/10\n",
            "30/30 - 0s - loss: 0.1180 - mae: 0.2611 - 264ms/epoch - 9ms/step\n",
            "Epoch 3/10\n",
            "30/30 - 0s - loss: 0.0972 - mae: 0.2350 - 265ms/epoch - 9ms/step\n",
            "Epoch 4/10\n",
            "30/30 - 0s - loss: 0.0872 - mae: 0.2217 - 337ms/epoch - 11ms/step\n",
            "Epoch 5/10\n",
            "30/30 - 1s - loss: 0.0788 - mae: 0.2110 - 530ms/epoch - 18ms/step\n",
            "Epoch 6/10\n",
            "30/30 - 1s - loss: 0.0702 - mae: 0.1995 - 501ms/epoch - 17ms/step\n",
            "Epoch 7/10\n",
            "30/30 - 1s - loss: 0.0703 - mae: 0.1990 - 519ms/epoch - 17ms/step\n",
            "Epoch 8/10\n",
            "30/30 - 0s - loss: 0.0650 - mae: 0.1918 - 496ms/epoch - 17ms/step\n",
            "Epoch 9/10\n",
            "30/30 - 1s - loss: 0.0644 - mae: 0.1919 - 541ms/epoch - 18ms/step\n",
            "Epoch 10/10\n",
            "30/30 - 1s - loss: 0.0639 - mae: 0.1908 - 543ms/epoch - 18ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "generate_predictions() missing 2 required positional arguments: 'data_norm' and 'data_norm_params'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-d8db6cde8c1b>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     epochs=epochs)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpredicciones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: generate_predictions() missing 2 required positional arguments: 'data_norm' and 'data_norm_params'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - Split #2"
      ],
      "metadata": {
        "id": "56uvfCGZfKek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # #########################################################################\n",
        "# # # Train desde 2017-01 hasta 2019-06\n",
        "# # # #########################################################################\n",
        "\n",
        "# data_train, data_valid = split_data_2(data_norm)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "\n",
        "# callbacks = MyCallbacks(model_name, patience)\n",
        "# model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# generate_predictions()"
      ],
      "metadata": {
        "id": "kMCpoMFHfMgh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - Split #1"
      ],
      "metadata": {
        "id": "Z2xfLSMdU96y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # #########################################################################\n",
        "# # # Train 2018 & 2018, Validation 2019 (10/06)\n",
        "# # # #########################################################################\n",
        "# data_train, data_valid = split_data_1(data_norm)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "\n",
        "# callbacks = MyCallbacks(model_name, patience)\n",
        "# model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# generate_predictions()"
      ],
      "metadata": {
        "id": "I4NrpvT1VgNz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate: Product 20001"
      ],
      "metadata": {
        "id": "9L6P6xEM3XPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data = data_filled[['20001']]\n",
        "\n",
        "# # Parámetros\n",
        "# window_size = 6  # Tamaño de la ventana de tiempo\n",
        "# n_future = 2  # Número de pasos futuros a predecir\n",
        "# batch_size = 32\n",
        "# n_splits = 5  # Número de divisiones para validación \"walk forward\"\n",
        "# n_features = data.shape[1]  # Número de características en el conjunto de datos\n",
        "\n",
        "# data_norm, data_norm_params = normalize_data(data, normalization=normalization)\n",
        "# data_norm"
      ],
      "metadata": {
        "id": "keCpJUPo3ggY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proof of Concept"
      ],
      "metadata": {
        "id": "y28Ak-HH3TEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #########################################################################\n",
        "# # New Pipeline (09/06)\n",
        "# #########################################################################\n",
        "# data_norm, data_norm_params = normalize_data(data_filled, normalization=normalization)\n",
        "# # data_train, data_valid = split_data(data_norm) # Split pendiente\n",
        "# data_train = data_norm\n",
        "# print(data_train.shape)\n",
        "# # print(data_valid.shape)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# # data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "\n",
        "# #########################################################################\n",
        "# # Old Pipeline (08/06)\n",
        "# #########################################################################\n",
        "# # data_all = group_data(data, data_productos_a_predecir)\n",
        "# # data_all_norm, data_all_norm_params = normalize_data(data_all, normalization=normalization)\n",
        "# # data_all_norm['20001'].describe()\n",
        "# # data_train, data_valid = split_data_all(data_all_norm)\n",
        "# # print(data_train.shape)\n",
        "# # print(data_valid.shape)\n",
        "# # data_train = data_all_norm\n",
        "# # data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# # data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "\n",
        "# #########################################################################\n",
        "# # Modelo\n",
        "# #########################################################################\n",
        "# data_train, data_valid = split_data_1(data_norm)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "\n",
        "# callbacks = MyCallbacks(model_name, patience)\n",
        "# model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# generate_predictions()"
      ],
      "metadata": {
        "id": "bCx0jH7rFU-M"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "CRuRNJCI8J7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evidentemente cuando el profe deidentifico los customers, lo hizo asignandoles ID secuenciales al listado ordenado por la suma de ventas(tn)\n",
        "# print('Listado de Clientes, ordenados por la sumatoria de ventas en tn:\\n', group_data(data, 'customer_id').sum(), '\\n')\n",
        "\n",
        "# # Lo mismo cuando deidentifico a los productos, solo que esta vez empezo desde 20000\n",
        "# print('Listado de Productos, ordenados por la sumatoria de ventas en tn:\\n', group_data(data, 'product_id').sum())"
      ],
      "metadata": {
        "id": "QOAjN92m8LJU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analisis del cliente top\n",
        "\n",
        "# data_top = data.query('customer_id == \"10001\"')\n",
        "# data_top = group_data(data_top, 'product_id')\n",
        "# data_top = fill_nulls(data_top)\n",
        "\n",
        "# # data_top"
      ],
      "metadata": {
        "id": "4_qyQISS8bMF"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}