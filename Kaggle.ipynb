{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNHmBHfQijfKnyiy+lza6H6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresMontesDeOca/Laboratorio3/blob/main/Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modulos"
      ],
      "metadata": {
        "id": "crj3eqSV_WvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "eL3soxEh_cgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ColabNotebook = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# if ColabNotebook: # maquina virtual colab\n",
        "#     # monta G-drive en entorno COLAB\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "HkJAzqZfivZx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "import warnings\n",
        "# warnings.filterwarnings('ignore', category=ValueWarning)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Asegurarte de que Pandas muestre los valores con la máxima precisión\n",
        "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
        "\n",
        "# Ajustar la opción para mostrar más filas\n",
        "pd.set_option('display.max_rows', 50)\n",
        "\n",
        "# Si también quieres mostrar más columnas\n",
        "pd.set_option('display.max_columns', 50)\n",
        "\n",
        "\n",
        "# Vamos a suprimir la notacion cientifica\n",
        "pd.set_option(\"display.float_format\", lambda x:\"%.2f\" %x)\n",
        "\n",
        "\n",
        "def print_ds(ds, take=3):\n",
        "    for ex in ds.take(take):\n",
        "        print(ex)\n"
      ],
      "metadata": {
        "id": "4dKLvveO9QDs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga Datos"
      ],
      "metadata": {
        "id": "CCBiyQvD_nC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local"
      ],
      "metadata": {
        "id": "rhxib23DtW_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Code to read csv file into Colaboratory:\n",
        "# # # !pip install -U -q PyDrive\n",
        "# # from pydrive.auth import GoogleAuth\n",
        "# # from pydrive.drive import GoogleDrive\n",
        "# # from google.colab import auth, drive\n",
        "# # from oauth2client.client import GoogleCredentials\n",
        "# #\n",
        "# # # Authenticate and create the PyDrive client.\n",
        "# # auth.authenticate_user()\n",
        "# # gauth = GoogleAuth()\n",
        "# # gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# # drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "# ################################# Datasets ###################################\n",
        "# # # Ventas\n",
        "# # id = \"158aOjqxaNO8l97yA6VWJkek_15YVLMhs\"\n",
        "# # downloaded = drive.CreateFile({'id':id})\n",
        "# # downloaded.GetContentFile('sell-in.txt')\n",
        "# data_ventas = pd.read_csv(\"G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\sell-in.txt\", sep=\"\\t\")\n",
        "# data_ventas['periodo'] = pd.to_datetime(data_ventas['periodo'], format='%Y%m')\n",
        "# data_ventas['customer_id'] = data_ventas['customer_id'].astype(str)\n",
        "# data_ventas['product_id'] = data_ventas['product_id'].astype(str)\n",
        "# data = data_ventas.copy()\n",
        "# data.set_index('periodo', inplace=True)\n",
        "# data_bkp = data.copy()\n",
        "\n",
        "# # # # Productos\n",
        "# # id = \"15JS_k86LS0sgJXma7BOVXWlyNcMwxdhE\"\n",
        "# # downloaded = drive.CreateFile({'id':id})\n",
        "# # downloaded.GetContentFile('tb_productos.txt')\n",
        "# data_productos = pd.read_csv(\"G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\tb_productos.txt\", sep=\"\\t\")\n",
        "# data_productos['product_id'] = data_productos['product_id'].astype(str)\n",
        "\n",
        "# # # # Stocks\n",
        "# # id = \"15EV-8f_U7onpA1AcTxxXeD-z8yVR4fQu\"\n",
        "# # downloaded = drive.CreateFile({'id':id})\n",
        "# # downloaded.GetContentFile('G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\tb_stocks.txt')\n",
        "# data_stocks = pd.read_csv('G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\tb_stocks.txt', sep=\"\\t\")\n",
        "# data_stocks['periodo'] = pd.to_datetime(data_stocks['periodo'], format='%Y%m')\n",
        "# data_stocks['product_id'] = data_stocks['product_id'].astype(str)\n",
        "\n",
        "# # # # Productos a predecir\n",
        "# # id = \"15LjADctFVwjzQFJvfJGFTEdgZx9xCoId\"\n",
        "# # downloaded = drive.CreateFile({'id':id})\n",
        "# # downloaded.GetContentFile('productos_a_predecir.txt')\n",
        "# data_productos_a_predecir = pd.read_csv(\"G:\\\\My Drive\\\\Colab Notebooks\\\\Data\\\\Labo3\\\\productos_a_predecir.txt\", sep=\"\\t\")\n",
        "# data_productos_a_predecir['product_id'] = data_productos_a_predecir['product_id'].astype(str)\n",
        "# # data_productos_a_predecir_con_categorias = data_productos_a_predecir.set_index('product_id').join(data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()[['cat1', 'cat2', 'cat3']]) # Esta mal\n",
        "# data_productos_a_predecir_con_categorias = data_productos_a_predecir.join(data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()[['cat1', 'cat2', 'cat3']])\n"
      ],
      "metadata": {
        "id": "jzQavP-RtCH2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab"
      ],
      "metadata": {
        "id": "04LDga8vtOpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "# !pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "################################# Datasets ###################################\n",
        "# # Ventas\n",
        "id = \"158aOjqxaNO8l97yA6VWJkek_15YVLMhs\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('sell-in.txt')\n",
        "data_ventas = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
        "data_ventas['periodo'] = pd.to_datetime(data_ventas['periodo'], format='%Y%m')\n",
        "data_ventas['customer_id'] = data_ventas['customer_id'].astype(str)\n",
        "data_ventas['product_id'] = data_ventas['product_id'].astype(str)\n",
        "data = data_ventas.copy()\n",
        "data.set_index('periodo', inplace=True)\n",
        "data_bkp = data.copy()\n",
        "\n",
        "# # Productos\n",
        "id = \"15JS_k86LS0sgJXma7BOVXWlyNcMwxdhE\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('tb_productos.txt')\n",
        "data_productos = pd.read_csv(\"tb_productos.txt\", sep=\"\\t\")\n",
        "data_productos['product_id'] = data_productos['product_id'].astype(str)\n",
        "\n",
        "# # Stocks\n",
        "id = \"15EV-8f_U7onpA1AcTxxXeD-z8yVR4fQu\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('tb_stocks.txt')\n",
        "data_stocks = pd.read_csv(\"tb_stocks.txt\", sep=\"\\t\")\n",
        "data_stocks['periodo'] = pd.to_datetime(data_stocks['periodo'], format='%Y%m')\n",
        "data_stocks['product_id'] = data_stocks['product_id'].astype(str)\n",
        "\n",
        "# # Productos a predecir\n",
        "id = \"15LjADctFVwjzQFJvfJGFTEdgZx9xCoId\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('productos_a_predecir.txt')\n",
        "data_productos_a_predecir = pd.read_csv(\"productos_a_predecir.txt\", sep=\"\\t\")\n",
        "data_productos_a_predecir['product_id'] = data_productos_a_predecir['product_id'].astype(str)\n",
        "\n",
        "# # Agregado 06/15, Ojo que puede romper en algun lado ya que antes no usaba el product_id como index\n",
        "data_productos_a_predecir_index = data_productos_a_predecir.set_index('product_id', inplace=True)\n",
        "# # No va\n",
        "# data_productos_a_predecir.set_index('product_id', inplace=True)\n",
        "\n",
        "\n",
        "# Comentado por las dudas 06/15\n",
        "data_productos_a_predecir_con_categorias = data_productos_a_predecir.join(data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()[['cat1', 'cat2', 'cat3']])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8GISdopF_obd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49220611-4d74-44b3-d9a7-9c39417c4477"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Functions"
      ],
      "metadata": {
        "id": "dJckQiyL08r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "def filter_active_clients_data(df, show_plot=False):\n",
        "  presencia_clientes = df.groupby(['periodo', 'customer_id']).size().unstack(fill_value=0)\n",
        "  # Convertimos las cantidades en 1 para indicar presencia\n",
        "  presencia_clientes[presencia_clientes > 0] = 1\n",
        "  # Creamos la Maskara, en este caso para los ultimos 3 meses\n",
        "  mask_active_clients = presencia_clientes.loc['2019-10':'2019-12'].sum(axis=0) == 3\n",
        "  # Filtramos y devolvemos el Dataset\n",
        "  active_data = df[df['customer_id'].isin(mask_active_clients[mask_active_clients].index)]\n",
        "\n",
        "  if show_plot:\n",
        "    # Configuramos el tamaño de la figura\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Creamos el heatmap\n",
        "    sns.heatmap(presencia_clientes, cmap='viridis', cbar=False, linewidths=.5)\n",
        "\n",
        "    # Añadimos los títulos y etiquetas\n",
        "    plt.title('Presencia de Clientes por Mes')\n",
        "    plt.xlabel('Clientes')\n",
        "    plt.ylabel('Mes')\n",
        "\n",
        "    # Mostramos el gráfico\n",
        "    plt.show()\n",
        "\n",
        "  return active_data\n",
        "##############################################################################\n",
        "def filter_dataframe(dataframe, boolean_series):\n",
        "    # Verificar que los índices de la Serie booleana y del DataFrame coinciden\n",
        "    if not boolean_series.index.equals(dataframe.columns):\n",
        "        raise ValueError(\"Los índices de la Serie booleana y las columnas del DataFrame no coinciden\")\n",
        "\n",
        "    # Filtrar el DataFrame usando la Serie booleana\n",
        "    filtered_dataframe = dataframe.loc[:, boolean_series]\n",
        "\n",
        "    return filtered_dataframe\n",
        "##############################################################################\n",
        "def filter_data(data_all, data_filter):\n",
        "    # Filtrar el DataFrame 'data_all' para que solo contenga los 'product_id' presentes en 'data_filter'\n",
        "    data_filtered = data_all[data_all['product_id'].isin(data_filter['product_id'])]\n",
        "\n",
        "    return data_filtered\n",
        "##############################################################################\n",
        "def filter_data_por_categoria(df, categoria, categoria_columna):\n",
        "    \"\"\"\n",
        "    Filtra los productos de un DataFrame dado una categoría y el DataFrame de productos con categorías.\n",
        "\n",
        "    Args:\n",
        "    dataframe (pd.DataFrame): DataFrame con las ventas de productos (cada columna es un product_id).\n",
        "    categoria (str): Categoría a filtrar (valor de cat1, cat2 o cat3).\n",
        "    categoria_columna (str): Nombre de la columna de categoría ('cat1', 'cat2' o 'cat3').\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame filtrado con solo los productos de la categoría especificada.\n",
        "    \"\"\"\n",
        "    # Filtrar los productos que pertenecen a la categoría especificada\n",
        "    productos_filtrados = data_productos_a_predecir_con_categorias[data_productos_a_predecir_con_categorias[categoria_columna] == categoria].index\n",
        "\n",
        "    # Filtrar el DataFrame de ventas usando los product_ids de los productos filtrados\n",
        "    productos_en_data = [col for col in df.columns if col in productos_filtrados]\n",
        "    df_filtrado = df[productos_en_data]\n",
        "\n",
        "    return df_filtrado\n",
        "##############################################################################"
      ],
      "metadata": {
        "id": "nnSdLDk60-bO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Functions"
      ],
      "metadata": {
        "id": "w-TmJKaCP5gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def plot_history(history, start_epoch=0, metrics=None):\n",
        "    if isinstance(metrics, str):\n",
        "        metrics = [metrics]\n",
        "\n",
        "    if metrics is None:\n",
        "        metrics = [x for x in history.history.keys() if x[:4] != 'val_']\n",
        "\n",
        "    if len(metrics) == 0:\n",
        "        print('No metrics to display.')\n",
        "        return\n",
        "\n",
        "    # Get the epochs and filter them starting from start_epoch\n",
        "    x = history.epoch[start_epoch:]\n",
        "\n",
        "    rows = 1\n",
        "    cols = len(metrics)\n",
        "    count = 0\n",
        "\n",
        "    plt.figure(figsize=(12 * cols, 8))\n",
        "\n",
        "    for metric in sorted(metrics):\n",
        "        count += 1\n",
        "        plt.subplot(rows, cols, count)\n",
        "        plt.plot(x, history.history[metric][start_epoch:], label='Train')\n",
        "        val_metric = f'val_{metric}'\n",
        "        if val_metric in history.history.keys():\n",
        "            plt.plot(x, history.history[val_metric][start_epoch:], label='Validation')\n",
        "        plt.title(metric.capitalize())\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "################################################################\n",
        "def plot_predictions(data_all, predictions):\n",
        "  data_conc = pd.concat([data_all, predictions])\n",
        "  separation = data_all.index[-1]\n",
        "\n",
        "  # Crear una figura y un eje\n",
        "  fig, ax = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "  # Plotear toda la serie con un color\n",
        "  ax.plot(data_conc.index, data_conc.values, label='Datos', color='blue')\n",
        "\n",
        "  ax.axvline(x=separation, color='green', linestyle='--', label='Fecha de separación')\n",
        "\n",
        "  # Sobrescribir los últimos dos elementos con un color diferente\n",
        "  ax.plot(data_conc.index[-3:], data_conc.values[-3:], color='red')\n",
        "\n",
        "  # Añadir texto para la fecha de separación\n",
        "  ax.text(separation, ax.get_ylim()[1], separation.strftime('%Y-%m'),\n",
        "            color='green', verticalalignment='bottom', horizontalalignment='right')\n",
        "\n",
        "  # Añadir leyenda y mostrar el gráfico\n",
        "  ax.legend(['Datos', 'Predicciones'])\n",
        "  plt.show()\n",
        "  display(predictions)\n",
        "  ################################################################\n",
        "def plot_predictions_dec2019(data_all, predictions_original):\n",
        "  separation = data_all.index[-3]\n",
        "  predictions_updated = pd.concat([data_all.loc['2019-10'], predictions_original])\n",
        "\n",
        "  # Crear una figura y un eje\n",
        "  fig, ax = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "  # Plotear toda la serie con un color\n",
        "  ax.plot(data_all.index, data_all.values, label='Datos', color='blue')\n",
        "\n",
        "  # Sobrescribir los últimos dos elementos con un color diferente\n",
        "  ax.plot(predictions_updated.index, predictions_updated.values, color='red', label='Predicciones')\n",
        "\n",
        "  # Linea de corte\n",
        "  ax.axvline(x=separation, color='black', linestyle='--', linewidth=0.5)\n",
        "\n",
        "  # Añadir texto para la fecha de corte\n",
        "  ax.text(separation, ax.get_ylim()[1], separation.strftime('%Y-%m'),\n",
        "            color='black', verticalalignment='bottom', horizontalalignment='right')\n",
        "\n",
        "  # Añadir una línea horizontal en el último valor de predictions_updated\n",
        "  ax.axhline(y=predictions_updated.iloc[-1], color='green', linestyle='--', linewidth=1)\n",
        "\n",
        "  # Calculo la Diferencia\n",
        "  diff =  np.round((predictions.iloc[-1] - data_all.iloc[-1]) / data_all.iloc[-1] * 100, 2)\n",
        "\n",
        "  # Añadir leyenda y mostrar el gráfico\n",
        "  ax.legend()\n",
        "  plt.title(f'Producto: {predictions.name}, Diferencia en la prediccion %{diff}')\n",
        "  plt.show()\n",
        "  print('Real value:', data_all.iloc[-1])\n",
        "  print(predictions)\n",
        "\n",
        "#############################################################################"
      ],
      "metadata": {
        "id": "vVoxNL0mP5od"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group Data"
      ],
      "metadata": {
        "id": "5L-kvH-J_7SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Siempre como values toma las toneladas vendidas\n",
        "def group_data(data, column):\n",
        "  grouped_data = data.groupby([column, 'periodo']).sum().reset_index()\n",
        "\n",
        "  # Crea un DataFrame pivoteado donde las filas son las fechas y las columnas son los product_id\n",
        "  pivot_data = grouped_data.pivot(index='periodo', columns=column, values='tn')\n",
        "\n",
        "  # Asegúrate de que los nombres de las columnas sean strings\n",
        "  pivot_data.columns = pivot_data.columns.astype(str)\n",
        "\n",
        "  # Restablece el índice para asegurarse de que 'product_id' no sea un índice compuesto\n",
        "  pivot_data.columns.name = None\n",
        "\n",
        "  return pivot_data"
      ],
      "metadata": {
        "id": "Mrkkv5Kd0yCG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill & Fix Functions"
      ],
      "metadata": {
        "id": "p_XRfvor4fhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## INVESTIGAR InterpolacionLineal y Media Movil\n",
        "# # Interpolación lineal\n",
        "# df_interpolated = df.interpolate(method='linear')\n",
        "\n",
        "# # Mostrar la serie con interpolación\n",
        "# print(\"\\nSerie con interpolación lineal:\")\n",
        "# print(df_interpolated)\n",
        "\n",
        "# # Relleno hacia adelante (forward fill)\n",
        "# df_ffill = df.fillna(method='ffill')\n",
        "\n",
        "# # Mostrar la serie con relleno hacia adelante\n",
        "# print(\"\\nSerie con relleno hacia adelante:\")\n",
        "# print(df_ffill)\n",
        "\n",
        "# # Relleno hacia atrás (backward fill)\n",
        "# df_bfill = df.fillna(method='bfill')\n",
        "\n",
        "# # Mostrar la serie con relleno hacia atrás\n",
        "# print(\"\\nSerie con relleno hacia atrás:\")\n",
        "# print(df_bfill)\n",
        "\n",
        "# # Relleno con la media móvil\n",
        "# window_size = 3\n",
        "# df['value'] = df['value'].fillna(df['value'].rolling(window=window_size, min_periods=1).mean())\n",
        "\n",
        "###########################################################################\n",
        "def fix_aug2019(df_grouped, option):\n",
        "  if option == 'mean':\n",
        "    df_grouped_tmp = df_grouped.drop(index='2019-08', axis=1)\n",
        "    data_agosto_2019_mean = df_grouped_tmp.loc[['2019-07', '2019-09']].mean().to_frame().transpose()\n",
        "    data_agosto_2019_mean.index = pd.to_datetime(['2019-08-01'])\n",
        "    data_grouped_return = pd.concat([df_grouped_tmp, data_agosto_2019_mean]).sort_index()\n",
        "  elif option == 'drop':\n",
        "    data_grouped_return = df_grouped.drop(index='2019-08', axis=1)\n",
        "  elif option == 'julplus10':\n",
        "    df_grouped_tmp = df_grouped.drop(index='2019-08', axis=1)\n",
        "    data_agosto_2019_jul_plus10 = df_grouped_tmp.loc['2019-07']*1.1\n",
        "    data_agosto_2019_jul_plus10.index = pd.to_datetime(['2019-08-01'])\n",
        "    data_grouped_return = pd.concat([df_grouped_tmp, data_agosto_2019_jul_plus10]).sort_index()\n",
        "  else:\n",
        "    raise ValueError(\"Invalid option. Choose 'mean' or 'drop'.\")\n",
        "    data_grouped_return= None\n",
        "\n",
        "  return data_grouped_return\n",
        "###########################################################################\n",
        "def fill_nulls(df):\n",
        "  # Primero usamos bfill para completar las ordenes mas viejas con los valores de las ordenes mas recientes\n",
        "  df = df.bfill()\n",
        "  # Luego completamos con ceros los productos que dejamos de vender, o se discontinuaron\n",
        "  df = df.fillna(0)\n",
        "  return df\n",
        "###########################################################################"
      ],
      "metadata": {
        "id": "WPghARRT5HE3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize Data"
      ],
      "metadata": {
        "id": "a2ctIs9UCRRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creo que solo come DataFrames\n",
        "###########################################################################\n",
        "def normalize_data(train, valid, normalization=\"MinMax\"):\n",
        "    if normalization == \"MinMax\":\n",
        "        scaler = MinMaxScaler()\n",
        "    elif normalization == \"ZScore\":\n",
        "        scaler = StandardScaler()\n",
        "    else:\n",
        "        raise ValueError(\"normalization parameter must be either 'MinMax' or 'ZScore'\")\n",
        "\n",
        "    # Normalizar el conjunto de entrenamiento\n",
        "    train_norm = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
        "\n",
        "    # Normalizar el conjunto de validación\n",
        "    valid_norm = pd.DataFrame(scaler.transform(valid), columns=valid.columns, index=valid.index)\n",
        "\n",
        "    if normalization == \"MinMax\":\n",
        "        normalization_params = pd.DataFrame({\n",
        "            'min': scaler.data_min_,\n",
        "            'max': scaler.data_max_\n",
        "        }, index=train.columns)\n",
        "    elif normalization == \"ZScore\":\n",
        "        normalization_params = pd.DataFrame({\n",
        "            'mean': scaler.mean_,\n",
        "            'std': scaler.scale_\n",
        "        }, index=train.columns)\n",
        "\n",
        "    return train_norm, valid_norm, normalization_params\n",
        "###########################################################################\n",
        "def denormalize_data(normalized_series, normalization_params, normalization=\"MinMax\"):\n",
        "    if normalization == \"MinMax\":\n",
        "        denormalized_data = normalized_series * (normalization_params['max'] - normalization_params['min']) + normalization_params['min']\n",
        "    elif normalization == \"ZScore\":\n",
        "        denormalized_data = normalized_series * normalization_params['std'] + normalization_params['mean']\n",
        "    else:\n",
        "        raise ValueError(\"normalization parameter must be either 'MinMax' or 'ZScore'\")\n",
        "\n",
        "    return denormalized_data\n",
        "###########################################################################"
      ],
      "metadata": {
        "id": "J2HbH2ew3yzL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Hay que modificarlo, fit_transform en Train, y transform en Test\n",
        "\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# def normalize_data(df, normalization=\"MinMax\"):\n",
        "#     \"\"\"\n",
        "#     Normaliza cada serie de tiempo (columna) de manera individual usando MinMax o Zscore.\n",
        "\n",
        "#     Args:\n",
        "#         df (pd.DataFrame): DataFrame con series de tiempo de distintos productos, cada columna es un producto.\n",
        "#         normalization (str): Tipo de normalización a aplicar. Opciones: \"MinMax\" o \"Zscore\". Default es \"MinMax\".\n",
        "\n",
        "#     Returns:\n",
        "#         normalized_df (pd.DataFrame): DataFrame con las series normalizadas.\n",
        "#         normalization_params (pd.DataFrame): DataFrame con los parámetros necesarios para desnormalizar cada columna.\n",
        "#             - Para \"MinMax\": valores min y max de cada columna.\n",
        "#             - Para \"Zscore\": valores mean y std de cada columna.\n",
        "#     \"\"\"\n",
        "#     normalization_params = pd.DataFrame(columns=[\"product_id\", \"min\", \"max\", \"mean\", \"std\"])\n",
        "#     normalized_df = pd.DataFrame(index=df.index)\n",
        "\n",
        "#     for column in df.columns:\n",
        "#         if normalization == \"MinMax\":\n",
        "#             scaler = MinMaxScaler()\n",
        "#             normalized_values = scaler.fit_transform(df[[column]]).flatten()\n",
        "#             new_params = pd.DataFrame({\n",
        "#                 \"product_id\": [column],\n",
        "#                 \"min\": [scaler.data_min_[0]],\n",
        "#                 \"max\": [scaler.data_max_[0]],\n",
        "#                 \"mean\": [None],\n",
        "#                 \"std\": [None]\n",
        "#             })\n",
        "#             normalization_params = pd.concat([normalization_params, new_params], ignore_index=True)\n",
        "#             normalized_df[column] = normalized_values\n",
        "\n",
        "#         elif normalization == \"ZScore\":\n",
        "#             scaler = StandardScaler()\n",
        "#             normalized_values = scaler.fit_transform(df[[column]]).flatten()\n",
        "#             new_params = pd.DataFrame({\n",
        "#                 \"product_id\": [column],\n",
        "#                 \"min\": [None],\n",
        "#                 \"max\": [None],\n",
        "#                 \"mean\": [scaler.mean_[0]],\n",
        "#                 \"std\": [scaler.scale_[0]]\n",
        "#             })\n",
        "#             normalization_params = pd.concat([normalization_params, new_params], ignore_index=True)\n",
        "#             normalized_df[column] = normalized_values\n",
        "\n",
        "#         else:\n",
        "#             raise ValueError(\"Invalid normalization method. Choose 'MinMax' or 'ZScore'.\")\n",
        "\n",
        "#     return normalized_df, normalization_params\n",
        "\n",
        "# def denormalize_series(normalized_series, normalization_params, normalization=\"MinMax\"):\n",
        "#     \"\"\"\n",
        "#     Desnormaliza una serie de tiempo usando los valores almacenados.\n",
        "\n",
        "#     Args:\n",
        "#         normalized_series (pd.Series or pd.DataFrame): Serie o DataFrame con los datos normalizados.\n",
        "#         normalization_params (pd.DataFrame): DataFrame con los parámetros necesarios para desnormalizar cada serie o columna.\n",
        "#             - Para \"MinMax\": valores min y max de cada serie o columna.\n",
        "#             - Para \"Zscore\": valores mean y std de cada serie o columna.\n",
        "#         normalization (str): Tipo de normalización a deshacer. Opciones: \"MinMax\" o \"Zscore\". Default es \"MinMax\".\n",
        "\n",
        "#     Returns:\n",
        "#         denormalized_series (pd.Series or pd.DataFrame): Serie o DataFrame con los datos desnormalizados.\n",
        "#     \"\"\"\n",
        "#     if isinstance(normalized_series, pd.DataFrame):\n",
        "#         denormalized_df = pd.DataFrame(index=normalized_series.index)\n",
        "#         for column in normalized_series.columns:\n",
        "#             params = normalization_params[normalization_params[\"product_id\"] == column]\n",
        "#             if normalization == \"MinMax\":\n",
        "#                 min_value = params[\"min\"].values[0]\n",
        "#                 max_value = params[\"max\"].values[0]\n",
        "#                 denormalized_values = normalized_series[column] * (max_value - min_value) + min_value\n",
        "#             elif normalization == \"ZScore\":\n",
        "#                 mean_value = params[\"mean\"].values[0]\n",
        "#                 std_value = params[\"std\"].values[0]\n",
        "#                 denormalized_values = normalized_series[column] * std_value + mean_value\n",
        "#             else:\n",
        "#                 raise ValueError(\"Invalid normalization method. Choose 'MinMax' or 'ZScore'.\")\n",
        "#             denormalized_df[column] = denormalized_values\n",
        "#         return denormalized_df\n",
        "#     elif isinstance(normalized_series, pd.Series):\n",
        "#         product_ids = normalized_series.index\n",
        "#         denormalized_values = []\n",
        "#         for product_id in product_ids:\n",
        "#             params = normalization_params[normalization_params[\"product_id\"] == product_id]\n",
        "#             if normalization == \"MinMax\":\n",
        "#                 min_value = params[\"min\"].values[0]\n",
        "#                 max_value = params[\"max\"].values[0]\n",
        "#                 denormalized_value = normalized_series[product_id] * (max_value - min_value) + min_value\n",
        "#             elif normalization == \"ZScore\":\n",
        "#                 mean_value = params[\"mean\"].values[0]\n",
        "#                 std_value = params[\"std\"].values[0]\n",
        "#                 denormalized_value = normalized_series[product_id] * std_value + mean_value\n",
        "#             else:\n",
        "#                 raise ValueError(\"Invalid normalization method. Choose 'MinMax' or 'ZScore'.\")\n",
        "#             denormalized_values.append(denormalized_value)\n",
        "#         denormalized_series = pd.Series(denormalized_values, index=product_ids, name=normalized_series.name)\n",
        "#         return denormalized_series\n",
        "#     else:\n",
        "#         raise TypeError(\"normalized_series should be either a pandas Series or DataFrame\")\n"
      ],
      "metadata": {
        "id": "L5vbt6hpxsNC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data"
      ],
      "metadata": {
        "id": "6z54dcDlIyHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Mientras mas chiquito el window size y batch size, mas datos podemos usar en el entrenamiento\n",
        "###############################################################################\n",
        "def split_data(df, window_size):\n",
        "  if window_size == 12:\n",
        "    df_train = df.loc['2017-01':'2018-10']\n",
        "    df_valid = df.loc['2018-11':'2019-12']\n",
        "  elif window_size == 6:\n",
        "    df_train = df.loc['2017-01':'2019-04']\n",
        "    df_valid = df.loc['2019-05':'2019-12']\n",
        "  else:\n",
        "    raise ValueError(\"window_size must be different\")\n",
        "  return pd.DataFrame(df_train), pd.DataFrame(df_valid)\n",
        "#############################################################################\n",
        "# Hay que seguir probando con otras window (ej 6, 12)\n",
        "# y ver como no pincha el entrenamiento por quedarnos con pocos datos de validacion\n",
        "def split_data_dec2019(df, window_size):\n",
        "  if window_size == 3:\n",
        "    df_train = df.loc['2017-01':'2019-05']\n",
        "    df_valid = df.loc['2019-06':'2019-10']\n",
        "    df_test = df.loc['2019-11':'2019-12']\n",
        "  elif window_size == 4:\n",
        "    df_train = df.loc['2017-01':'2019-04']\n",
        "    df_valid = df.loc['2019-05':'2019-10']\n",
        "    df_test = df.loc['2019-11':'2019-12']\n",
        "  elif window_size == 6:\n",
        "    df_train = df.loc['2017-01':'2019-02']\n",
        "    df_valid = df.loc['2019-03':'2019-10']\n",
        "  else:\n",
        "    raise ValueError(\"window_size must be different\")\n",
        "  return pd.DataFrame(df_train), pd.DataFrame(df_valid)#, pd.DataFrame(df_test)\n",
        "#############################################################################\n",
        "# split mas chico posible, con window 3 y batch 1\n",
        "def split_data_test(df):\n",
        "  df_train = df.loc['2017-01':'2019-07']\n",
        "  df_valid = df.loc['2019-08':'2019-12']\n",
        "  return pd.DataFrame(df_train), pd.DataFrame(df_valid)\n",
        "############################################################################\n",
        "# # No se si este split tiene sentido, pierdo muchos datos de entrenamiento\n",
        "# def split_data_2019(df):\n",
        "#   df_train = df.loc['2017-01':'2018-12']\n",
        "#   df_valid = df.loc['2019-01':'2019-10']\n",
        "#   df_test =  df.loc['2019-11':'2019-12']\n",
        "#   return df_train, df_valid, df_test\n",
        "############################################################################"
      ],
      "metadata": {
        "id": "zZUKfILzC6Un"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Window Data"
      ],
      "metadata": {
        "id": "xbYFP2j6EiEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def windowed_dataset(sequence, data_split, window_size, horizon, batch_size, shuffle_buffer=1000):\n",
        "    \"\"\"Generates dataset windows.\n",
        "\n",
        "    Args:\n",
        "      sequence (array-like): Contains the values of the time series.\n",
        "      data_split (str): Specifies if the dataset is for training or validation/test.\n",
        "      window_size (int): The number of time steps to include in the feature.\n",
        "      horizon (int): The number of future time steps to predict.\n",
        "      batch_size (int): The batch size.\n",
        "      shuffle_buffer (int): Buffer size to use for the shuffle method.\n",
        "\n",
        "    Returns:\n",
        "      tf.data.Dataset: TF Dataset containing time windows.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "\n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size + horizon, shift=1, drop_remainder=True)\n",
        "\n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + horizon))\n",
        "\n",
        "    # Create tuples with features and labels\n",
        "    dataset = dataset.map(lambda window: (window[:-horizon], window[-horizon:]))\n",
        "\n",
        "    if data_split == 'train':\n",
        "        # Shuffle the training data to improve generalization\n",
        "        dataset = dataset.shuffle(shuffle_buffer)\n",
        "    else:\n",
        "        # Cache the validation/test data for improved performance\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "    # Create batches of windows and prefetch for performance\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "OlImIwoE3Mh-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# # Viejos\n",
        "##############################################\n",
        "# def window_dataset(sequence, data_split, window_size, batch_size, n_future, shuffle_buffer=1000, seed=None):\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "#     dataset = dataset.window(window_size + n_future, shift=1, drop_remainder=True)\n",
        "#     dataset = dataset.flat_map(lambda window: window.batch(window_size + n_future))\n",
        "#     dataset = dataset.map(lambda window: (window[:window_size], window[window_size:]))\n",
        "\n",
        "#     if data_split == 'train':\n",
        "#         dataset = dataset.shuffle(shuffle_buffer, seed=seed)\n",
        "#     else:\n",
        "#         dataset = dataset.cache()\n",
        "\n",
        "#     dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#     return dataset\n",
        "##############################################\n",
        "# def window_dataset(sequence, data_split, window_size, batch_size, n_future, shuffle_buffer=1000, seed=None):\n",
        "#     \"\"\"Generates dataset windows for multi-step forecasting in a multivariable context.\n",
        "\n",
        "#     Args:\n",
        "#       sequence (array-like): Contains the values of the time series, where each element is an array of feature values.\n",
        "#       data_split (str): Specifies if the dataset is for training or validation/test.\n",
        "#       window_size (int): The number of time steps to include in the feature.\n",
        "#       batch_size (int): The batch size.\n",
        "#       n_future (int): The number of future steps to predict.\n",
        "#       shuffle_buffer (int): Buffer size to use for the shuffle method.\n",
        "#       seed (int, optional): Random seed for reproducibility.\n",
        "\n",
        "#     Returns:\n",
        "#       tf.data.Dataset: TF Dataset containing time windows.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Generate a TF Dataset from the series values\n",
        "#     dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "\n",
        "#     # Window the data but only take those with the specified size\n",
        "#     dataset = dataset.window(window_size + n_future, shift=1, drop_remainder=True)\n",
        "\n",
        "#     # Flatten the windows by putting its elements in a single batch\n",
        "#     dataset = dataset.flat_map(lambda window: window.batch(window_size + n_future))\n",
        "\n",
        "#     # Create tuples with features and labels\n",
        "#     dataset = dataset.map(lambda window: (window[:window_size], window[window_size:]))\n",
        "\n",
        "#     if data_split == 'train':\n",
        "#         # Shuffle the training data to improve generalization\n",
        "#         dataset = dataset.shuffle(shuffle_buffer, seed=seed)\n",
        "#     else:\n",
        "#         # Cache the validation/test data for improved performance\n",
        "#         dataset = dataset.cache()\n",
        "\n",
        "#     # Create batches of windows and prefetch for performance\n",
        "#     dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#     return dataset\n"
      ],
      "metadata": {
        "id": "c5g4WjakEjcM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Functions"
      ],
      "metadata": {
        "id": "x_Zm-EgneATh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################################\n",
        "# # Este va a reemplazar al otro\n",
        "def generate_predictions_2(model_trained, data_valid_norm, data_norm_params):\n",
        "  data_norm_array = data_valid_norm.values\n",
        "  column_names = data_valid_norm.columns\n",
        "  input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "  pred = model_trained.predict(input_data)\n",
        "  pred = pred.reshape((1, horizon, n_features))\n",
        "  pred_df = pd.DataFrame(pred[0], columns=column_names)\n",
        "  pred_df.index = pd.date_range(start=data_valid_norm.index[-1] + pd.DateOffset(months=1), periods=horizon, freq='MS')\n",
        "\n",
        "  # Generamos la salida de la primer prediccion, Enero 2020 (Mes +1 del ultimo mes en data_validation_norm)\n",
        "  pred_plus1 = pred_df.iloc[0]\n",
        "  pred_plus1_denorm = denormalize_data(pred_plus1, data_norm_params, normalization=normalization)\n",
        "  pred_plus1_denorm = pd.Series(pred_plus1_denorm, name=str(column_names.values[0]))\n",
        "  pred_plus1_df = pd.DataFrame(pred_plus1_denorm)\n",
        "  pred_plus1_df['periodo'] = pd.to_datetime(pred_df.iloc[0].name)\n",
        "  pred_plus1_df.set_index('periodo', inplace=True)\n",
        "\n",
        "  # Generamos la salida de la segunda prediccion, Febrero 2020 (Mes +2 del ultimo mes en data_validation_norm)\n",
        "  pred_plus2 = pred_df.iloc[1]\n",
        "  pred_plus2_denorm = denormalize_data(pred_plus2, data_norm_params, normalization=normalization)\n",
        "  pred_plus2_denorm = pd.Series(pred_plus2_denorm, name=str(column_names.values[0]))\n",
        "  pred_plus2_df = pd.DataFrame(pred_plus2_denorm)\n",
        "  pred_plus2_df['periodo'] = pd.to_datetime(pred_df.iloc[1].name)\n",
        "  pred_plus2_df.set_index('periodo', inplace=True)\n",
        "\n",
        "  # Concateno las dos predicciones\n",
        "  pred_final_df = pd.concat([pred_plus1_df,pred_plus2_df])\n",
        "\n",
        "  # En lugar de DF, devuelvo una serie, para que matchee con el input\n",
        "  pred_final_serie = pred_final_df.squeeze()\n",
        "\n",
        "  return pred_final_serie\n",
        "##########################################################################\n",
        "def generate_predictions(data_norm, data_norm_params, export_csv):\n",
        "  data_norm_array = data_norm.values\n",
        "  column_names = data_norm.columns\n",
        "  input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "  pred = model.predict(input_data)\n",
        "  pred = pred.reshape((1, horizon, n_features))\n",
        "  pred_df = pd.DataFrame(pred[0], columns=column_names)\n",
        "  pred_df.index = pd.date_range(start='2020-01-01', periods=horizon, freq='MS')\n",
        "  pred_feb = pred_df.loc['2020-02-01']\n",
        "  pred_denorm = denormalize_data(pred_feb, norm_params, normalization)\n",
        "  pred_denorm = pred_denorm.reset_index()\n",
        "  pred_denorm.columns = ['product_id', 'tn']\n",
        "  # display(pred_denorm)\n",
        "  # # Esto no creo que sea necesario\n",
        "  # predicciones = filter_data(data_pred1_denorm, data_productos_a_predecir)\n",
        "\n",
        "  filename = f\"{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "\n",
        "  if export_csv:\n",
        "    pred_denorm.to_csv(filename, header=True, index=False)\n",
        "\n",
        "  # print(filename)\n",
        "\n",
        "  return pred_denorm['tn']\n",
        "##########################################################################\n",
        "def to_kaggle(serie, name='kaggle_submission'):\n",
        "  serie.columns = ['product_id', 'tn']\n",
        "  serie.to_csv(f'{name}.csv', header=True, index=False)\n",
        "##########################################################################\n",
        "def sumar_predicciones(df1, df2):\n",
        "    # Asegúrate de que las columnas necesarias estén en los DataFrames\n",
        "    if 'product_id' not in df1.columns or 'tn' not in df1.columns:\n",
        "        raise ValueError(\"df1 debe contener las columnas 'product_id' y 'tn'\")\n",
        "    if 'product_id' not in df2.columns or 'tn' not in df2.columns:\n",
        "        raise ValueError(\"df2 debe contener las columnas 'product_id' y 'tn'\")\n",
        "\n",
        "    # Suma los valores de 'tn' para cada 'product_id' de ambos DataFrames\n",
        "    result = df1.set_index('product_id').add(df2.set_index('product_id'), fill_value=0).reset_index()\n",
        "\n",
        "    return result\n",
        "# ##########################################################################\n",
        "# # Boostea los productos de Health Care cuya predicciones tienen mucho error\n",
        "# def HC_boost(predicciones_serie, boost=-0.5):\n",
        "#     boost_mask = ['20006', '20007', '20008', '20009', '20010', '20012', '20014', '20015']\n",
        "\n",
        "#     # Convertir el índice a string si no lo es\n",
        "#     predicciones_serie.index = predicciones_serie.index.astype(str)\n",
        "\n",
        "#     # Aplicar el boost a los valores en la máscara\n",
        "#     predicciones_serie.loc[boost_mask] = predicciones_serie.loc[boost_mask] * (1 + boost)\n",
        "\n",
        "#     return predicciones_serie\n",
        "# ##########################################################################"
      ],
      "metadata": {
        "id": "jmLuXgOGeC57"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ##########################################################################\n",
        "# def generate_predictions(data_norm, data_norm_params, export_csv): # Verificar\n",
        "# # def generate_predictions(export_csv): # Viejo\n",
        "#   data_norm_array = data_norm.values\n",
        "#   column_names = data_norm.columns\n",
        "#   input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "#   pred = model.predict(input_data)\n",
        "#   pred = pred.reshape((1, horizon, n_features))\n",
        "#   pred_df = pd.DataFrame(pred[0], columns=column_names)\n",
        "#   pred_df.index = pd.date_range(start='2020-01-01', periods=horizon, freq='MS')\n",
        "#   pred_feb = pred_df.loc['2020-02-01']\n",
        "#   pred_1_denorm = denormalize_series(pred_feb, data_norm_params, normalization=normalization)\n",
        "#   data_pred1_denorm = pred_1_denorm.reset_index()\n",
        "#   data_pred1_denorm.columns = ['product_id', 'tn']\n",
        "#   # Esto no creo que sea necesario\n",
        "#   predicciones = filter_data(data_pred1_denorm, data_productos_a_predecir)\n",
        "\n",
        "#   # Ojo con esto, caja negra. Vuelve a predecir usando lo predicho antes.\n",
        "#   input_data2 = np.append(input_data[:, 1:, :], pred[:, 0, :].reshape(1, 1, n_features), axis=1)\n",
        "#   pred2 = model.predict(input_data2)\n",
        "#   pred2 = pred2.reshape((1, horizon, n_features))\n",
        "#   pred2_df = pd.DataFrame(pred2[0], columns=column_names)\n",
        "#   pred2_df.index = pd.date_range(start='2020-02-01', periods=horizon, freq='MS')\n",
        "#   pred2_feb = pred2_df.loc['2020-02-01']\n",
        "#   pred_2_denorm = denormalize_series(pred2_feb, data_norm_params, normalization=normalization)\n",
        "#   data_pred2_denorm = pred_2_denorm.reset_index()\n",
        "#   data_pred2_denorm.columns = ['product_id', 'tn']\n",
        "#   predicciones2 = filter_data(data_pred2_denorm, data_productos_a_predecir)\n",
        "\n",
        "#   filename = f\"{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "#   filename2 = f\"RECURRENTE_{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "\n",
        "#   if export_csv:\n",
        "#     predicciones.to_csv(filename, header=True, index=False)\n",
        "\n",
        "#   # predicciones.to_csv(filename, header=True, index=False)\n",
        "#   # predicciones2.to_csv(filename2, header=True, index=False)\n",
        "#   print(filename)\n",
        "\n",
        "#   return predicciones # Probar con predicciones 2"
      ],
      "metadata": {
        "id": "KWsm0gdSDZ9w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "### OLD\n",
        "###############################################################################\n",
        "# def generate_predictions(data_norm, data_norm_params):\n",
        "\n",
        "#     # Convertir el DataFrame a un array de NumPy\n",
        "#     data_norm_array = data_norm.values\n",
        "\n",
        "#     # Extraer la última ventana de datos de 2019 para predecir enero de 2020\n",
        "#     column_names = data_norm.columns  # Obtener los nombres de las columnas\n",
        "\n",
        "#     # Extraer los últimos `window_size` meses de 2019\n",
        "#     input_data = data_norm_array[-window_size:].reshape((1, window_size, n_features))\n",
        "\n",
        "#     # Predecir enero de 2020\n",
        "#     pred_january = model.predict(input_data)\n",
        "\n",
        "#     # Asegurarse de que la predicción tenga la forma correcta\n",
        "#     pred_january = pred_january.reshape((1, n_future, n_features))\n",
        "\n",
        "#     # Crear un DataFrame para la predicción de enero de 2020\n",
        "#     pred_january_df = pd.DataFrame(pred_january[0], columns=column_names)\n",
        "#     pred_january_df.index = pd.date_range(start='2020-01-01', periods=n_future, freq='MS')\n",
        "\n",
        "#     # Actualizar la ventana de entrada para predecir febrero de 2020\n",
        "#     input_data = np.append(input_data[:, 1:, :], pred_january[:, 0, :].reshape(1, 1, n_features), axis=1)\n",
        "\n",
        "#     # Predecir febrero de 2020\n",
        "#     pred_february = model.predict(input_data)\n",
        "\n",
        "#     # Asegurarse de que la predicción tenga la forma correcta\n",
        "#     pred_february = pred_february.reshape((1, n_future, n_features))\n",
        "\n",
        "#     # Crear un DataFrame para la predicción de febrero de 2020\n",
        "#     pred_february_df = pd.DataFrame(pred_february[0], columns=column_names)\n",
        "#     pred_february_df.index = pd.date_range(start='2020-02-01', periods=n_future, freq='MS')\n",
        "\n",
        "#     # Obtener la predicción de febrero de 2020\n",
        "#     pred_1 = pred_january_df.loc['2020-02-01']\n",
        "\n",
        "#     # Desnormalizar la predicción\n",
        "#     pred_1_denorm = denormalize_series(pred_1, data_norm_params, normalization=normalization)\n",
        "#     data_pred1_denorm = pred_1_denorm.reset_index()\n",
        "#     data_pred1_denorm.columns = ['product_id', 'tn']\n",
        "#     predicciones = filter_data(data_pred1_denorm, data_productos_a_predecir)\n",
        "\n",
        "#     # Crear el nombre del archivo\n",
        "#     filename = f\"{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}_epochs{epochs}.csv\"\n",
        "#     predicciones.to_csv(filename, header=True, index=False)\n",
        "\n",
        "#     print(filename)\n",
        "\n",
        "#     return predicciones\n",
        "##########################################################################"
      ],
      "metadata": {
        "id": "G1YhW2wUNh6O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Sales"
      ],
      "metadata": {
        "id": "Tsi7tkBEgtWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_sales(df1, df2):\n",
        "    # Crear un rango de fechas desde enero 2017 hasta diciembre 2019\n",
        "    fechas_completas = pd.date_range(start='2017-01-01', end='2019-12-01', freq='MS')\n",
        "\n",
        "    # Reindexar el DataFrame para asegurar que todas las fechas estén presentes\n",
        "    df1 = df1.reindex(fechas_completas, fill_value=pd.NA)\n",
        "\n",
        "    # Obtener los product_id del primer DataFrame (nombres de las columnas)\n",
        "    product_ids_df1 = df1.columns.tolist()\n",
        "\n",
        "    # Obtener los product_id del segundo DataFrame (valores en la columna 'product_id')\n",
        "    product_ids_df2 = df2['product_id'].tolist()\n",
        "\n",
        "    # Identificar los product_id que faltan en df1\n",
        "    product_ids_faltantes = [pid for pid in product_ids_df2 if pid not in product_ids_df1]\n",
        "\n",
        "    # Crear un DataFrame con las columnas faltantes y valores NaN\n",
        "    df_faltantes = pd.DataFrame(index=df1.index, columns=product_ids_faltantes)\n",
        "\n",
        "    # Concatenar el DataFrame original con el DataFrame de faltantes\n",
        "    df_resultante = pd.concat([df1, df_faltantes], axis=1)\n",
        "\n",
        "    return df_resultante\n",
        "\n"
      ],
      "metadata": {
        "id": "aSHUuvHXgtf8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks"
      ],
      "metadata": {
        "id": "brfkj2XkXena"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def MyCallbacks(patience):\n",
        "    \"\"\"\n",
        "    Devuelve una lista de callbacks para el entrenamiento del modelo.\n",
        "\n",
        "    Parameters:\n",
        "    patience (int): Número de épocas a esperar para ver una mejora en 'val_loss' antes de detener el entrenamiento.\n",
        "\n",
        "    Returns:\n",
        "    list: Lista de callbacks de Keras.\n",
        "    \"\"\"\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "    return [early_stop]"
      ],
      "metadata": {
        "id": "fcfAKyAS_iD2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "#### VIEJO\n",
        "# #############################################################################\n",
        "# class MAEThresholdCallback(Callback):\n",
        "#     def __init__(self, threshold=0.15):\n",
        "#         super(MAEThresholdCallback, self).__init__()\n",
        "#         self.threshold = threshold\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "#         val_mae = logs.get('val_mae')\n",
        "#         if val_mae is not None and val_mae <= self.threshold:\n",
        "#             print(f'\\nEpoch {epoch+1}: Validation MAE has reached {val_mae:.4f}, stopping training.')\n",
        "#             self.model.stop_training = True\n",
        "\n",
        "# def MyCallbacks(model_name, patience):\n",
        "#     earlystop = tf.keras.callbacks.EarlyStopping('val_loss', patience=patience, restore_best_weights=True)\n",
        "#     # checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=f'ckpts/{model_name}-' + '{epoch:02d}-{val_loss:.4f}.h5', monitor='val_loss')\n",
        "#     # mae_threshold_callback = MAEThresholdCallback(threshold=0.015)\n",
        "#     return [earlystop] #, checkpoint] #, mae_threshold_callback]\n",
        "\n",
        "# #############################################################################"
      ],
      "metadata": {
        "id": "ETzh0JyBXgRt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Design"
      ],
      "metadata": {
        "id": "cGAz7W4mXqO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "def compile_model(new_model, loss, optimizer):\n",
        "    new_model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
        "    print(new_model.summary())\n",
        "    return new_model\n",
        "#############################################################################\n",
        "def MyModel(loss, optimizer, window_size, horizon, n_features):\n",
        "    new_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer((window_size, n_features)),\n",
        "        tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='causal'),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=False)),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(n_features * horizon, activation='relu'),\n",
        "        tf.keras.layers.Reshape((horizon, n_features)),\n",
        "    ])\n",
        "    return compile_model(new_model, loss, optimizer)\n",
        "#############################################################################"
      ],
      "metadata": {
        "id": "v1XojStQ3FQw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################################################\n",
        "# def compile_model(new_model, loss, optimizer):\n",
        "#   new_model.compile(optimizer=optimizer, loss=loss, metrics=['mse'])\n",
        "#   print(new_model.summary())\n",
        "#   return new_model\n",
        "# #############################################################################\n",
        "# def MyModel(loss, optimizer, window_size, n_future, n_features):\n",
        "#     new_model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.InputLayer((window_size, n_features)),\n",
        "#         tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='causal'),\n",
        "#         tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "#         tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "#         tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=False)),\n",
        "#         tf.keras.layers.Dropout(0.4),\n",
        "#         tf.keras.layers.Dense(n_features * n_future, activation='relu'),\n",
        "#         tf.keras.layers.Reshape((n_future, n_features)),\n",
        "#         ])\n",
        "#     return compile_model(new_model, loss, optimizer)"
      ],
      "metadata": {
        "id": "eCESYECOXr45"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Train"
      ],
      "metadata": {
        "id": "7uqfuDfTJD8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repetimos la prediccion n veces\n",
        "def model_train(epochs, iteraciones):\n",
        "  pred_list = []\n",
        "\n",
        "  for i in range(iteraciones):\n",
        "    print(f'Interacion {i+1}')\n",
        "    history = model.fit(\n",
        "        data_train_windowed,\n",
        "        validation_data = data_valid_windowed,\n",
        "        callbacks = callbacks,\n",
        "        verbose=0,\n",
        "        epochs=epochs)\n",
        "\n",
        "    predicted = generate_predictions(data_valid_norm, norm_params, False)\n",
        "    print(f'Prediction: {predicted}')\n",
        "    pred_list.append(generate_predictions(data_valid_norm, norm_params, False))\n",
        "\n",
        "    plot_history(history)\n",
        "  print('Producto: ', producto)\n",
        "  print(f'Mean Loss across all splits: {np.mean(pred_list)}')\n",
        "  print(f'Median Loss across all splits: {np.median(pred_list)}')\n",
        "\n",
        "  return(np.mean(pred_list), np.median(pred_list))\n",
        "\n"
      ],
      "metadata": {
        "id": "DSrrGEGPJFyO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines"
      ],
      "metadata": {
        "id": "F4yiWWu8FJZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "HWg00hIFX64c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Crear una serie de tiempo de ejemplo con valores nulos\n",
        "dates = pd.date_range('2023-01-01', periods=10, freq='D')\n",
        "data = {'value': [1, 2, np.nan, 4, 5, np.nan, 7, 8, np.nan, 10]}\n",
        "df = pd.DataFrame(data, index=dates)\n",
        "\n",
        "# Mostrar la serie original\n",
        "print(\"Serie original con valores nulos:\")\n",
        "print(df)\n",
        "\n",
        "# Interpolación lineal\n",
        "df_interpolated = df.interpolate(method='linear')\n",
        "\n",
        "# Mostrar la serie con interpolación\n",
        "print(\"\\nSerie con interpolación lineal:\")\n",
        "print(df_interpolated)\n",
        "\n",
        "# Relleno hacia adelante (forward fill)\n",
        "df_ffill = df.fillna(method='ffill')\n",
        "\n",
        "# Mostrar la serie con relleno hacia adelante\n",
        "print(\"\\nSerie con relleno hacia adelante:\")\n",
        "print(df_ffill)\n",
        "\n",
        "# Relleno hacia atrás (backward fill)\n",
        "df_bfill = df.fillna(method='bfill')\n",
        "\n",
        "# Mostrar la serie con relleno hacia atrás\n",
        "print(\"\\nSerie con relleno hacia atrás:\")\n",
        "print(df_bfill)\n",
        "\n",
        "# Relleno con la media móvil\n",
        "window_size = 3\n",
        "df['value'] = df['value'].fillna(df['value'].rolling(window=window_size, min_periods=1).mean())\n",
        "\n",
        "# Mostrar la serie con relleno de media móvil\n",
        "print(\"\\nSerie con relleno de media móvil:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "H8jePTIq8SrK",
        "outputId": "46ed6cc3-e176-4d20-832f-a3d664e03245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serie original con valores nulos:\n",
            "            value\n",
            "2023-01-01   1.00\n",
            "2023-01-02   2.00\n",
            "2023-01-03    NaN\n",
            "2023-01-04   4.00\n",
            "2023-01-05   5.00\n",
            "2023-01-06    NaN\n",
            "2023-01-07   7.00\n",
            "2023-01-08   8.00\n",
            "2023-01-09    NaN\n",
            "2023-01-10  10.00\n",
            "\n",
            "Serie con interpolación lineal:\n",
            "            value\n",
            "2023-01-01   1.00\n",
            "2023-01-02   2.00\n",
            "2023-01-03   3.00\n",
            "2023-01-04   4.00\n",
            "2023-01-05   5.00\n",
            "2023-01-06   6.00\n",
            "2023-01-07   7.00\n",
            "2023-01-08   8.00\n",
            "2023-01-09   9.00\n",
            "2023-01-10  10.00\n",
            "\n",
            "Serie con relleno hacia adelante:\n",
            "            value\n",
            "2023-01-01   1.00\n",
            "2023-01-02   2.00\n",
            "2023-01-03   2.00\n",
            "2023-01-04   4.00\n",
            "2023-01-05   5.00\n",
            "2023-01-06   5.00\n",
            "2023-01-07   7.00\n",
            "2023-01-08   8.00\n",
            "2023-01-09   8.00\n",
            "2023-01-10  10.00\n",
            "\n",
            "Serie con relleno hacia atrás:\n",
            "            value\n",
            "2023-01-01   1.00\n",
            "2023-01-02   2.00\n",
            "2023-01-03   4.00\n",
            "2023-01-04   4.00\n",
            "2023-01-05   5.00\n",
            "2023-01-06   7.00\n",
            "2023-01-07   7.00\n",
            "2023-01-08   8.00\n",
            "2023-01-09  10.00\n",
            "2023-01-10  10.00\n",
            "\n",
            "Serie con relleno de media móvil:\n",
            "            value\n",
            "2023-01-01   1.00\n",
            "2023-01-02   2.00\n",
            "2023-01-03   1.50\n",
            "2023-01-04   4.00\n",
            "2023-01-05   5.00\n",
            "2023-01-06   4.50\n",
            "2023-01-07   7.00\n",
            "2023-01-08   8.00\n",
            "2023-01-09   7.50\n",
            "2023-01-10  10.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # data\n",
        "# # data_productos\n",
        "# # data_stocks\n",
        "# # data_productos_a_predecir\n",
        "\n",
        "# Pre-Processing Variables\n",
        "split_strategy = 'S1'\n",
        "normalization = 'MinMax'\n",
        "window_size = None\n",
        "horizon = 2\n",
        "batch_size = None\n",
        "\n",
        "\n",
        "# Model Variables: Dentro de cada Experimento, no son generales\n",
        "n_features = None  # Esto va a depender de cada modelo, es el data_train.shape[1]\n",
        "n_splits = None # No mas, la usabamos con el TimeSeriesSplit\n",
        "# model_name = 'CAT1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 10\n",
        "\n",
        "# Si quiero solo las ventas de los clientes activos\n",
        "data = filter_active_clients_data(data)\n",
        "\n",
        "# Si quiero las ventas solo de los productos a predecir\n",
        "data = filter_data(data, data_productos_a_predecir.reset_index())\n",
        "\n",
        "# Matriz reducida, agrupadas las ventas por mes y producto\n",
        "data_grouped = group_data(data, 'product_id')\n",
        "\n",
        "# Estrategia para completar los NaN de los datos agrupados\n",
        "data_grouped = fill_nulls(data_grouped)\n",
        "\n",
        "# Estrategia para solucionar el problema de Agosto 2019\n",
        "data_grouped = fix_aug2019(data_grouped, 'julplus10')\n",
        "\n",
        "# Ploteamos para ver como quedan las ventas totales acumuladas\n",
        "data_grouped.sum(axis=1).plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vLrQTz5YEkPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "24c77aab-7f34-4870-9271-7ddb924249bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimentos Individuales"
      ],
      "metadata": {
        "id": "SaiJuaUP3JUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/18 - Pruebas contra Diciembre 2019"
      ],
      "metadata": {
        "id": "-ORvfte2nIee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/17 - Ventas por dia"
      ],
      "metadata": {
        "id": "1HbokWwRHZEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data = data_bkp.copy()\n",
        "# data.sort_values(by=['periodo', 'customer_id', 'product_id'])\n",
        "# data = data[['customer_id', 'product_id', 'tn']]\n",
        "# data = data.merge(data_productos_a_predecir_con_categorias, left_on='product_id', right_on='product_id', how='left')\n",
        "# data.set_index(data_bkp.index, inplace=True)\n",
        "# data.sort_index(inplace=True)\n",
        "# data.sort_values(by=['periodo', 'customer_id', 'product_id'], inplace=True)\n",
        "\n",
        "\n",
        "# # # Vemos los nulos por variable\n",
        "# # print(data_merged.isnull().sum())\n",
        "\n",
        "\n",
        "# mask_cat1_null = data['cat1'].isnull()\n",
        "# # display(data_merged[mask_cat1_null])\n",
        "\n",
        "# # # Ninguno de estos productos nos interesa, no estan en nuestas categorias\n",
        "# # print(data_productos_a_predecir_con_categorias.index.isin(data_merged[mask_cat1_null]['product_id']).sum())\n",
        "# # print(data_merged.shape[0])\n",
        "\n",
        "# # Elimino los nulos\n",
        "# data.dropna(subset=['cat1'], inplace=True)\n",
        "# # print(\"Nulos:\", data.isnull().sum())\n",
        "# # print(data.shape)\n",
        "\n",
        "\n",
        "# # Contar las ocurrencias de cada combinación de periodo, customer_id y product_id\n",
        "# repeated_entries = data.groupby(['periodo', 'customer_id', 'product_id']).size()\n",
        "\n",
        "# # Filtrar combinaciones que aparecen más de una vez\n",
        "# repeated_entries = repeated_entries[repeated_entries > 1]\n",
        "\n",
        "# # Mostrar las combinaciones repetidas\n",
        "# print(repeated_entries)\n",
        "\n",
        "# # sin combinaciones repetidas\n",
        "\n",
        "# print(data.shape)\n",
        "# display(data.head())"
      ],
      "metadata": {
        "id": "N6XR0zgQTFJ3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ############################################################################################################\n",
        "# # # Verificamos los clientes Activos\n",
        "# ############################################################################################################\n",
        "# # Generamos la matriz de presencia\n",
        "# presencia_clientes = data.groupby(['periodo', 'customer_id']).size().unstack(fill_value=0)\n",
        "# presencia_clientes[presencia_clientes > 0] = 1  # Convertimos las cantidades en 1 para indicar presencia\n",
        "# mask_active_clients = presencia_clientes.loc['2019-10':'2019-12'].sum(axis=0) == 3\n",
        "\n",
        "# # Configuramos el tamaño de la figura\n",
        "# plt.figure(figsize=(15, 10))\n",
        "\n",
        "# # Creamos el heatmap\n",
        "# sns.heatmap(presencia_clientes, cmap='viridis', cbar=False, linewidths=.5)\n",
        "\n",
        "# # Añadimos los títulos y etiquetas\n",
        "# plt.title('Presencia de Clientes por Mes')\n",
        "# plt.xlabel('Clientes')\n",
        "# plt.ylabel('Mes')\n",
        "\n",
        "# # Mostramos el gráfico\n",
        "# plt.show()\n",
        "# ############################################################################################################\n",
        "\n",
        "# # Verificamos que el cliente 10039 no compro en el ultimo semestre, como algunos otros.\n",
        "# # Maybe podemos removerlos del experimento, ya que sabemos que no van a volver a comprar\n",
        "# display(data.groupby(['customer_id', 'periodo']).count()[['product_id']].loc[['10039']])\n",
        "\n",
        "\n",
        "# presencia_clientes = data.groupby(['periodo', 'customer_id']).size().unstack(fill_value=0)\n",
        "# presencia_clientes[presencia_clientes > 0] = 1\n",
        "\n",
        "# # Vemos 173 clientes, no compraron en diciembre\n",
        "# print(presencia_clientes.loc['2019-12'].sum(axis=0).value_counts())\n",
        "\n",
        "\n",
        "# # Y que de esos 173, 111 tampoco compraron en noviembre\n",
        "# print(presencia_clientes.loc['2019-11':'2019-12'].sum(axis=0).value_counts())\n",
        "\n",
        "# # y de esos 111, 98 tampoco compraron en Octubre\n",
        "# print(presencia_clientes.loc['2019-10':'2019-12'].sum(axis=0).value_counts())\n",
        "\n",
        "# Concluimos que los 111 clientes que no nos compraron en Octibre, Noviembre ni Diciembre\n",
        "# no son mas nuestros clietnes, y damos de baja sus ventas, ya que es muy probable\n",
        "# que no nos vuelvan a comprar, aunque si un porcentje sera absosrvido por la competencia\n",
        "\n",
        "\n",
        "# print(mask_active_clients.shape)\n",
        "# display(mask_active_clients.head())"
      ],
      "metadata": {
        "id": "P0D6gjK_54l6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ####################################################################################################################\n",
        "# # 2024-06-07: Excluyo las ventas de los clientes marcados como desactivados\n",
        "# # Ventas de clientes que no compraron en los ultimos dos meses, eliminadas\n",
        "# active_data = data[data['customer_id'].isin(mask_active_clients[mask_active_clients].index)]\n",
        "\n",
        "# # Pivotear los datos\n",
        "# data_grouped_active_clients = active_data.pivot_table(\n",
        "#     index='periodo',\n",
        "#     columns='product_id',\n",
        "#     values='tn',\n",
        "#     aggfunc='sum'\n",
        "# )\n",
        "\n",
        "# # Asegurarnos de que el índice esté en el formato correcto\n",
        "# data_grouped_active_clients.index = pd.to_datetime(data_grouped_active_clients.index)\n",
        "\n",
        "# # Ordenar el índice si fuera necesario\n",
        "# data_grouped_active_clients = data_grouped_active_clients.sort_index()\n",
        "# ####################################################################################################################\n",
        "# # Si quiero usar el Dataset con las ventas de los clientes desactivados excluidas\n",
        "# # Y trasca le completo los nulos\n",
        "# data_grouped = fill_nulls(data_grouped_active_clients)\n",
        "\n",
        "# # Relacion entre el promedio anual anterior, y el febrero siguiente\n",
        "# data_2017_mean = pd.Series(data_grouped.loc['2017'].mean(axis=0), name='2017_mean')\n",
        "# data_2018_mean = pd.Series(data_grouped.loc['2018'].mean(axis=0), name='2018_mean')\n",
        "# data_2019_mean = pd.Series(data_grouped.loc['2019'].mean(axis=0), name='2019_mean')\n",
        "# feb_2018 = pd.Series(data_grouped.loc['2018-02'].T.squeeze(), name='Feb_2018')\n",
        "# feb_2019 = pd.Series(data_grouped.loc['2019-02'].T.squeeze(), name='Feb_2019')\n",
        "\n",
        "# data = pd.concat([data_2017_mean, feb_2018, data_2018_mean, feb_2019, data_2019_mean], axis=1)\n",
        "\n",
        "# # display(data.head())\n",
        "\n",
        "# # Calculamos cuanto represetan las ventas de Febrero, con respecto al Mean de todo el ano anterior\n",
        "# ratio_2017 = pd.Series(data['Feb_2018'] / data['2017_mean'], name='Ratio_2017')\n",
        "# ratio_2018 = pd.Series(data['Feb_2019'] / data['2018_mean'], name='Ratio_2018')\n",
        "# data_ratios = pd.concat([ratio_2017, ratio_2018], axis=1)\n",
        "\n",
        "# # Nos enfocamos solo en los primeros 40 productos, los mas importantes\n",
        "# data_ratios[:40].plot(figsize=(25, 6))\n",
        "# plt.show()\n",
        "\n",
        "# # display(data_ratios.head())\n",
        "# # Vemos el huevo del producto #32, ya que no tenemos ventas todos los meses\n",
        "# # Vemos como son bastantes parecidas\n",
        "# # Se ve que el producto 19 fue suplementado por el 22 en las ventas del 2018\n",
        "\n",
        "\n",
        "# # Si el ratio del producto vendido entre el promedo del Ano y Febrero del ano siguiente es menor al 3%, sigue manteniendo ese ratio\n",
        "# data_ratios['diff3'] = (abs(data_ratios['Ratio_2017'] - data_ratios['Ratio_2018']) <= 0.03)\n",
        "# data_final = pd.concat([data_ratios, data_2019_mean], axis=1)\n",
        "\n",
        "# # Inflation Multiplier, dejar en 1 para que no modifique las predicciones\n",
        "# inflation = .92\n",
        "# data_final['tn'] = data_final.apply(lambda row: row['2019_mean'] * row['Ratio_2018'] * inflation if row['diff3'] else row['2019_mean'], axis=1)\n",
        "\n",
        "# # display(data_final.head())\n",
        "\n",
        "# # Formateo y exporto a Kaggle.\n",
        "# to_kaggle(data_final['tn'].reset_index(), name='modelo_loco_3diff_inlf92_active3')\n",
        "\n",
        "# # Guardo los productos que tengo que mejorar, los que los ratio de ventas con Febrero sobrepasan el 3%\n",
        "# mask_mejorar = ~ data_final['diff3']\n",
        "# ####################################################################################################################\n",
        "# # Comentar esta linea para que no se excluyan los clientes desactivados\n",
        "# data_grouped = data_grouped_active_clients\n",
        "\n",
        "# # Igual que Diciembre 2020 (ultimos datos)\n",
        "# pred_202012 = data_grouped.loc['2019-12'].T.reset_index()\n",
        "# pred_202012.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_202012.to_csv('BASELINE-pred_202012.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 3 meses\n",
        "# pred_mean3 = data_grouped.loc['2019-10':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean3.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean3.to_csv('BASELINE-pred_mean3.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 6 meses\n",
        "# pred_mean6 = data_grouped.loc['2019-07':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean6.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean6.to_csv('BASELINE-pred_mean6.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 12 meses\n",
        "# pred_mean12 = data_grouped.loc['2019-01':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean12.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean12.to_csv('BASELINE-pred_mean12_act3.csv', header=True, index=False)\n",
        "# pred_mean12"
      ],
      "metadata": {
        "id": "Zg8foW6XMcMr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/17 - Concatenamos Estadistica con Redes Neuronales"
      ],
      "metadata": {
        "id": "-sB43EDztBui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # De la noebook anterior\n",
        "\n",
        "# # Relacion entre el promedio anual anterior, y el febrero siguiente\n",
        "# data_2017_mean = pd.Series(data_grouped.loc['2017'].mean(axis=0), name='2017_mean')\n",
        "# data_2018_mean = pd.Series(data_grouped.loc['2018'].mean(axis=0), name='2018_mean')\n",
        "# data_2019_mean = pd.Series(data_grouped.loc['2019'].mean(axis=0), name='2019_mean')\n",
        "# feb_2018 = pd.Series(data_grouped.loc['2018-02'].T.squeeze(), name='Feb_2018')\n",
        "# feb_2019 = pd.Series(data_grouped.loc['2019-02'].T.squeeze(), name='Feb_2019')\n",
        "\n",
        "# data = pd.concat([data_2017_mean, feb_2018, data_2018_mean, feb_2019, data_2019_mean], axis=1)\n",
        "\n",
        "# # display(data.head())\n",
        "\n",
        "# # Calculamos cuanto represetan las ventas de Febrero, con respecto al Mean de todo el ano anterior\n",
        "# ratio_2017 = pd.Series(data['Feb_2018'] / data['2017_mean'], name='Ratio_2017')\n",
        "# ratio_2018 = pd.Series(data['Feb_2019'] / data['2018_mean'], name='Ratio_2018')\n",
        "# data_ratios = pd.concat([ratio_2017, ratio_2018], axis=1)\n",
        "\n",
        "# # # Nos enfocamos solo en los primeros 40 productos, los mas importantes\n",
        "# # data_ratios[:40].plot(figsize=(25, 6))\n",
        "# # plt.show()\n",
        "\n",
        "# # display(data_ratios.head())\n",
        "# # Vemos el huevo del producto #32, ya que no tenemos ventas todos los meses\n",
        "# # Vemos como son bastantes parecidas\n",
        "# # Se ve que el producto 19 fue suplementado por el 22 en las ventas del 2018\n",
        "\n",
        "\n",
        "# # Si el ratio del producto vendido entre el promedo del Ano y Febrero del ano siguiente es menor al 3%, sigue manteniendo ese ratio\n",
        "# data_ratios['diff3'] = (abs(data_ratios['Ratio_2017'] - data_ratios['Ratio_2018']) <= 0.03)\n",
        "# data_final = pd.concat([data_ratios, data_2019_mean], axis=1)\n",
        "\n",
        "# # Inflation Multiplier, dejar en 1 para que no modifique las predicciones\n",
        "# inflation = 0.90\n",
        "# data_final['tn'] = data_final.apply(lambda row: row['2019_mean'] * row['Ratio_2018'] * inflation if row['diff3'] else row['2019_mean'], axis=1)\n",
        "\n",
        "# # display(data_final.head())\n",
        "\n",
        "# # # Formateo y exporto a Kaggle.\n",
        "# # to_kaggle(data_final['tn'].reset_index(), name='modelo_loco_3diff_inlf90')\n",
        "\n",
        "# # Guardo los productos que tengo que mejorar, los que los ratio de ventas con Febrero sobrepasan el 3%\n",
        "# mask_mejorar = ~ data_final['diff3']"
      ],
      "metadata": {
        "id": "cL2qjeKy481y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Predecimos las ventas de productos que no fueron buenos usando la estadistica clasica\n",
        "\n",
        "# #########################################################################################\n",
        "# # # Viene de otro EDA, si da error hay que ejecutar ese analisis primero que genera esta variable\n",
        "# # mask_mejorar = ~ data_final['diff3']\n",
        "\n",
        "# products_list = mask_mejorar[mask_mejorar == True].index\n",
        "\n",
        "# # Probamos con los datos de los clientes activos only\n",
        "# data_grouped_filled = fill_nulls(data_grouped_active_clients)\n",
        "\n",
        "# # Probamos reemplazando la crisis de Agosto 2019 por Julio + 10% (visto en el EDA)\n",
        "# data_grouped_filled.drop(index='2019-08', axis=1, inplace=True)\n",
        "# data_agosto_2019_jul_plus10 = data_grouped_filled.loc['2019-07']*1.1\n",
        "# data_agosto_2019_jul_plus10.index = pd.to_datetime(['2019-08-01'])\n",
        "# data_grouped_filled = pd.concat([data_grouped_filled, data_agosto_2019_jul_plus10]).sort_index()\n",
        "# #########################################################################################\n",
        "\n",
        "# # Inicializamos el vector de predicciones\n",
        "# vector_predictions = data_productos_a_predecir.copy()\n",
        "# vector_predictions['tn'] =0\n",
        "# # vector_predictions = vector_predictions.squeeze()\n",
        "\n",
        "# for product in products_list[:50]:\n",
        "#   print(product)\n",
        "\n",
        "#   # Modificar el valor del filtro, para analizar otros Productos. Ojo que hay productos que no tenemos todos los anos y pincha\n",
        "#   data_product = data_grouped[product]\n",
        "\n",
        "#   # Jugamos con esto aca, para ver el tema de la validacion en el entrenamiento\n",
        "#   window_size = 12\n",
        "#   batch_size = 1\n",
        "\n",
        "\n",
        "#   data_train, data_valid = split_data(data_product, window_size)\n",
        "#   data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "#   # print(data_train.index)\n",
        "#   # print(data_valid.index)\n",
        "#   data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'xx'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 30\n",
        "#   epochs = 300\n",
        "#   n_features = data_train.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       validation_data = data_valid_windowed,\n",
        "#       callbacks = callbacks,\n",
        "#       verbose=0,\n",
        "#       epochs=epochs)\n",
        "\n",
        "#   # plot_history(history)\n",
        "\n",
        "#   predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "#   # plot_predictions_dec2019(data_product, predictions)\n",
        "\n",
        "#   # Vamos sumando en el vector de acum\n",
        "#   value_to_add = predictions.loc['2020-02'].values[0]\n",
        "#   vector_predictions.loc[predictions.name]+= value_to_add\n",
        "\n",
        "# vector_predictions.to_csv('predicciones_win12.csv')"
      ],
      "metadata": {
        "id": "xPSg0lhQtM6J"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/17 - Errores contra Diciembre 2019 (WIP)"
      ],
      "metadata": {
        "id": "70vs0Lkdlgsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analizamos las series de los productos mas vendidos, pero esta vez testeamos contra los ultimos dos meses del dataset Nov y Dev 2019\n",
        "\n",
        "# # # Inicializamos el vector de predicciones\n",
        "# # vector_predictions = data_productos_a_predecir.copy()\n",
        "# # vector_predictions['tn'] =0\n",
        "\n",
        "\n",
        "# # Modificar el valor del filtro, para analizar otros Productos. Ojo que hay productos que no tenemos todos los anos y pincha\n",
        "# data_product = data_grouped['20003']\n",
        "\n",
        "# # Jugamos con esto aca, para ver el tema de la validacion en el entrenamiento\n",
        "# window_size = 6\n",
        "# batch_size = 1\n",
        "\n",
        "\n",
        "# data_train, data_valid = split_data_dec2019(data_product, window_size)\n",
        "# data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "# # print(data_train.index)\n",
        "# # print(data_valid.index)\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'xx'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 300\n",
        "# n_features = data_train.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=1,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# plot_history(history)\n",
        "\n",
        "# predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "# # plot_predictions_dec2019(data_product, predictions)\n",
        "# display(predictions)"
      ],
      "metadata": {
        "id": "ENuMHEl_nL-3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/17 - Ratio Febrero con Promedio de anos anteriores"
      ],
      "metadata": {
        "id": "_a6ldKf-vb-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Si quiero usar el Dataset con las ventas de los clientes desactivados excluidas\n",
        "# # Y trasca le completo los nulos\n",
        "# data_grouped = fill_nulls(data_grouped_active_clients)\n",
        "\n",
        "# # Relacion entre el promedio anual anterior, y el febrero siguiente\n",
        "# data_2017_mean = pd.Series(data_grouped.loc['2017'].mean(axis=0), name='2017_mean')\n",
        "# data_2018_mean = pd.Series(data_grouped.loc['2018'].mean(axis=0), name='2018_mean')\n",
        "# data_2019_mean = pd.Series(data_grouped.loc['2019'].mean(axis=0), name='2019_mean')\n",
        "# feb_2018 = pd.Series(data_grouped.loc['2018-02'].T.squeeze(), name='Feb_2018')\n",
        "# feb_2019 = pd.Series(data_grouped.loc['2019-02'].T.squeeze(), name='Feb_2019')\n",
        "\n",
        "# data = pd.concat([data_2017_mean, feb_2018, data_2018_mean, feb_2019, data_2019_mean], axis=1)\n",
        "\n",
        "# # display(data.head())\n",
        "\n",
        "# # Calculamos cuanto represetan las ventas de Febrero, con respecto al Mean de todo el ano anterior\n",
        "# ratio_2017 = pd.Series(data['Feb_2018'] / data['2017_mean'], name='Ratio_2017')\n",
        "# ratio_2018 = pd.Series(data['Feb_2019'] / data['2018_mean'], name='Ratio_2018')\n",
        "# data_ratios = pd.concat([ratio_2017, ratio_2018], axis=1)\n",
        "\n",
        "# # Nos enfocamos solo en los primeros 40 productos, los mas importantes\n",
        "# data_ratios[:40].plot(figsize=(25, 6))\n",
        "# plt.show()\n",
        "\n",
        "# # display(data_ratios.head())\n",
        "# # Vemos el huevo del producto #32, ya que no tenemos ventas todos los meses\n",
        "# # Vemos como son bastantes parecidas\n",
        "# # Se ve que el producto 19 fue suplementado por el 22 en las ventas del 2018\n",
        "\n",
        "\n",
        "# # Si el ratio del producto vendido entre el promedo del Ano y Febrero del ano siguiente es menor al 3%, sigue manteniendo ese ratio\n",
        "# data_ratios['diff3'] = (abs(data_ratios['Ratio_2017'] - data_ratios['Ratio_2018']) <= 0.03)\n",
        "# data_final = pd.concat([data_ratios, data_2019_mean], axis=1)\n",
        "\n",
        "# # Inflation Multiplier, dejar en 1 para que no modifique las predicciones\n",
        "# inflation = .92\n",
        "# data_final['tn'] = data_final.apply(lambda row: row['2019_mean'] * row['Ratio_2018'] * inflation if row['diff3'] else row['2019_mean'], axis=1)\n",
        "\n",
        "# # display(data_final.head())\n",
        "\n",
        "# # Formateo y exporto a Kaggle.\n",
        "# to_kaggle(data_final['tn'].reset_index(), name='modelo_loco_3diff_inlf92_active3')\n",
        "\n",
        "# # Guardo los productos que tengo que mejorar, los que los ratio de ventas con Febrero sobrepasan el 3%\n",
        "# mask_mejorar = ~ data_final['diff3']"
      ],
      "metadata": {
        "id": "dcF-bI-2vgIv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/17 - Error Analysis: Predicciones contra Diciembre 2019"
      ],
      "metadata": {
        "id": "pc4pDaocEdhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/16 - Error Analysis"
      ],
      "metadata": {
        "id": "GiO91wV2MWuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analizamos las series de los productos mas vendidos, vemos si las redes neuronales pueden entenderlas\n",
        "\n",
        "# # Modificar el valor del filtro, para analizar otros Productos. Ojo que hay productos que no tenemos todos los anos y pincha\n",
        "# data_product = data_grouped['20024']\n",
        "\n",
        "\n",
        "# data_train, data_valid = split_data_test(data_product)\n",
        "# data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'xx'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 20\n",
        "# epochs = 200\n",
        "# n_features = data_train.shape[1]\n",
        "\n",
        "\n",
        "# # Jugamos con esto aca, para ver el tema de la validacion en el entrenamiento\n",
        "# window_size = 3\n",
        "# batch_size = 1\n",
        "\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# plot_history(history)\n",
        "\n",
        "# predictions = generate_predictions_2(model, data_valid_norm, norm_params)\n",
        "# plot_predictions(data_product, predictions)"
      ],
      "metadata": {
        "id": "2Fr3EDZnMbQD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/15 - BASELINE + TOP Productos"
      ],
      "metadata": {
        "id": "Waoyxx3TzKh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # PARA LOCAL ONLY, DEMORA MUCHO\n",
        "\n",
        "# #############################################################################################\n",
        "# #############################################################################################\n",
        "#  # El EDA tiene que estar arriba\n",
        "# mask_product_id_sold36 = (data_grouped > 0).sum(axis=0)==36\n",
        "# # print('Productos vendidos los 36 meses:', mask_product_id_sold36.sum())\n",
        "# # mask_product_id_sold12 = (data_grouped > 0).sum(axis=0)<=12\n",
        "# # print('Productos vendidos en 12 meses o menos:', mask_product_id_sold12.sum())\n",
        "\n",
        "# # Marco los que se vendieron los 36 meses\n",
        "# mask_products_id_top74 = mask_product_id_sold36\n",
        "\n",
        "# # No me interesan los +75\n",
        "# mask_products_id_top74.loc['20085':] = False\n",
        "# mask_products_id_top74.sum()\n",
        "# #############################################################################################\n",
        "# #############################################################################################\n",
        "# top_productos = data_productos_a_predecir[mask_products_id_top74].index\n",
        "\n",
        "# # # Prueba con los dos primeros\n",
        "# # top_productos = top_productos[:2]\n",
        "\n",
        "\n",
        "# # # Continuacion a mano desde producto 20017\n",
        "# # mask_17 = mask_products_id_top74.loc['20017':]\n",
        "# # data_17 = data_productos_a_predecir.loc['20017':]\n",
        "# # data_17[mask_17]\n",
        "# # top_productos = data_17[mask_17].index\n",
        "\n",
        "# data_prod_pred = data_productos_a_predecir.copy()\n",
        "# data_prod_pred['mean'] = 0\n",
        "# data_prod_pred['median'] = 0\n",
        "\n",
        "# for producto in top_productos:\n",
        "#   # print(f'Producto: {producto}')\n",
        "#   data_grouped_loc = group_data(data, 'product_id')[producto]\n",
        "#   # display(data_grouped_loc)\n",
        "\n",
        "#   data_train, data_valid = split_data(data_grouped_loc)\n",
        "#   data_train = pd.DataFrame(data_train)\n",
        "#   data_valid = pd.DataFrame(data_valid)\n",
        "#   # display(data_train.head())\n",
        "\n",
        "#   data_train_norm, data_valid_norm, norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "#   # display(data_valid_norm.head())\n",
        "\n",
        "#   data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'M1'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 20\n",
        "#   epochs = 100\n",
        "#   interaciones = 10\n",
        "#   n_features = data_train.shape[1]\n",
        "#   # print(n_features)\n",
        "\n",
        "\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   # Promedio 810\n",
        "#   mean, median = model_train(epochs, interaciones)\n",
        "#   data_prod_pred.loc[producto, ['mean', 'median']] = mean, median\n",
        "\n",
        "# # data_prod_pred.to_csv('predicciones_top74.csv', header=True, index=True)"
      ],
      "metadata": {
        "id": "Rce7LpdHi__X"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/15 - Nuevos BASELINES"
      ],
      "metadata": {
        "id": "J6t7e7l9bEeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ####################################################################################################################\n",
        "# # 2024-06-07: Excluyo las ventas de los clientes marcados como desactivados\n",
        "# # Ventas de clientes que no compraron en los ultimos dos meses, eliminadas\n",
        "# active_data = data[data['customer_id'].isin(mask_active_clients[mask_active_clients].index)]\n",
        "\n",
        "# # Pivotear los datos\n",
        "# data_grouped_active_clients = active_data.pivot_table(\n",
        "#     index='periodo',\n",
        "#     columns='product_id',\n",
        "#     values='tn',\n",
        "#     aggfunc='sum'\n",
        "# )\n",
        "\n",
        "# # Asegurarnos de que el índice esté en el formato correcto\n",
        "# data_grouped_active_clients.index = pd.to_datetime(data_grouped_active_clients.index)\n",
        "\n",
        "# # Ordenar el índice si fuera necesario\n",
        "# data_grouped_active_clients = data_grouped_active_clients.sort_index()\n",
        "# ####################################################################################################################\n",
        "\n",
        "# # Comentar esta linea para que no se excluyan los clientes desactivados\n",
        "# data_grouped = data_grouped_active_clients\n",
        "\n",
        "# # Igual que Diciembre 2020 (ultimos datos)\n",
        "# pred_202012 = data_grouped.loc['2019-12'].T.reset_index()\n",
        "# pred_202012.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_202012.to_csv('BASELINE-pred_202012.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 3 meses\n",
        "# pred_mean3 = data_grouped.loc['2019-10':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean3.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean3.to_csv('BASELINE-pred_mean3.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 6 meses\n",
        "# pred_mean6 = data_grouped.loc['2019-07':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean6.columns = ['product_id', 'tn']\n",
        "\n",
        "# # pred_mean6.to_csv('BASELINE-pred_mean6.csv', header=True, index=False)\n",
        "\n",
        "# # Promedio ultimos 12 meses\n",
        "# pred_mean12 = data_grouped.loc['2019-01':'2019-12'].T.mean(axis=1).reset_index()\n",
        "# pred_mean12.columns = ['product_id', 'tn']\n",
        "\n",
        "# pred_mean12.to_csv('BASELINE-pred_mean12_act4.csv', header=True, index=False)\n"
      ],
      "metadata": {
        "id": "Sy0QnKMklwzq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/12 - Walk Forward Validation\n",
        "- Son muy pocos datos los que tenemos, no funciona bien"
      ],
      "metadata": {
        "id": "PCQSAqstQinS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # #########################################################################\n",
        "# # # # TimeSeriesSplit\n",
        "# # # # #########################################################################\n",
        "\n",
        "# # TimeSeriesSplit: 3 splits para ejemplo\n",
        "# tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# # Almacenar las pérdidas para cada split\n",
        "# split_losses = []\n",
        "\n",
        "# # Inicializo el vector de predicciones\n",
        "# predicciones_all = data_productos_a_predecir.copy()\n",
        "# predicciones_all['tn'] = 0\n",
        "\n",
        "# # Probar si esto se puede sacar del bucle\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "\n",
        "# # Iterar sobre cada split\n",
        "# for i, (train_index, test_index) in enumerate(tscv.split(data_norm)):\n",
        "#     train_tscv = data_norm.iloc[train_index]\n",
        "#     test_tscv = data_norm.iloc[test_index]\n",
        "#     print('Train:\\n', train_tscv.shape[0])\n",
        "#     print('Test:\\n', test_tscv.shape[0])\n",
        "\n",
        "\n",
        "#     # Crear datasets de ventanas\n",
        "#     data_train_wrangled = windowed_dataset(train_tscv.values, 'train', window_size, horizon, batch_size)\n",
        "#     data_valid_wrangled = windowed_dataset(test_tscv.values, 'valid', window_size, horizon, batch_size)\n",
        "\n",
        "#     # Check if datasets are empty and adjust if necessary\n",
        "#     if len(list(data_train_wrangled)) == 0 or len(list(data_valid_wrangled)) == 0:\n",
        "#       print(f\"Warning: Empty dataset encountered for split {i+1}. Skipping this split.\")\n",
        "#       continue  # Skip to the next split\n",
        "\n",
        "#     history = model.fit(\n",
        "#     data_train_wrangled,\n",
        "#     validation_data = data_valid_wrangled,\n",
        "#     epochs=epochs,\n",
        "#     verbose=2,\n",
        "#     callbacks = callbacks)\n",
        "\n",
        "#     # Evaluar el modelo en el conjunto de validación\n",
        "#     val_loss = model.evaluate(data_valid_wrangled)\n",
        "#     print(f'Split {i+1} - Loss: {val_loss}')\n",
        "#     split_losses.append(val_loss)\n",
        "#     plot_history(history)\n",
        "\n",
        "#     predicciones_all = sumar_predicciones(predicciones_all, generate_predictions(False))\n",
        "\n",
        "\n",
        "# # Promedio de las pérdidas en todos los splits. El axis es por si analizamos mas de una metrica\n",
        "# avg_loss = np.mean(split_losses, axis=0)\n",
        "# print(f'Average Loss across all splits: {avg_loss}')\n",
        "\n",
        "# # Promedio las predicciones\n",
        "# predicciones_final = data_productos_a_predecir.copy()\n",
        "# predicciones_final['tn'] = predicciones_all['tn']/n_splits\n",
        "\n",
        "# # Exporto el CSV para Kaggle\n",
        "# filename = f\"{split_strategy}_{model_name}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "# predicciones_final.to_csv(filename, header=True, index=False)\n",
        "# print(filename)"
      ],
      "metadata": {
        "id": "LA0pXJB_7OD7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/11 - Agrupando por Categoria 1"
      ],
      "metadata": {
        "id": "Mj0hs54Uc8_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Inicializo el vector de predicciones\n",
        "# predictions_acum = data_productos_a_predecir.copy()\n",
        "# predictions_acum['tn'] = 0\n",
        "# # predictions_acum.set_index('product_id', inplace=True)\n",
        "# predictions_acum = predictions_acum.squeeze()\n",
        "# predictions_acum\n",
        "\n",
        "# # Genero el vector de categorias\n",
        "# categorias = data_productos_a_predecir_con_categorias.cat1.unique()\n",
        "\n",
        "# # Creo un modelo para cada categoria 1\n",
        "# for cat in categorias:\n",
        "#   data_cat1 = filter_data_por_categoria(data_grouped, cat, 'cat1')\n",
        "#   n_features = data_cat1.shape[1]\n",
        "#   # display(data_cat1)\n",
        "#   data_train_cat1, data_valid_cat1 = split_data(data_cat1)\n",
        "#   print(f'Categoria {cat}: {data_train_cat1.shape}, {data_valid_cat1.shape}')\n",
        "#   data_train_cat1_norm, data_valid_cat1_norm, data_norm_params = normalize_data(data_train_cat1, data_valid_cat1, normalization)\n",
        "#   data_train_windowed = windowed_dataset(data_train_cat1_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_train_cat1, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#   # Creo que tengo que llenar los huecos\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'CAT1'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 50\n",
        "#   epochs = 500\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       validation_data = data_valid_windowed,\n",
        "#       callbacks = callbacks,\n",
        "#       verbose=0,\n",
        "#       epochs=epochs)\n",
        "\n",
        "#   plot_history(history)\n",
        "\n",
        "# #   # Seleccionar los últimos x meses de data_train\n",
        "# #   data_for_prediction = data_train_cat1[-window_size:]\n",
        "# #   # Convierte los datos a un formato compatible con la función window_dataset\n",
        "# #   data_for_prediction = data_for_prediction.values.reshape((1, window_size, n_features))\n",
        "# #   predictions = model.predict(data_for_prediction)\n",
        "\n",
        "# #   # # Convertir las predicciones a un DataFrame para desnormalizar\n",
        "# #   predictions_df = pd.DataFrame(predictions[0], columns=data_train_cat1.columns)\n",
        "\n",
        "# #   # # Desnormalizar las predicciones\n",
        "# #   predictions_denorm_cat1 = denormalize_series(predictions_df, data_norm_params, normalization=normalization).iloc[1]\n",
        "\n",
        "# #   # Voy sumando las predicciones de cada categoria\n",
        "# #   predictions_acum = predictions_acum.add(predictions_denorm_cat1, fill_value=0)\n",
        "\n",
        "# # # Exporto a formato Kaggle\n",
        "# # predictions_acum_df = pd.DataFrame(predictions_acum).reset_index()\n",
        "# # predictions_acum_df.columns = ['product_id', 'tn']\n",
        "# # filename = f\"{model_name}_{split_strategy}_win{window_size}_batch{batch_size}_{normalization}_{loss}.csv\"\n",
        "# # predictions_acum_df.to_csv(filename, header=True, index=False)\n",
        "# # print(filename)"
      ],
      "metadata": {
        "id": "4i5vHk16uOl8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/11 - Analisis del Error por Producto"
      ],
      "metadata": {
        "id": "OisFvYA0feHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # NO FUNCIONA\n",
        "# # # # # # #########################################################################\n",
        "# # # # # # Train hasta 2019-10, para predecir 2019-12\n",
        "# # # # # # #########################################################################\n",
        "# split_data_2019\n",
        "# data_filled = fill_nulls(data_grouped)\n",
        "# data_train, data_valid, data_test = split_data_2019(data_filled)\n",
        "# data_train = pd.DataFrame(data_train)\n",
        "# data_valid = pd.DataFrame(data_valid)\n",
        "# data_test = pd.DataFrame(data_test)\n",
        "# print(data_train.shape, data_valid.shape, data_test.shape)\n",
        "\n",
        "\n",
        "# data_train_norm, data_valid_norm, data_norm_params = normalize_data(data_train, data_valid, normalization)\n",
        "# data_train_windowed = windowed_dataset(data_train_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid_norm, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size,)\n",
        "# display(data_test)\n",
        "\n",
        "# # Al hacer el split disitnto, hay que modificar esto sino me voy de rango creando las window\n",
        "# # split_strategy = 'S1'\n",
        "# # window_size = 3\n",
        "# # horizon = 2\n",
        "# # batch_size = 1\n",
        "# # normalization = 'MinMax'\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'ErrorAnalysis'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 20\n",
        "# epochs = 200\n",
        "# n_features = data_train.shape[1]\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# plot_history(history)\n",
        "\n",
        "# # # No sirve para este caso, esta hardcodeada a Febrero 2020. Tengo que actualizar esta funcion\n",
        "# # # generate_predictions(data_norm, data_norm_params)\n",
        "\n",
        "# # # Seleccionar los últimos x meses de data_train\n",
        "# # data_for_prediction = data_train[-window_size:]\n",
        "# # # Convierte los datos a un formato compatible con la función window_dataset\n",
        "# # data_for_prediction = data_for_prediction.values.reshape((1, window_size, n_features))\n",
        "# # predictions = model.predict(data_for_prediction)\n",
        "\n",
        "# # # Convertir las predicciones a un DataFrame para desnormalizar\n",
        "# # predictions_df = pd.DataFrame(predictions[0], columns=data_train.columns)\n",
        "\n",
        "# # # Desnormalizar las predicciones\n",
        "# # predictions_denorm = denormalize_series(predictions_df, data_norm_params, normalization=normalization)\n",
        "\n",
        "# # # Imprimir las predicciones desnormalizadas\n",
        "# # # display(predictions_denorm)\n",
        "\n",
        "\n",
        "# # # Genero las Series para plotear el error entre predicho y real. Armo un Dataframe\n",
        "# # data_dec2019_pred = pd.Series(predictions_denorm.iloc[1], name='Pred')\n",
        "# # data_dec2019_true = pd.Series(data_grouped.fillna(0).loc['2019-12-01'], name='True')\n",
        "# # data_dec2019_error = pd.concat([data_dec2019_pred, data_dec2019_true], axis=1)\n",
        "\n",
        "# # # Clusterizo por Categorias 1 y 2 de productos\n",
        "# # data_productos_indexed = data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()\n",
        "# # data_dec2019_error_detail = data_dec2019_error.join(data_productos_indexed[['cat1', 'cat2', 'cat3']])\n",
        "\n",
        "# # # Ploteo los Errores, con sus clusters\n",
        "# # sns.scatterplot(data=data_dec2019_error_detail, x='Pred', y='True', hue='cat1')\n",
        "# # plt.axline((0, 0), slope=1, color='r', linestyle='--')\n",
        "# # plt.show()\n",
        "\n",
        "# # # Verificamos que Categoria es la que engloba mas productos\n",
        "# # print(data_productos['cat1'].value_counts())\n",
        "\n",
        "# # # Productos con mayores diferencias en la prediccion\n",
        "# # predictions_worst10 = abs(data_dec2019_pred - data_dec2019_true).sort_values(ascending=False).head(10)\n",
        "# # display(pd.DataFrame(predictions_worst10).join(data_productos[['product_id', 'cat1']].set_index('product_id').sort_index()).rename(columns={0: 'tn_diff'}).sort_values(by='tn_diff', ascending=False))\n",
        "\n",
        "# # # Ploteamos el error en las predicciones acumulaod por Categoria\n",
        "# # pd.DataFrame(abs(data_dec2019_pred - data_dec2019_true).sort_values(ascending=False)).join(data_productos[['product_id', 'cat1']].set_index('product_id').sort_index()).rename(columns={0: 'tn_diff'}).groupby('cat1').sum().sort_values(by='tn_diff', ascending=False).plot(kind='bar')\n",
        "# # plt.title('Diferencia en toneladas por Categoria')\n",
        "# # plt.show()"
      ],
      "metadata": {
        "id": "8tw6MlALfgrx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vemos claramente como estsamos prediciendo mal los productos de Health Care, mas que nada los que predice entre 350 y 600, esta prediciendo bastante de menos."
      ],
      "metadata": {
        "id": "7hM59Wm47VGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - Cada cliente por separado"
      ],
      "metadata": {
        "id": "fPq4wBabw7gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Esto lo corro en mi maquina local, en Colab se cuelga antes de terminar con los casi 600 clientes\n",
        "\n",
        "# # Inicializo el vector de predicciones\n",
        "# predicciones_all = data_productos_a_predecir.copy()\n",
        "# predicciones_all['tn'] = 0\n",
        "\n",
        "# # Probamos solo con los 3 primeros clientes\n",
        "# # customers = data['customer_id'].unique()\n",
        "# customers = ['10001', '10002', '10003']\n",
        "# customers.sort()\n",
        "\n",
        "# i = 0\n",
        "\n",
        "# for customer in customers:\n",
        "#   print('Vuelta: ', i)\n",
        "#   i += 1\n",
        "#   data_customer = data_filter.query('customer_id == @customer')\n",
        "#   data_customer_grouped = group_data(data_customer, 'product_id')\n",
        "#   data_customer_grouped_fixed = complete_sales(data_customer_grouped, data_productos_a_predecir)\n",
        "#   # display(data_customer_grouped_fixed)\n",
        "#   data_customer_filled = fill_nulls(data_customer_grouped_fixed) # Probar cual funciona mejor\n",
        "\n",
        "#     # Probamos reemplazando la crisis de Agosto 2019 por el promedio en Julio y Septiembre. Maybe no funciona\n",
        "#   data_customer_filled.drop(index='2019-08', axis=1, inplace=True)\n",
        "#   data_agosto_2019 = data_customer_filled.loc[['2019-07', '2019-09']].mean().to_frame().transpose()\n",
        "#   data_agosto_2019.index = pd.to_datetime(['2019-08-01'])\n",
        "#   data_customer_filled = pd.concat([data_customer_filled, data_agosto_2019]).sort_index()\n",
        "\n",
        "#   data_customer_norm, data_customer_norm_params = normalize_data(data_customer_filled, normalization=normalization)\n",
        "#   data_customer_norm_train, data_customer_norm_valid = split_data(data_customer_norm)\n",
        "#   data_train_windowed = windowed_dataset(data_customer_norm_train, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "#   data_valid_windowed = windowed_dataset(data_customer_norm_valid, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "#   # Model Variables\n",
        "#   model_name = 'M1'\n",
        "#   loss = 'mse'\n",
        "#   optimizer = 'adam'\n",
        "#   patience = 30\n",
        "#   epochs = 500\n",
        "#   callbacks = MyCallbacks(patience)\n",
        "#   model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "#   history = model.fit(\n",
        "#       data_train_windowed,\n",
        "#       validation_data = data_valid_windowed,\n",
        "#       callbacks = callbacks,\n",
        "#       verbose=0,\n",
        "#       epochs=epochs)\n",
        "\n",
        "#   plot_history(history)\n",
        "\n",
        "\n",
        "#   predicciones = generate_predictions(data_customer_norm, data_customer_norm_params, False)\n",
        "#   predicciones_all = sumar_predicciones(predicciones_all, predicciones)\n",
        "\n",
        "# predicciones_all.to_csv('predicciones_local.csv', header=True, index=False)"
      ],
      "metadata": {
        "id": "TvvXLKmpyS5s"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - No Split"
      ],
      "metadata": {
        "id": "MFRKlhKimrIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # #########################################################################\n",
        "# # # Sin Splitear los datos, usando todo para entrenar\n",
        "# # # #########################################################################\n",
        "\n",
        "# data_train_windowed = windowed_dataset(data_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# # data_train_windowed = window_dataset(data_norm, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# # loss = 'mse'\n",
        "# # optimizer = 'adam'\n",
        "# # patience = 30\n",
        "# epochs = 10\n",
        "\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     # validation_data = data_valid_windowed,\n",
        "#     # callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# plot_history(history)\n",
        "# predicciones = generate_predictions(True)\n",
        "# predicciones"
      ],
      "metadata": {
        "id": "JFTI6TUpmrRP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - Split #2"
      ],
      "metadata": {
        "id": "56uvfCGZfKek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # #########################################################################\n",
        "# # # Train desde 2017-01 hasta 2019-06\n",
        "# # # #########################################################################\n",
        "\n",
        "# data_train, data_valid = split_data_2(data_norm)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "\n",
        "# callbacks = MyCallbacks(model_name, patience)\n",
        "# model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# generate_predictions()"
      ],
      "metadata": {
        "id": "kMCpoMFHfMgh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06/10 - Split #1"
      ],
      "metadata": {
        "id": "Z2xfLSMdU96y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PKqWpm9EmC_a"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # #########################################################################\n",
        "# # # # Train 2018 & 2018, Validation 2019 (10/06)\n",
        "# # # # #########################################################################\n",
        "# data_train, data_valid = split_data_1(data_norm)\n",
        "# data_train_windowed = windowed_dataset(data_train, data_split='train', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "# data_valid_windowed = windowed_dataset(data_valid, data_split='valid', window_size=window_size, horizon=horizon, batch_size=batch_size)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "\n",
        "# callbacks = MyCallbacks(patience)\n",
        "# model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# plot_history(history)\n",
        "# preddicciones = generate_predictions(True)\n",
        "# preddicciones"
      ],
      "metadata": {
        "id": "I4NrpvT1VgNz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate: Product 20001"
      ],
      "metadata": {
        "id": "9L6P6xEM3XPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data = data_filled[['20001']]\n",
        "\n",
        "# # Parámetros\n",
        "# window_size = 6  # Tamaño de la ventana de tiempo\n",
        "# n_future = 2  # Número de pasos futuros a predecir\n",
        "# batch_size = 32\n",
        "# n_splits = 5  # Número de divisiones para validación \"walk forward\"\n",
        "# n_features = data.shape[1]  # Número de características en el conjunto de datos\n",
        "\n",
        "# data_norm, data_norm_params = normalize_data(data, normalization=normalization)\n",
        "# data_norm"
      ],
      "metadata": {
        "id": "keCpJUPo3ggY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proof of Concept"
      ],
      "metadata": {
        "id": "y28Ak-HH3TEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #########################################################################\n",
        "# # New Pipeline (09/06)\n",
        "# #########################################################################\n",
        "# data_norm, data_norm_params = normalize_data(data_filled, normalization=normalization)\n",
        "# # data_train, data_valid = split_data(data_norm) # Split pendiente\n",
        "# data_train = data_norm\n",
        "# print(data_train.shape)\n",
        "# # print(data_valid.shape)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# # data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "\n",
        "# #########################################################################\n",
        "# # Old Pipeline (08/06)\n",
        "# #########################################################################\n",
        "# # data_all = group_data(data, data_productos_a_predecir)\n",
        "# # data_all_norm, data_all_norm_params = normalize_data(data_all, normalization=normalization)\n",
        "# # data_all_norm['20001'].describe()\n",
        "# # data_train, data_valid = split_data_all(data_all_norm)\n",
        "# # print(data_train.shape)\n",
        "# # print(data_valid.shape)\n",
        "# # data_train = data_all_norm\n",
        "# # data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# # data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "\n",
        "# #########################################################################\n",
        "# # Modelo\n",
        "# #########################################################################\n",
        "# data_train, data_valid = split_data_1(data_norm)\n",
        "# data_train_windowed = window_dataset(data_train, data_split='train', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "# data_valid_windowed = window_dataset(data_valid, data_split='valid', window_size=window_size, batch_size=batch_size, n_future=n_future)\n",
        "\n",
        "# # Model Variables\n",
        "# model_name = 'M1'\n",
        "# loss = 'mse'\n",
        "# optimizer = 'adam'\n",
        "# patience = 30\n",
        "# epochs = 500\n",
        "\n",
        "# callbacks = MyCallbacks(model_name, patience)\n",
        "# model = MyModel(loss, optimizer, window_size, n_future, n_features)\n",
        "\n",
        "# history = model.fit(\n",
        "#     data_train_windowed,\n",
        "#     validation_data = data_valid_windowed,\n",
        "#     callbacks = callbacks,\n",
        "#     verbose=2,\n",
        "#     epochs=epochs)\n",
        "\n",
        "# generate_predictions()"
      ],
      "metadata": {
        "id": "bCx0jH7rFU-M"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "CRuRNJCI8J7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivos Preeliminares\n",
        "* [x] Algunos productos por promedio, otros por red neuronal. Identificar cuales\n",
        " (los que no tengo ventas todos o la mayoria de los meses)\n",
        "*[x] Saber como fueron las ventas en cada Agosto, comparado con Julio y Septiembre del mismo ano. Esto para poder sortear el problema de la no ventas en Agosto 2018\n",
        "*[x] Entender la distribucion de ventas por categoria\n",
        "*[ ] Entender la distribucion de los errores por categoria\n",
        "*[ ] entrenar en 2017 y 2018, y predecir feb 2019, teniendo en cuenta el trend negativo de alguna manera\n",
        "*[ ] Saber si la serie de todos los productos por categoria, son similares\n",
        "*[ ] Dividr las ventas por tipo de calidad de producto (Alto, Media Bajo), pedirlo si no esta\n",
        "*[ ] Ver el error por producto, para saber cual analizar individualmente\n",
        "*[ ] Identificar los productos mas importantes, separarlos por categoria, y fijarse si las series son similares. No con todos, maybe los 150 orimeros solametne\n",
        "*[ ] Probar Unvariado en cada producto?\n",
        "\n"
      ],
      "metadata": {
        "id": "oX5UG0OxbRrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot de Ventas General\n",
        "# data.groupby(['periodo'])['tn'].sum().plot()\n",
        "# plt.show()\n",
        "\n",
        "# # # Deidentificacionde los productos y clientes\n",
        "# # # Evidentemente cuando el profe deidentifico los customers, lo hizo asignandoles ID secuenciales al listado ordenado por la suma de ventas(tn)\n",
        "# # print('Listado de Clientes, ordenados por la sumatoria de ventas en tn:\\n', group_data(data, 'customer_id').sum(), '\\n')\n",
        "\n",
        "# # # Lo mismo cuando deidentifico a los productos, solo que esta vez empezo desde 20000\n",
        "# # print('Listado de Productos, ordenados por la sumatoria de ventas en tn:\\n', group_data(data, 'product_id').sum())"
      ],
      "metadata": {
        "id": "-PQsvGzWAIMb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vemos la Tendencia Negativa\n",
        "* Tambien vemos la seasonalidad anual, y mas chica puede que tabien\n",
        "* Tambien se ve la caida en 2019-08, ya que decidimos no vender ese mes"
      ],
      "metadata": {
        "id": "talT1z4hc8jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 74"
      ],
      "metadata": {
        "id": "tEP9EZ77NM46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # data\n",
        "# # # data_productos\n",
        "# # # data_stocks\n",
        "# # # data_productos_a_predecir\n",
        "\n",
        "\n",
        "# mask_product_id_sold36 = (data_grouped > 0).sum(axis=0)==36\n",
        "# print('Productos vendidos los 36 meses:', mask_product_id_sold36.sum())\n",
        "# mask_product_id_sold12 = (data_grouped > 0).sum(axis=0)<=12\n",
        "# print('Productos vendidos en 12 meses o menos:', mask_product_id_sold12.sum())\n"
      ],
      "metadata": {
        "id": "tjN0AgGnNL7Q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ajustar la opción para mostrar más filas\n",
        "# pd.set_option('display.max_rows', None)\n",
        "\n",
        "# # mask_product_id_sold36[:76]"
      ],
      "metadata": {
        "id": "-mfMCEo-SrSX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vamos a ver de el TOP 74 (hasta ID 84) de los productos mas vendidos, y solo los product_id 20032 y 20049 no se vendieron los 36 meses\n"
      ],
      "metadata": {
        "id": "Zn2bX_L8VB5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analizamos esos casos en particular\n",
        "# data_grouped.loc[:, ['20032', '20049']].plot(title='Productos Top75 que no se vendieron los 36 meses')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "YuBGEZQhUQ4d"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Uno se empezo a vender en 2019-02, y el otro 2017-08"
      ],
      "metadata": {
        "id": "x-fmExFCWE1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Hacemos el CumSum de cada producto\n",
        "# SUM_TOT = data_grouped.sum(axis=0).sum()\n",
        "# product_id_32_sum = data_grouped.sum(axis=0).loc['20032']/SUM_TOT\n",
        "# product_id_49_sum = data_grouped.sum(axis=0).loc['20049']/SUM_TOT\n",
        "\n",
        "# (data_grouped.sum(axis=0)/SUM_TOT).loc[:'20084'].sum() - product_id_32_sum - product_id_49_sum"
      ],
      "metadata": {
        "id": "sxyEGV2WWCG0"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* De los 74 productos mas vendidos (menos 32 y 49), disponemos de ventas los 36 meses\n",
        "* Esos 74 productos representan un %66 de las Toneladas totales vendidas"
      ],
      "metadata": {
        "id": "3HVeAeneZhpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Marco los que se vendieron los 36 meses\n",
        "# mask_products_id_top74 = mask_product_id_sold36\n",
        "\n",
        "# # No me interesan los +75\n",
        "# mask_products_id_top74.loc['20085':] = False\n",
        "# mask_products_id_top74.sum()"
      ],
      "metadata": {
        "id": "CozfxDIHjVgC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisis Agosto 2019"
      ],
      "metadata": {
        "id": "ePaHFgdPb0VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_grouped.loc['2017-06':'2017-10'].sum(axis=1).plot(title='Agosto crece-crece en 2017')\n",
        "# plt.show()\n",
        "\n",
        "# data_grouped.loc['2018-06':'2018-10'].sum(axis=1).plot(title='Agosto crece-decrece en 2018')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "SPJsHIyQdkhe"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pareciera ser que siempre sube un 10% con respecto al Julio anterior"
      ],
      "metadata": {
        "id": "hiN9yUs1vtTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis por Categoria"
      ],
      "metadata": {
        "id": "AoScqaZOIrIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Genero el vector de categorias\n",
        "# categorias = data_productos_a_predecir_con_categorias.cat1.unique()\n",
        "\n",
        "# tn_TOT = data_grouped.sum().sum()\n",
        "\n",
        "# for cat in categorias:\n",
        "#   data_cat1 = filter_data_por_categoria(data_grouped, cat, 'cat1')\n",
        "#   n_features = data_cat1.shape[1]\n",
        "#   cat_tot = np.round(data_cat1.sum().sum() / tn_TOT, 2)\n",
        "#   print(f'{cat}, {data_cat1.shape}, {cat_tot}')\n"
      ],
      "metadata": {
        "id": "YIojBdWVI2m-"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* La categoria con mas productos es PC con 444\n",
        "* Pero la que mas vende en toneladas es HC, con un 61% del total"
      ],
      "metadata": {
        "id": "dD5QG3mnLjKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisis Ventas en los Febreros"
      ],
      "metadata": {
        "id": "9M0IKSYdSrzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # En 2017\n",
        "\n",
        "# data_2017 =data_grouped.loc['2017'].mean(axis=0)\n",
        "# feb_2017 = data_grouped.loc['2017-02']\n",
        "\n",
        "# # # Ploteamos 2017\n",
        "# # fig, ax = plt.subplots(figsize=(10, 6))\n",
        "# # # Ploteamos promedio del 2017\n",
        "# # data_2017[:50].plot(ax=ax, label='Promedio 2017')\n",
        "# # # Ploteamos Febrero 2017\n",
        "# # feb_2017.T[:50].plot(ax=ax, label='Febrero 2017')\n",
        "# # ax.legend()\n",
        "# # plt.show()\n",
        "# # # Vemos como el promedio anual y las ventas de Feb tienen una cierta relacion\n",
        "\n",
        "# # Calculamos el ratio que Febrero corresponde al promedio anual\n",
        "# diff_2017 = pd.Series(((data_2017- feb_2017)/feb_2017).T.squeeze(), name='diff')\n",
        "\n",
        "# # Lo joineamos a las categorias\n",
        "# diff_2017_cat = pd.concat([diff_2017, data_productos_a_predecir_con_categorias[['cat1', 'cat2']]], axis=1)\n",
        "\n",
        "# # Analizamos\n",
        "# diff_2017_cat.groupby('cat1')['diff'].mean().plot(kind='bar')\n",
        "# plt.title('Ratio ventas Proemdio Anual sobre Febrero en 2017')\n",
        "# plt.show()\n",
        "\n",
        "# # display(diff_cat.sort_values(by='diff', ascending=False).head(30))\n",
        "# # Vemos que ciertos productos de Personal Care no siguen el patron\n",
        "# # Esto habra sido un problema puntual en 2017, el profe habia comentaso lo de las cremas?\n",
        "# # Lo que si nos interesa es ver que HC y FOODS estan cerca del 0.5"
      ],
      "metadata": {
        "id": "PaHvU0QWSs4x"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # En 2018\n",
        "\n",
        "# data_2018 =data_grouped.loc['2018'].mean(axis=0)\n",
        "# feb_2018 = data_grouped.loc['2018-02']\n",
        "\n",
        "\n",
        "# # Calculamos el ratio que Febrero corresponde al promedio anual\n",
        "# diff_2018 = pd.Series(((data_2018 - feb_2018)/feb_2018).T.squeeze(), name='diff')\n",
        "\n",
        "# # Lo joineamos a las categorias\n",
        "# diff_2018_cat = pd.concat([diff_2018, data_productos_a_predecir_con_categorias[['cat1', 'cat2']]], axis=1)\n",
        "\n",
        "# # Analizamos\n",
        "# diff_2018_cat.groupby('cat1')['diff'].mean().plot(kind='bar')\n",
        "# plt.title('Ratio ventas Proemdio Anual sobre Febrero en 2018')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # HC y FOODS estan cerca del 0.5, como antes\n",
        "# # En FOODS paso algo raro, igual no vamos a analizarlo\n",
        "# # REF es insifnificante (solo 17 productos)"
      ],
      "metadata": {
        "id": "yOhqXjPQgAeN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # En 2019\n",
        "# data_2019 =data_grouped.loc['2019'].mean(axis=0)\n",
        "# feb_2019 = data_grouped.loc['2019-02']\n",
        "\n",
        "\n",
        "# # Calculamos el ratio que Febrero corresponde al promedio anual\n",
        "# diff_2019 = pd.Series(((data_2019 - feb_2019)/feb_2019).T.squeeze(), name='diff')\n",
        "\n",
        "# # Lo joineamos a las categorias\n",
        "# diff_2019_cat = pd.concat([diff_2019, data_productos_a_predecir_con_categorias[['cat1', 'cat2']]], axis=1)\n",
        "\n",
        "# # Analizamos\n",
        "# diff_2019_cat.groupby('cat1')['diff'].mean().plot(kind='bar')\n",
        "# plt.title('Ratio ventas Proemdio Anual sobre Febrero en 2019')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "K4lr12YdhUTI"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}