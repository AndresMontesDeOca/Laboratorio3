{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNpkLUcS0erEIxB27k8fqJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresMontesDeOca/Laboratorio3/blob/main/TimeSeriesSplit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temperatura\n",
        "\n"
      ],
      "metadata": {
        "id": "m5nnqCzlVjZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "#############################################################################\n",
        "def normalize_data(df, normalization=\"MinMax\"):\n",
        "    \"\"\"\n",
        "    Normaliza cada serie de tiempo (columna) de manera individual usando MinMax o Zscore.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame con series de tiempo de distintos productos, cada columna es un producto.\n",
        "        normalization (str): Tipo de normalización a aplicar. Opciones: \"MinMax\" o \"Zscore\". Default es \"MinMax\".\n",
        "\n",
        "    Returns:\n",
        "        normalized_df (pd.DataFrame): DataFrame con las series normalizadas.\n",
        "        normalization_params (pd.DataFrame): DataFrame con los parámetros necesarios para desnormalizar cada columna.\n",
        "            - Para \"MinMax\": valores min y max de cada columna.\n",
        "            - Para \"Zscore\": valores mean y std de cada columna.\n",
        "    \"\"\"\n",
        "    normalization_params = pd.DataFrame(columns=[\"product_id\", \"min\", \"max\", \"mean\", \"std\"])\n",
        "    normalized_df = pd.DataFrame(index=df.index)\n",
        "\n",
        "    for column in df.columns:\n",
        "        if normalization == \"MinMax\":\n",
        "            scaler = MinMaxScaler()\n",
        "            normalized_values = scaler.fit_transform(df[[column]]).flatten()\n",
        "            new_params = pd.DataFrame({\n",
        "                \"product_id\": [column],\n",
        "                \"min\": [scaler.data_min_[0]],\n",
        "                \"max\": [scaler.data_max_[0]],\n",
        "                \"mean\": [None],\n",
        "                \"std\": [None]\n",
        "            })\n",
        "            normalization_params = pd.concat([normalization_params, new_params], ignore_index=True)\n",
        "            normalized_df[column] = normalized_values\n",
        "\n",
        "        elif normalization == \"ZScore\":\n",
        "            scaler = StandardScaler()\n",
        "            normalized_values = scaler.fit_transform(df[[column]]).flatten()\n",
        "            new_params = pd.DataFrame({\n",
        "                \"product_id\": [column],\n",
        "                \"min\": [None],\n",
        "                \"max\": [None],\n",
        "                \"mean\": [scaler.mean_[0]],\n",
        "                \"std\": [scaler.scale_[0]]\n",
        "            })\n",
        "            normalization_params = pd.concat([normalization_params, new_params], ignore_index=True)\n",
        "            normalized_df[column] = normalized_values\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid normalization method. Choose 'MinMax' or 'ZScore'.\")\n",
        "\n",
        "    return normalized_df, normalization_params\n",
        "#############################################################################\n",
        "def denormalize_series(normalized_series, normalization_params, normalization=\"MinMax\"):\n",
        "    \"\"\"\n",
        "    Desnormaliza una serie de tiempo usando los valores almacenados.\n",
        "\n",
        "    Args:\n",
        "        normalized_series (pd.Series or pd.DataFrame): Serie o DataFrame con los datos normalizados.\n",
        "        normalization_params (pd.DataFrame): DataFrame con los parámetros necesarios para desnormalizar cada serie o columna.\n",
        "            - Para \"MinMax\": valores min y max de cada serie o columna.\n",
        "            - Para \"Zscore\": valores mean y std de cada serie o columna.\n",
        "        normalization (str): Tipo de normalización a deshacer. Opciones: \"MinMax\" o \"Zscore\". Default es \"MinMax\".\n",
        "\n",
        "    Returns:\n",
        "        denormalized_series (pd.Series or pd.DataFrame): Serie o DataFrame con los datos desnormalizados.\n",
        "    \"\"\"\n",
        "    if isinstance(normalized_series, pd.DataFrame):\n",
        "        denormalized_df = pd.DataFrame(index=normalized_series.index)\n",
        "        for column in normalized_series.columns:\n",
        "            params = normalization_params[normalization_params[\"product_id\"] == column]\n",
        "            if normalization == \"MinMax\":\n",
        "                min_value = params[\"min\"].values[0]\n",
        "                max_value = params[\"max\"].values[0]\n",
        "                denormalized_values = normalized_series[column] * (max_value - min_value) + min_value\n",
        "            elif normalization == \"ZScore\":\n",
        "                mean_value = params[\"mean\"].values[0]\n",
        "                std_value = params[\"std\"].values[0]\n",
        "                denormalized_values = normalized_series[column] * std_value + mean_value\n",
        "            else:\n",
        "                raise ValueError(\"Invalid normalization method. Choose 'MinMax' or 'ZScore'.\")\n",
        "            denormalized_df[column] = denormalized_values\n",
        "        return denormalized_df\n",
        "    elif isinstance(normalized_series, pd.Series):\n",
        "        product_ids = normalized_series.index\n",
        "        denormalized_values = []\n",
        "        for product_id in product_ids:\n",
        "            params = normalization_params[normalization_params[\"product_id\"] == product_id]\n",
        "            if normalization == \"MinMax\":\n",
        "                min_value = params[\"min\"].values[0]\n",
        "                max_value = params[\"max\"].values[0]\n",
        "                denormalized_value = normalized_series[product_id] * (max_value - min_value) + min_value\n",
        "            elif normalization == \"ZScore\":\n",
        "                mean_value = params[\"mean\"].values[0]\n",
        "                std_value = params[\"std\"].values[0]\n",
        "                denormalized_value = normalized_series[product_id] * std_value + mean_value\n",
        "            else:\n",
        "                raise ValueError(\"Invalid normalization method. Choose 'MinMax' or 'ZScore'.\")\n",
        "            denormalized_values.append(denormalized_value)\n",
        "        denormalized_series = pd.Series(denormalized_values, index=product_ids, name=normalized_series.name)\n",
        "        return denormalized_series\n",
        "    else:\n",
        "        raise TypeError(\"normalized_series should be either a pandas Series or DataFrame\")\n",
        "#############################################################################\n",
        "def split_data(df):\n",
        "  df_train = df.loc['2017':'2019-05']\n",
        "  df_valid = df.loc['2019-06':]\n",
        "  return df_train, df_valid\n",
        "#############################################################################\n",
        "def windowed_dataset(sequence, data_split, window_size, horizon, batch_size, shuffle_buffer=1000):\n",
        "    \"\"\"Generates dataset windows.\n",
        "\n",
        "    Args:\n",
        "      sequence (array-like): Contains the values of the time series.\n",
        "      data_split (str): Specifies if the dataset is for training or validation/test.\n",
        "      window_size (int): The number of time steps to include in the feature.\n",
        "      horizon (int): The number of future time steps to predict.\n",
        "      batch_size (int): The batch size.\n",
        "      shuffle_buffer (int): Buffer size to use for the shuffle method.\n",
        "\n",
        "    Returns:\n",
        "      tf.data.Dataset: TF Dataset containing time windows.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "\n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size + horizon, shift=1, drop_remainder=True)\n",
        "\n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + horizon))\n",
        "\n",
        "    # Create tuples with features and labels\n",
        "    dataset = dataset.map(lambda window: (window[:-horizon], window[-horizon:]))\n",
        "\n",
        "    if data_split == 'train':\n",
        "        # Shuffle the training data to improve generalization\n",
        "        dataset = dataset.shuffle(shuffle_buffer)\n",
        "    else:\n",
        "        # Cache the validation/test data for improved performance\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "    # Create batches of windows and prefetch for performance\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "#############################################################################\n",
        "def compile_model(new_model, loss, optimizer):\n",
        "    new_model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
        "    print(new_model.summary())\n",
        "    return new_model\n",
        "#############################################################################\n",
        "def MyModel(loss, optimizer, window_size, horizon, n_features):\n",
        "    new_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer((window_size, n_features)),\n",
        "        # tf.keras.layers.Conv1D(filters=window_size +  horizon, kernel_size=3, activation='relu', padding='causal'),\n",
        "        # tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=False)),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(n_features * horizon, activation='relu'),\n",
        "        tf.keras.layers.Reshape((horizon, n_features)),\n",
        "    ])\n",
        "    return compile_model(new_model, loss, optimizer)\n",
        "#############################################################################\n",
        "def MyCallbacks(patience):\n",
        "    \"\"\"\n",
        "    Devuelve una lista de callbacks para el entrenamiento del modelo.\n",
        "\n",
        "    Parameters:\n",
        "    patience (int): Número de épocas a esperar para ver una mejora en 'val_loss' antes de detener el entrenamiento.\n",
        "\n",
        "    Returns:\n",
        "    list: Lista de callbacks de Keras.\n",
        "    \"\"\"\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "    return [early_stop]\n",
        "#############################################################################\n",
        "def plot_history(history, start_epoch=0, metrics=None):\n",
        "    if isinstance(metrics, str):\n",
        "        metrics = [metrics]\n",
        "\n",
        "    if metrics is None:\n",
        "        metrics = [x for x in history.history.keys() if x[:4] != 'val_']\n",
        "\n",
        "    if len(metrics) == 0:\n",
        "        print('No metrics to display.')\n",
        "        return\n",
        "\n",
        "    # Get the epochs and filter them starting from start_epoch\n",
        "    x = history.epoch[start_epoch:]\n",
        "\n",
        "    rows = 1\n",
        "    cols = len(metrics)\n",
        "    count = 0\n",
        "\n",
        "    plt.figure(figsize=(12 * cols, 8))\n",
        "\n",
        "    for metric in sorted(metrics):\n",
        "        count += 1\n",
        "        plt.subplot(rows, cols, count)\n",
        "        plt.plot(x, history.history[metric][start_epoch:], label='Train')\n",
        "        val_metric = f'val_{metric}'\n",
        "        if val_metric in history.history.keys():\n",
        "            plt.plot(x, history.history[val_metric][start_epoch:], label='Validation')\n",
        "        plt.title(metric.capitalize())\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "################################################################"
      ],
      "metadata": {
        "id": "Z1q_dC75Cfw1"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "# !pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "\n",
        "################################# Datasets ###################################\n",
        "# # Ventas\n",
        "id = \"158aOjqxaNO8l97yA6VWJkek_15YVLMhs\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('sell-in.txt')\n",
        "data_ventas = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
        "data_ventas['periodo'] = pd.to_datetime(data_ventas['periodo'], format='%Y%m')\n",
        "data_ventas['customer_id'] = data_ventas['customer_id'].astype(str)\n",
        "data_ventas['product_id'] = data_ventas['product_id'].astype(str)\n",
        "data = data_ventas.copy()\n",
        "\n",
        "# # Productos\n",
        "id = \"15JS_k86LS0sgJXma7BOVXWlyNcMwxdhE\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('tb_productos.txt')\n",
        "data_productos = pd.read_csv(\"tb_productos.txt\", sep=\"\\t\")\n",
        "data_productos['product_id'] = data_productos['product_id'].astype(str)\n",
        "\n",
        "# # Stocks\n",
        "id = \"15EV-8f_U7onpA1AcTxxXeD-z8yVR4fQu\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('tb_stocks.txt')\n",
        "data_stocks = pd.read_csv(\"tb_stocks.txt\", sep=\"\\t\")\n",
        "data_stocks['periodo'] = pd.to_datetime(data_stocks['periodo'], format='%Y%m')\n",
        "data_stocks['product_id'] = data_stocks['product_id'].astype(str)\n",
        "\n",
        "# # Productos a predecir\n",
        "id = \"15LjADctFVwjzQFJvfJGFTEdgZx9xCoId\"\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('productos_a_predecir.txt')\n",
        "data_productos_a_predecir = pd.read_csv(\"productos_a_predecir.txt\", sep=\"\\t\")\n",
        "data_productos_a_predecir['product_id'] = data_productos_a_predecir['product_id'].astype(str)\n",
        "data_productos_a_predecir_con_categorias = data_productos_a_predecir.set_index('product_id').join(data_productos.drop_duplicates('product_id').set_index('product_id').sort_index()[['cat1', 'cat2', 'cat3']])\n",
        "\n",
        "\n",
        "data = data.query('product_id == \"20001\"').groupby('periodo')['tn'].sum()\n",
        "data.index.freq = 'MS'\n",
        "data = data.to_frame()\n",
        "# data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qB9aVZp-9tW2",
        "outputId": "eceecbce-fa7c-4f22-ed8d-857e8c7b7424"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    tn\n",
              "periodo               \n",
              "2017-01-01   934.77222\n",
              "2017-02-01   798.01620\n",
              "2017-03-01  1303.35771\n",
              "2017-04-01  1069.96130\n",
              "2017-05-01  1502.20132\n",
              "2017-06-01  1520.06539\n",
              "2017-07-01  1030.67391\n",
              "2017-08-01  1267.39462\n",
              "2017-09-01  1316.94604\n",
              "2017-10-01  1439.75563\n",
              "2017-11-01  1580.47401\n",
              "2017-12-01  1049.38860\n",
              "2018-01-01  1169.07532\n",
              "2018-02-01  1043.76470\n",
              "2018-03-01  1856.83534\n",
              "2018-04-01  1251.28462\n",
              "2018-05-01  1293.89788\n",
              "2018-06-01  1150.79169\n",
              "2018-07-01  1470.41009\n",
              "2018-08-01  1800.96168\n",
              "2018-09-01  1438.67455\n",
              "2018-10-01  2295.19832\n",
              "2018-11-01  1813.01511\n",
              "2018-12-01  1486.68669\n",
              "2019-01-01  1275.77351\n",
              "2019-02-01  1259.09363\n",
              "2019-03-01  1470.65653\n",
              "2019-04-01  1647.63848\n",
              "2019-05-01  1629.78233\n",
              "2019-06-01  1109.93769\n",
              "2019-07-01  1678.99318\n",
              "2019-08-01  1261.34529\n",
              "2019-09-01  1660.00561\n",
              "2019-10-01  1561.50552\n",
              "2019-11-01  1397.37231\n",
              "2019-12-01  1504.68856"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d4cfd2dc-93bf-4942-b940-6c506438b709\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tn</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>periodo</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-01</th>\n",
              "      <td>934.77222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-02-01</th>\n",
              "      <td>798.01620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-03-01</th>\n",
              "      <td>1303.35771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-04-01</th>\n",
              "      <td>1069.96130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-05-01</th>\n",
              "      <td>1502.20132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-01</th>\n",
              "      <td>1520.06539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-07-01</th>\n",
              "      <td>1030.67391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-01</th>\n",
              "      <td>1267.39462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-09-01</th>\n",
              "      <td>1316.94604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-10-01</th>\n",
              "      <td>1439.75563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-01</th>\n",
              "      <td>1580.47401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-01</th>\n",
              "      <td>1049.38860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-01</th>\n",
              "      <td>1169.07532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-02-01</th>\n",
              "      <td>1043.76470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-03-01</th>\n",
              "      <td>1856.83534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-04-01</th>\n",
              "      <td>1251.28462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-05-01</th>\n",
              "      <td>1293.89788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-06-01</th>\n",
              "      <td>1150.79169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-07-01</th>\n",
              "      <td>1470.41009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-08-01</th>\n",
              "      <td>1800.96168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-09-01</th>\n",
              "      <td>1438.67455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-10-01</th>\n",
              "      <td>2295.19832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-11-01</th>\n",
              "      <td>1813.01511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-12-01</th>\n",
              "      <td>1486.68669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-01-01</th>\n",
              "      <td>1275.77351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-02-01</th>\n",
              "      <td>1259.09363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-03-01</th>\n",
              "      <td>1470.65653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-04-01</th>\n",
              "      <td>1647.63848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-05-01</th>\n",
              "      <td>1629.78233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-06-01</th>\n",
              "      <td>1109.93769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-07-01</th>\n",
              "      <td>1678.99318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-08-01</th>\n",
              "      <td>1261.34529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-09-01</th>\n",
              "      <td>1660.00561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-10-01</th>\n",
              "      <td>1561.50552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-01</th>\n",
              "      <td>1397.37231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-01</th>\n",
              "      <td>1504.68856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4cfd2dc-93bf-4942-b940-6c506438b709')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d4cfd2dc-93bf-4942-b940-6c506438b709 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d4cfd2dc-93bf-4942-b940-6c506438b709');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b2acbb23-8350-4f9d-95ee-64d00c5fb932\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b2acbb23-8350-4f9d-95ee-64d00c5fb932')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b2acbb23-8350-4f9d-95ee-64d00c5fb932 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b71f22a5-a5fb-46a3-ad98-2b869a99137b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b71f22a5-a5fb-46a3-ad98-2b869a99137b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": \"periodo\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2017-01-01 00:00:00\",\n        \"max\": \"2019-12-01 00:00:00\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"2019-12-01 00:00:00\",\n          \"2018-02-01 00:00:00\",\n          \"2019-03-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tn\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 298.1454602000109,\n        \"min\": 798.0162,\n        \"max\": 2295.19832,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          1504.68856,\n          1043.7647,\n          1470.65653\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal Split\n",
        "normalization = 'MinMax'\n",
        "window_size = 6\n",
        "horizon = 1\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "data_norm, data_norm_params = normalize_data(data, normalization=normalization)\n",
        "data_norm_train, data_norm_valid = split_data(data_norm)\n",
        "data_train_wrangled = windowed_dataset(data_norm_train, 'train', window_size, horizon, batch_size)\n",
        "data_valid_wrangled = windowed_dataset(data_norm_valid, 'valid', window_size, horizon, batch_size)\n",
        "\n",
        "model_name = 'TimeSeries'\n",
        "loss = 'mse'\n",
        "optimizer = 'adam'\n",
        "n_features = data.shape[1]\n",
        "patience = 3\n",
        "epochs = 20\n",
        "\n",
        "callbacks = MyCallbacks(patience)\n",
        "model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "history = model.fit(\n",
        "    data_train_wrangled,\n",
        "    validation_data = data_valid_wrangled,\n",
        "    epochs=epochs,\n",
        "    verbose=2,\n",
        "    callbacks = callbacks)\n",
        "\n",
        "# Evaluar el modelo en el conjunto de validación\n",
        "val_loss = model.evaluate(data_valid_wrangled)\n",
        "print(f'Loss: {val_loss}')\n",
        "\n",
        "\n",
        "# plot_history(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWcTL7R-KeXj",
        "outputId": "da70aa57-d9b1-43f2-ca58-7111523ab5ae"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_60 (Bidirect  (None, 6, 64)             8704      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_61 (Bidirect  (None, 32)                10368     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            " reshape_30 (Reshape)        (None, 1, 1)              0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19105 (74.63 KB)\n",
            "Trainable params: 19105 (74.63 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "1/1 - 10s - loss: 0.1282 - mae: 0.3243 - val_loss: 0.6146 - val_mae: 0.7666 - 10s/epoch - 10s/step\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 - 0s - loss: 0.1277 - mae: 0.3230 - val_loss: 0.6146 - val_mae: 0.7666 - 41ms/epoch - 41ms/step\n",
            "Epoch 3/20\n",
            "1/1 - 0s - loss: 0.1225 - mae: 0.3186 - val_loss: 0.6144 - val_mae: 0.7663 - 73ms/epoch - 73ms/step\n",
            "Epoch 4/20\n",
            "1/1 - 0s - loss: 0.1275 - mae: 0.3221 - val_loss: 0.6114 - val_mae: 0.7638 - 70ms/epoch - 70ms/step\n",
            "Epoch 5/20\n",
            "1/1 - 0s - loss: 0.1206 - mae: 0.3133 - val_loss: 0.5858 - val_mae: 0.7473 - 72ms/epoch - 72ms/step\n",
            "Epoch 6/20\n",
            "1/1 - 0s - loss: 0.1028 - mae: 0.2875 - val_loss: 0.5474 - val_mae: 0.7221 - 78ms/epoch - 78ms/step\n",
            "Epoch 7/20\n",
            "1/1 - 0s - loss: 0.1121 - mae: 0.2980 - val_loss: 0.5072 - val_mae: 0.6947 - 70ms/epoch - 70ms/step\n",
            "Epoch 8/20\n",
            "1/1 - 0s - loss: 0.0986 - mae: 0.2678 - val_loss: 0.4663 - val_mae: 0.6657 - 72ms/epoch - 72ms/step\n",
            "Epoch 9/20\n",
            "1/1 - 0s - loss: 0.0661 - mae: 0.2214 - val_loss: 0.4255 - val_mae: 0.6352 - 71ms/epoch - 71ms/step\n",
            "Epoch 10/20\n",
            "1/1 - 0s - loss: 0.0722 - mae: 0.2184 - val_loss: 0.3853 - val_mae: 0.6036 - 73ms/epoch - 73ms/step\n",
            "Epoch 11/20\n",
            "1/1 - 0s - loss: 0.0680 - mae: 0.2188 - val_loss: 0.3459 - val_mae: 0.5709 - 73ms/epoch - 73ms/step\n",
            "Epoch 12/20\n",
            "1/1 - 0s - loss: 0.0661 - mae: 0.2002 - val_loss: 0.3083 - val_mae: 0.5377 - 75ms/epoch - 75ms/step\n",
            "Epoch 13/20\n",
            "1/1 - 0s - loss: 0.0508 - mae: 0.1782 - val_loss: 0.2723 - val_mae: 0.5036 - 77ms/epoch - 77ms/step\n",
            "Epoch 14/20\n",
            "1/1 - 0s - loss: 0.0419 - mae: 0.1676 - val_loss: 0.2384 - val_mae: 0.4692 - 72ms/epoch - 72ms/step\n",
            "Epoch 15/20\n",
            "1/1 - 0s - loss: 0.0294 - mae: 0.1447 - val_loss: 0.2067 - val_mae: 0.4344 - 73ms/epoch - 73ms/step\n",
            "Epoch 16/20\n",
            "1/1 - 0s - loss: 0.0213 - mae: 0.1307 - val_loss: 0.1778 - val_mae: 0.3997 - 72ms/epoch - 72ms/step\n",
            "Epoch 17/20\n",
            "1/1 - 0s - loss: 0.0239 - mae: 0.1308 - val_loss: 0.1518 - val_mae: 0.3655 - 72ms/epoch - 72ms/step\n",
            "Epoch 18/20\n",
            "1/1 - 0s - loss: 0.0164 - mae: 0.1131 - val_loss: 0.1298 - val_mae: 0.3334 - 71ms/epoch - 71ms/step\n",
            "Epoch 19/20\n",
            "1/1 - 0s - loss: 0.0259 - mae: 0.1409 - val_loss: 0.1113 - val_mae: 0.3037 - 78ms/epoch - 78ms/step\n",
            "Epoch 20/20\n",
            "1/1 - 0s - loss: 0.0239 - mae: 0.1316 - val_loss: 0.0973 - val_mae: 0.2788 - 71ms/epoch - 71ms/step\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0973 - mae: 0.2788\n",
            "Loss: [0.09733103215694427, 0.2787761092185974]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import tensorflow as tf\n",
        "\n",
        "# ... (rest of the code remains the same)\n",
        "\n",
        "# TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Iterar sobre cada split\n",
        "for i, (train_index, test_index) in enumerate(tscv.split(data_norm)):\n",
        "    train_tscv = data_norm.iloc[train_index]\n",
        "    test_tscv = data_norm.iloc[test_index]\n",
        "\n",
        "    # ... (rest of the code remains the same)\n",
        "\n",
        "    # Crear datasets de ventanas\n",
        "    data_train_wrangled = windowed_dataset(train_tscv.values, 'train', window_size, horizon, batch_size)\n",
        "    data_valid_wrangled = windowed_dataset(test_tscv.values, 'test', window_size, horizon, batch_size)\n",
        "\n",
        "    # Check if datasets are empty and adjust if necessary\n",
        "    if len(list(data_train_wrangled)) == 0 or len(list(data_valid_wrangled)) == 0:\n",
        "        print(f\"Warning: Empty dataset encountered for split {i+1}. Skipping this split.\")\n",
        "        continue  # Skip to the next split\n",
        "\n",
        "    # ... (rest of the code remains the same)"
      ],
      "metadata": {
        "id": "zhAT3TllySbz",
        "outputId": "34574c9f-c17b-419f-d0c9-d9325e5ac30d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TimeSeries Split\n",
        "normalization = 'MinMax'\n",
        "window_size = 3\n",
        "horizon = 1\n",
        "batch_size = 32\n",
        "n_splits = 5\n",
        "\n",
        "data_norm, data_norm_params = normalize_data(data, normalization=normalization)\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# TimeSeriesSplit: 3 splits para ejemplo\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Almacenar las pérdidas para cada split\n",
        "split_losses = []\n",
        "\n",
        "# Iterar sobre cada split\n",
        "for i, (train_index, test_index) in enumerate(tscv.split(data_norm)):\n",
        "    train_tscv = data.iloc[train_index]\n",
        "    test_tscv = data.iloc[test_index]\n",
        "    # print(i)\n",
        "\n",
        "    # Crear datasets de ventanas\n",
        "    # data_norm_train, data_norm_valid = split_data(data_norm)\n",
        "    data_train_wrangled = windowed_dataset(train_tscv.values, 'train', window_size, horizon, batch_size)\n",
        "    data_valid_wrangled = windowed_dataset(test_tscv.values, 'test', window_size, horizon, batch_size)\n",
        "\n",
        "    # Check if datasets are empty and adjust if necessary\n",
        "    if len(list(data_train_wrangled)) == 0 or len(list(data_valid_wrangled)) == 0:\n",
        "      print(f\"Warning: Empty dataset encountered for split {i+1}. Skipping this split.\")\n",
        "      continue  # Skip to the next split\n",
        "\n",
        "    model_name = 'TimeSeries'\n",
        "    loss = 'mse'\n",
        "    optimizer = 'adam'\n",
        "    n_features = data.shape[1]\n",
        "    patience = 3\n",
        "    epochs = 20\n",
        "\n",
        "    callbacks = MyCallbacks(patience)\n",
        "    model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "    history = model.fit(\n",
        "        data_train_wrangled,\n",
        "        validation_data = data_valid_wrangled,\n",
        "        epochs=epochs,\n",
        "        verbose=2,\n",
        "        callbacks = callbacks)\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de validación\n",
        "    val_loss = model.evaluate(data_valid_wrangled)\n",
        "    print(f'Split {i+1} - Loss: {val_loss}')\n",
        "    split_losses.append(val_loss)\n",
        "\n",
        "# Promedio de las pérdidas en todos los splits\n",
        "avg_loss = np.mean(split_losses)\n",
        "print(f'Average Loss across all splits: {avg_loss}')\n"
      ],
      "metadata": {
        "id": "1cB9m0sZwBur",
        "outputId": "37cbbfb3-812c-442b-dece-504570987a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_50 (Bidirect  (None, 3, 64)             8704      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_51 (Bidirect  (None, 32)                10368     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            " reshape_25 (Reshape)        (None, 1, 1)              0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19105 (74.63 KB)\n",
            "Trainable params: 19105 (74.63 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "1/1 - 10s - loss: 0.4170 - mae: 0.6390 - val_loss: 0.2951 - val_mae: 0.5413 - 10s/epoch - 10s/step\n",
            "Epoch 2/20\n",
            "1/1 - 0s - loss: 0.4001 - mae: 0.6265 - val_loss: 0.2801 - val_mae: 0.5273 - 72ms/epoch - 72ms/step\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 - 0s - loss: 0.3920 - mae: 0.6226 - val_loss: 0.2644 - val_mae: 0.5123 - 70ms/epoch - 70ms/step\n",
            "Epoch 4/20\n",
            "1/1 - 0s - loss: 0.3817 - mae: 0.6099 - val_loss: 0.2487 - val_mae: 0.4967 - 74ms/epoch - 74ms/step\n",
            "Epoch 5/20\n",
            "1/1 - 0s - loss: 0.3594 - mae: 0.5898 - val_loss: 0.2330 - val_mae: 0.4807 - 65ms/epoch - 65ms/step\n",
            "Epoch 6/20\n",
            "1/1 - 0s - loss: 0.3422 - mae: 0.5784 - val_loss: 0.2176 - val_mae: 0.4644 - 67ms/epoch - 67ms/step\n",
            "Epoch 7/20\n",
            "1/1 - 0s - loss: 0.3277 - mae: 0.5649 - val_loss: 0.2023 - val_mae: 0.4477 - 68ms/epoch - 68ms/step\n",
            "Epoch 8/20\n",
            "1/1 - 0s - loss: 0.3175 - mae: 0.5592 - val_loss: 0.1874 - val_mae: 0.4307 - 67ms/epoch - 67ms/step\n",
            "Epoch 9/20\n",
            "1/1 - 0s - loss: 0.2981 - mae: 0.5322 - val_loss: 0.1728 - val_mae: 0.4134 - 71ms/epoch - 71ms/step\n",
            "Epoch 10/20\n",
            "1/1 - 0s - loss: 0.2563 - mae: 0.4971 - val_loss: 0.1583 - val_mae: 0.3956 - 70ms/epoch - 70ms/step\n",
            "Epoch 11/20\n",
            "1/1 - 0s - loss: 0.2830 - mae: 0.5254 - val_loss: 0.1443 - val_mae: 0.3776 - 81ms/epoch - 81ms/step\n",
            "Epoch 12/20\n",
            "1/1 - 0s - loss: 0.2505 - mae: 0.4893 - val_loss: 0.1306 - val_mae: 0.3590 - 73ms/epoch - 73ms/step\n",
            "Epoch 13/20\n",
            "1/1 - 0s - loss: 0.2519 - mae: 0.4896 - val_loss: 0.1174 - val_mae: 0.3401 - 67ms/epoch - 67ms/step\n",
            "Epoch 14/20\n",
            "1/1 - 0s - loss: 0.2280 - mae: 0.4685 - val_loss: 0.1044 - val_mae: 0.3206 - 67ms/epoch - 67ms/step\n",
            "Epoch 15/20\n",
            "1/1 - 0s - loss: 0.2076 - mae: 0.4435 - val_loss: 0.0919 - val_mae: 0.3004 - 68ms/epoch - 68ms/step\n",
            "Epoch 16/20\n",
            "1/1 - 0s - loss: 0.1824 - mae: 0.4094 - val_loss: 0.0799 - val_mae: 0.2798 - 71ms/epoch - 71ms/step\n",
            "Epoch 17/20\n",
            "1/1 - 0s - loss: 0.1951 - mae: 0.4216 - val_loss: 0.0684 - val_mae: 0.2586 - 68ms/epoch - 68ms/step\n",
            "Epoch 18/20\n",
            "1/1 - 0s - loss: 0.1421 - mae: 0.3507 - val_loss: 0.0575 - val_mae: 0.2366 - 67ms/epoch - 67ms/step\n",
            "Epoch 19/20\n",
            "1/1 - 0s - loss: 0.1794 - mae: 0.4099 - val_loss: 0.0473 - val_mae: 0.2140 - 72ms/epoch - 72ms/step\n",
            "Epoch 20/20\n",
            "1/1 - 0s - loss: 0.1540 - mae: 0.3651 - val_loss: 0.0378 - val_mae: 0.1906 - 82ms/epoch - 82ms/step\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0378 - mae: 0.1906\n",
            "Split 1 - Loss: [0.037799518555402756, 0.19056756794452667]\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_52 (Bidirect  (None, 3, 64)             8704      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_53 (Bidirect  (None, 32)                10368     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            " reshape_26 (Reshape)        (None, 1, 1)              0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19105 (74.63 KB)\n",
            "Trainable params: 19105 (74.63 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "1/1 - 10s - loss: 0.3878 - mae: 0.6173 - val_loss: 0.0074 - val_mae: 0.0758 - 10s/epoch - 10s/step\n",
            "Epoch 2/20\n",
            "1/1 - 0s - loss: 0.3540 - mae: 0.5901 - val_loss: 0.0066 - val_mae: 0.0723 - 67ms/epoch - 67ms/step\n",
            "Epoch 3/20\n",
            "1/1 - 0s - loss: 0.3469 - mae: 0.5840 - val_loss: 0.0059 - val_mae: 0.0686 - 72ms/epoch - 72ms/step\n",
            "Epoch 4/20\n",
            "1/1 - 0s - loss: 0.3309 - mae: 0.5674 - val_loss: 0.0054 - val_mae: 0.0648 - 68ms/epoch - 68ms/step\n",
            "Epoch 5/20\n",
            "1/1 - 0s - loss: 0.3084 - mae: 0.5478 - val_loss: 0.0051 - val_mae: 0.0608 - 71ms/epoch - 71ms/step\n",
            "Epoch 6/20\n",
            "1/1 - 0s - loss: 0.2834 - mae: 0.5219 - val_loss: 0.0051 - val_mae: 0.0594 - 68ms/epoch - 68ms/step\n",
            "Epoch 7/20\n",
            "1/1 - 0s - loss: 0.2740 - mae: 0.5150 - val_loss: 0.0052 - val_mae: 0.0633 - 33ms/epoch - 33ms/step\n",
            "Epoch 8/20\n",
            "1/1 - 0s - loss: 0.2468 - mae: 0.4882 - val_loss: 0.0056 - val_mae: 0.0673 - 33ms/epoch - 33ms/step\n",
            "Epoch 9/20\n",
            "1/1 - 0s - loss: 0.2286 - mae: 0.4682 - val_loss: 0.0063 - val_mae: 0.0714 - 37ms/epoch - 37ms/step\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0051 - mae: 0.0594\n",
            "Split 2 - Loss: [0.005067544057965279, 0.059356968849897385]\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_54 (Bidirect  (None, 3, 64)             8704      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_55 (Bidirect  (None, 32)                10368     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            " reshape_27 (Reshape)        (None, 1, 1)              0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19105 (74.63 KB)\n",
            "Trainable params: 19105 (74.63 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "1/1 - 11s - loss: 0.2496 - mae: 0.4359 - val_loss: 0.2280 - val_mae: 0.4715 - 11s/epoch - 11s/step\n",
            "Epoch 2/20\n",
            "1/1 - 0s - loss: 0.2461 - mae: 0.4316 - val_loss: 0.2206 - val_mae: 0.4639 - 75ms/epoch - 75ms/step\n",
            "Epoch 3/20\n",
            "1/1 - 0s - loss: 0.2420 - mae: 0.4268 - val_loss: 0.2130 - val_mae: 0.4558 - 77ms/epoch - 77ms/step\n",
            "Epoch 4/20\n",
            "1/1 - 0s - loss: 0.2160 - mae: 0.4025 - val_loss: 0.2052 - val_mae: 0.4474 - 91ms/epoch - 91ms/step\n",
            "Epoch 5/20\n",
            "1/1 - 0s - loss: 0.2156 - mae: 0.4017 - val_loss: 0.1972 - val_mae: 0.4387 - 69ms/epoch - 69ms/step\n",
            "Epoch 6/20\n",
            "1/1 - 0s - loss: 0.1970 - mae: 0.3796 - val_loss: 0.1892 - val_mae: 0.4296 - 69ms/epoch - 69ms/step\n",
            "Epoch 7/20\n",
            "1/1 - 0s - loss: 0.1919 - mae: 0.3752 - val_loss: 0.1811 - val_mae: 0.4204 - 82ms/epoch - 82ms/step\n",
            "Epoch 8/20\n",
            "1/1 - 0s - loss: 0.1671 - mae: 0.3461 - val_loss: 0.1731 - val_mae: 0.4109 - 68ms/epoch - 68ms/step\n",
            "Epoch 9/20\n",
            "1/1 - 0s - loss: 0.1634 - mae: 0.3377 - val_loss: 0.1650 - val_mae: 0.4011 - 73ms/epoch - 73ms/step\n",
            "Epoch 10/20\n",
            "1/1 - 0s - loss: 0.1516 - mae: 0.3255 - val_loss: 0.1569 - val_mae: 0.3912 - 67ms/epoch - 67ms/step\n",
            "Epoch 11/20\n",
            "1/1 - 0s - loss: 0.1499 - mae: 0.3195 - val_loss: 0.1488 - val_mae: 0.3809 - 68ms/epoch - 68ms/step\n",
            "Epoch 12/20\n",
            "1/1 - 0s - loss: 0.1209 - mae: 0.2948 - val_loss: 0.1407 - val_mae: 0.3704 - 74ms/epoch - 74ms/step\n",
            "Epoch 13/20\n",
            "1/1 - 0s - loss: 0.1182 - mae: 0.2859 - val_loss: 0.1327 - val_mae: 0.3596 - 70ms/epoch - 70ms/step\n",
            "Epoch 14/20\n",
            "1/1 - 0s - loss: 0.1010 - mae: 0.2605 - val_loss: 0.1246 - val_mae: 0.3484 - 69ms/epoch - 69ms/step\n",
            "Epoch 15/20\n",
            "1/1 - 0s - loss: 0.0965 - mae: 0.2549 - val_loss: 0.1166 - val_mae: 0.3369 - 70ms/epoch - 70ms/step\n",
            "Epoch 16/20\n",
            "1/1 - 0s - loss: 0.1011 - mae: 0.2562 - val_loss: 0.1087 - val_mae: 0.3252 - 69ms/epoch - 69ms/step\n",
            "Epoch 17/20\n",
            "1/1 - 0s - loss: 0.0775 - mae: 0.2414 - val_loss: 0.1009 - val_mae: 0.3131 - 95ms/epoch - 95ms/step\n",
            "Epoch 18/20\n",
            "1/1 - 0s - loss: 0.0903 - mae: 0.2520 - val_loss: 0.0933 - val_mae: 0.3009 - 72ms/epoch - 72ms/step\n",
            "Epoch 19/20\n",
            "1/1 - 0s - loss: 0.0601 - mae: 0.2127 - val_loss: 0.0859 - val_mae: 0.2885 - 74ms/epoch - 74ms/step\n",
            "Epoch 20/20\n",
            "1/1 - 0s - loss: 0.0622 - mae: 0.2199 - val_loss: 0.0787 - val_mae: 0.2759 - 69ms/epoch - 69ms/step\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0787 - mae: 0.2759\n",
            "Split 3 - Loss: [0.07867009192705154, 0.2758747637271881]\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_56 (Bidirect  (None, 3, 64)             8704      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_57 (Bidirect  (None, 32)                10368     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            " reshape_28 (Reshape)        (None, 1, 1)              0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19105 (74.63 KB)\n",
            "Trainable params: 19105 (74.63 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "1/1 - 13s - loss: 0.2206 - mae: 0.3932 - val_loss: 1.8620 - val_mae: 1.3579 - 13s/epoch - 13s/step\n",
            "Epoch 2/20\n",
            "1/1 - 0s - loss: 0.2206 - mae: 0.3932 - val_loss: 1.8620 - val_mae: 1.3579 - 36ms/epoch - 36ms/step\n",
            "Epoch 3/20\n",
            "1/1 - 0s - loss: 0.2206 - mae: 0.3932 - val_loss: 1.8620 - val_mae: 1.3579 - 35ms/epoch - 35ms/step\n",
            "Epoch 4/20\n",
            "1/1 - 0s - loss: 0.2206 - mae: 0.3932 - val_loss: 1.8620 - val_mae: 1.3579 - 40ms/epoch - 40ms/step\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.8620 - mae: 1.3579\n",
            "Split 4 - Loss: [1.862005352973938, 1.3579355478286743]\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_58 (Bidirect  (None, 3, 64)             8704      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_59 (Bidirect  (None, 32)                10368     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            " reshape_29 (Reshape)        (None, 1, 1)              0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19105 (74.63 KB)\n",
            "Trainable params: 19105 (74.63 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "1/1 - 22s - loss: 0.4744 - mae: 0.5584 - val_loss: 1.0594 - val_mae: 1.0288 - 22s/epoch - 22s/step\n",
            "Epoch 2/20\n",
            "1/1 - 0s - loss: 0.4577 - mae: 0.5511 - val_loss: 1.0221 - val_mae: 1.0105 - 71ms/epoch - 71ms/step\n",
            "Epoch 3/20\n",
            "1/1 - 0s - loss: 0.4588 - mae: 0.5455 - val_loss: 0.9656 - val_mae: 0.9822 - 73ms/epoch - 73ms/step\n",
            "Epoch 4/20\n",
            "1/1 - 0s - loss: 0.4399 - mae: 0.5325 - val_loss: 0.9089 - val_mae: 0.9529 - 72ms/epoch - 72ms/step\n",
            "Epoch 5/20\n",
            "1/1 - 0s - loss: 0.4389 - mae: 0.5294 - val_loss: 0.8523 - val_mae: 0.9227 - 77ms/epoch - 77ms/step\n",
            "Epoch 6/20\n",
            "1/1 - 0s - loss: 0.4153 - mae: 0.5101 - val_loss: 0.7962 - val_mae: 0.8917 - 84ms/epoch - 84ms/step\n",
            "Epoch 7/20\n",
            "1/1 - 0s - loss: 0.3593 - mae: 0.4830 - val_loss: 0.7406 - val_mae: 0.8600 - 71ms/epoch - 71ms/step\n",
            "Epoch 8/20\n",
            "1/1 - 0s - loss: 0.3541 - mae: 0.4718 - val_loss: 0.6859 - val_mae: 0.8275 - 88ms/epoch - 88ms/step\n",
            "Epoch 9/20\n",
            "1/1 - 0s - loss: 0.3488 - mae: 0.4715 - val_loss: 0.6323 - val_mae: 0.7944 - 74ms/epoch - 74ms/step\n",
            "Epoch 10/20\n",
            "1/1 - 0s - loss: 0.3036 - mae: 0.4376 - val_loss: 0.5798 - val_mae: 0.7606 - 71ms/epoch - 71ms/step\n",
            "Epoch 11/20\n",
            "1/1 - 0s - loss: 0.3012 - mae: 0.4337 - val_loss: 0.5285 - val_mae: 0.7260 - 79ms/epoch - 79ms/step\n",
            "Epoch 12/20\n",
            "1/1 - 0s - loss: 0.2613 - mae: 0.4065 - val_loss: 0.4785 - val_mae: 0.6907 - 73ms/epoch - 73ms/step\n",
            "Epoch 13/20\n",
            "1/1 - 0s - loss: 0.2588 - mae: 0.4006 - val_loss: 0.4300 - val_mae: 0.6545 - 79ms/epoch - 79ms/step\n",
            "Epoch 14/20\n",
            "1/1 - 0s - loss: 0.2441 - mae: 0.3908 - val_loss: 0.3828 - val_mae: 0.6173 - 89ms/epoch - 89ms/step\n",
            "Epoch 15/20\n",
            "1/1 - 0s - loss: 0.2347 - mae: 0.3804 - val_loss: 0.3373 - val_mae: 0.5791 - 74ms/epoch - 74ms/step\n",
            "Epoch 16/20\n",
            "1/1 - 0s - loss: 0.2200 - mae: 0.3715 - val_loss: 0.2937 - val_mae: 0.5400 - 69ms/epoch - 69ms/step\n",
            "Epoch 17/20\n",
            "1/1 - 0s - loss: 0.1995 - mae: 0.3619 - val_loss: 0.2521 - val_mae: 0.4999 - 84ms/epoch - 84ms/step\n",
            "Epoch 18/20\n",
            "1/1 - 0s - loss: 0.1694 - mae: 0.3291 - val_loss: 0.2126 - val_mae: 0.4584 - 80ms/epoch - 80ms/step\n",
            "Epoch 19/20\n",
            "1/1 - 0s - loss: 0.1802 - mae: 0.3294 - val_loss: 0.1756 - val_mae: 0.4160 - 73ms/epoch - 73ms/step\n",
            "Epoch 20/20\n",
            "1/1 - 0s - loss: 0.1374 - mae: 0.3131 - val_loss: 0.1416 - val_mae: 0.3725 - 74ms/epoch - 74ms/step\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1416 - mae: 0.3725\n",
            "Split 5 - Loss: [0.14157693088054657, 0.3725212812423706]\n",
            "Average Loss across all splits: 0.4381375567987561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#############################################################################\n",
        "def compile_model(new_model, loss, optimizer):\n",
        "    new_model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
        "    print(new_model.summary())\n",
        "    return new_model\n",
        "#############################################################################\n",
        "def MyModel(loss, optimizer, window_size, horizon, n_features):\n",
        "    new_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer((window_size, n_features)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=False)),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(n_features * horizon, activation='relu'),\n",
        "        tf.keras.layers.Reshape((horizon, n_features)),\n",
        "    ])\n",
        "    return compile_model(new_model, loss, optimizer)\n",
        "\n",
        "#############################################################################\n",
        "def windowed_dataset(sequence, data_split, window_size, horizon, batch_size, shuffle_buffer=1000):\n",
        "    \"\"\"Generates dataset windows.\n",
        "\n",
        "    Args:\n",
        "      sequence (array-like): Contains the values of the time series.\n",
        "      data_split (str): Specifies if the dataset is for training or validation/test.\n",
        "      window_size (int): The number of time steps to include in the feature.\n",
        "      horizon (int): The number of future time steps to predict.\n",
        "      batch_size (int): The batch size.\n",
        "      shuffle_buffer (int): Buffer size to use for the shuffle method.\n",
        "\n",
        "    Returns:\n",
        "      tf.data.Dataset: TF Dataset containing time windows.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "\n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size + horizon, shift=1, drop_remainder=True)\n",
        "\n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + horizon))\n",
        "\n",
        "    # Create tuples with features and labels\n",
        "    dataset = dataset.map(lambda window: (window[:-horizon], window[-horizon:]))\n",
        "\n",
        "    if data_split == 'train':\n",
        "        # Shuffle the training data to improve generalization\n",
        "        dataset = dataset.shuffle(shuffle_buffer)\n",
        "    else:\n",
        "        # Cache the validation/test data for improved performance\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "    # Create batches of windows and prefetch for performance\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "WUGg_bvh0cvc"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import tensorflow as tf\n",
        "\n",
        "# Función de normalización (supongamos ya definida)\n",
        "def normalize_data(df, normalization=\"MinMax\"):\n",
        "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "    if normalization == \"MinMax\":\n",
        "        scaler = MinMaxScaler()\n",
        "    elif normalization == \"Standard\":\n",
        "        scaler = StandardScaler()\n",
        "    else:\n",
        "        raise ValueError(\"Normalization method not supported.\")\n",
        "\n",
        "    data_norm = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
        "    return data_norm, scaler\n",
        "\n",
        "# Función de callbacks (supuesta ya definida)\n",
        "def MyCallbacks(patience):\n",
        "    return [\n",
        "        tf.keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', save_best_only=True)\n",
        "    ]\n",
        "\n",
        "#############################################################################\n",
        "def compile_model(new_model, loss, optimizer):\n",
        "    new_model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
        "    print(new_model.summary())\n",
        "    return new_model\n",
        "\n",
        "#############################################################################\n",
        "def MyModel(loss, optimizer, window_size, horizon, n_features):\n",
        "    new_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer((window_size, n_features)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=False)),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(n_features * horizon, activation='relu'),\n",
        "        tf.keras.layers.Reshape((horizon, n_features)),\n",
        "    ])\n",
        "    return compile_model(new_model, loss, optimizer)\n",
        "\n",
        "#############################################################################\n",
        "def windowed_dataset(sequence, data_split, window_size, horizon, batch_size, shuffle_buffer=1000):\n",
        "    \"\"\"Generates dataset windows.\n",
        "\n",
        "    Args:\n",
        "      sequence (array-like): Contains the values of the time series.\n",
        "      data_split (str): Specifies if the dataset is for training or validation/test.\n",
        "      window_size (int): The number of time steps to include in the feature.\n",
        "      horizon (int): The number of future time steps to predict.\n",
        "      batch_size (int): The batch size.\n",
        "      shuffle_buffer (int): Buffer size to use for the shuffle method.\n",
        "\n",
        "    Returns:\n",
        "      tf.data.Dataset: TF Dataset containing time windows.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "\n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size + horizon, shift=1, drop_remainder=True)\n",
        "\n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + horizon))\n",
        "\n",
        "    # Create tuples with features and labels\n",
        "    dataset = dataset.map(lambda window: (window[:-horizon], window[-horizon:]))\n",
        "\n",
        "    if data_split == 'train':\n",
        "        # Shuffle the training data to improve generalization\n",
        "        dataset = dataset.shuffle(shuffle_buffer)\n",
        "    else:\n",
        "        # Cache the validation/test data for improved performance\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "    # Create batches of windows and prefetch for performance\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Crear datos ficticios con estacionalidad\n",
        "np.random.seed(0)\n",
        "date_range = pd.date_range(start='1/1/2018', periods=36, freq='M')\n",
        "trend = np.linspace(0, 1, 36)\n",
        "seasonal = 0.5 * np.sin(np.linspace(0, 3 * np.pi, 36))\n",
        "random = 0.1 * np.random.randn(36)\n",
        "data = pd.DataFrame(trend + seasonal + random, index=date_range, columns=['value'])\n",
        "\n",
        "# Normalización de los datos\n",
        "normalization = 'MinMax'\n",
        "window_size = 6\n",
        "horizon = 1\n",
        "batch_size = 32\n",
        "n_splits = 5\n",
        "data_norm, data_norm_params = normalize_data(data, normalization=normalization)\n",
        "\n",
        "# TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Iterar sobre cada split\n",
        "for i, (train_index, test_index) in enumerate(tscv.split(data_norm)):\n",
        "    train_tscv = data_norm.iloc[train_index]\n",
        "    test_tscv = data_norm.iloc[test_index]\n",
        "\n",
        "    print(f'Split {i+1}')\n",
        "    print(f'Train indices: {train_index}')\n",
        "    print(f'Test indices: {test_index}')\n",
        "    print(f'Train data shape: {train_tscv.shape}')\n",
        "    print(f'Test data shape: {test_tscv.shape}')\n",
        "\n",
        "    # Crear datasets de ventanas\n",
        "    data_train_wrangled = windowed_dataset(train_tscv.values, 'train', window_size, horizon, batch_size)\n",
        "    data_valid_wrangled = windowed_dataset(test_tscv.values, 'test', window_size, horizon, batch_size)\n",
        "\n",
        "    # Verificar que los datasets no estén vacíos\n",
        "    print(f'Train dataset size: {len(list(data_train_wrangled))}')\n",
        "    print(f'Validation dataset size: {len(list(data_valid_wrangled))}')\n",
        "\n",
        "    # Configuración del modelo\n",
        "    model_name = 'TimeSeries'\n",
        "    loss = 'mse'\n",
        "    optimizer = 'adam'\n",
        "    n_features = data_norm.shape[1]\n",
        "    patience = 3\n",
        "    epochs = 20\n",
        "\n",
        "    callbacks = MyCallbacks(patience)\n",
        "    model = MyModel(loss, optimizer, window_size, horizon, n_features)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(\n",
        "        data_train_wrangled,\n",
        "        validation_data=data_valid_wrangled,\n",
        "        epochs=epochs,\n",
        "        verbose=2,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de validación\n",
        "    val_loss = model.evaluate(data_valid_wrangled)\n",
        "    print(f'Split {i+1} - Loss: {val_loss}')\n"
      ],
      "metadata": {
        "id": "wJupoxOH1GY_",
        "outputId": "41a6c3bb-8041-4c52-a55a-16f5e7590cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        }
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1\n",
            "Train indices: [0 1 2 3 4 5]\n",
            "Test indices: [ 6  7  8  9 10 11]\n",
            "Train data shape: (6, 1)\n",
            "Test data shape: (6, 1)\n",
            "Train dataset size: 0\n",
            "Validation dataset size: 0\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_36 (Bidirect  (None, 6, 64)             8704      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_37 (Bidirect  (None, 32)                10368     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            " reshape_18 (Reshape)        (None, 1, 1)              0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19105 (74.63 KB)\n",
            "Trainable params: 19105 (74.63 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-201-dd29a7ab3c39>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# Entrenar el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mdata_train_wrangled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_valid_wrangled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   1820\u001b[0m                         \u001b[0;34m\"Unexpected result of `train_function` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m                         \u001b[0;34m\"(Empty logs). This could be due to issues in input \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar los últimos x meses de data_train\n",
        "#   data_for_prediction = data_train[-window_size:]\n",
        "#   # Convierte los datos a un formato compatible con la función window_dataset\n",
        "#   data_for_prediction = data_for_prediction.values.reshape((1, window_size, n_features))\n",
        "#   predictions = model.predict(data_for_prediction)\n",
        "\n",
        "#   # # Convertir las predicciones a un DataFrame para desnormalizar\n",
        "#   predictions_df = pd.DataFrame(predictions[0], columns=data_train.columns)\n",
        "\n",
        "#   # # Desnormalizar las predicciones\n",
        "#   predictions_denorm_cat1 = denormalize_series(predictions_df, data_norm_params, normalization=normalization).iloc[1]\n",
        "\n",
        "#   # Voy sumando las predicciones de cada categoria\n",
        "#   predictions_acum = predictions_acum.add(predictions_denorm_cat1, fill_value=0)"
      ],
      "metadata": {
        "id": "2lhZ4OFjtyrh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}